{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cs231n Optimization, Derivatives and Backprop.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paruliansaragi/Notebooks/blob/master/cs231n_Optimization%2C_Derivatives_and_Backprop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWiLcdMgrTbH",
        "colab_type": "text"
      },
      "source": [
        "#Optimization\n",
        "\n",
        "http://cs231n.github.io/optimization-1/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05Yc8SaHqwtx",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Optimization is the process of finding the set of $W$ that minimizes the loss.\n",
        "\n",
        "The loss functions are hard to visualize as they are over high-dimensional spaces. \n",
        "\n",
        "We can slice a plane or ray to get a 1 or 2-D view. We can generate a random $W_1$ and compute the loss along this direction by evaluating $L(W + aW_1)$ for different value of $a$. This plots $a$ as the x-axis and the loss as y. \n",
        "\n",
        "Lets explain the piecewise-linear structure of the loss by the math:\n",
        "\n",
        "$L_i = \\sum_{j\\neq y_i} \\left[ \\max(0, w_j^Tx_i - w_{y_i}^Tx_i + 1) \\right]$ \n",
        "\n",
        "From the equation the data loss for each example is a sum of (zero threshold) linear functions of W. Each row of $W$ has positive when its the wrong class and negative when correct class. Consider a dataset that has 3-D points and three classes. Full SVM loss w/out regularisation:\n",
        "\n",
        "$\\begin{align}\n",
        "L_0 = & \\max(0, w_1^Tx_0 - w_0^Tx_0 + 1) + \\max(0, w_2^Tx_0 - w_0^Tx_0 + 1) \\\\\\\\\n",
        "L_1 = & \\max(0, w_0^Tx_1 - w_1^Tx_1 + 1) + \\max(0, w_2^Tx_1 - w_1^Tx_1 + 1) \\\\\\\\\n",
        "L_2 = & \\max(0, w_0^Tx_2 - w_2^Tx_2 + 1) + \\max(0, w_1^Tx_2 - w_2^Tx_2 + 1) \\\\\\\\\n",
        "L = & (L_0 + L_1 + L_2)/3\n",
        "\\end{align}$\n",
        "\n",
        "Since these examples are 1-D, the data $x_i$ and weights $w_j$ are numbers. \n",
        "\n",
        "![alt text](http://cs231n.github.io/assets/svmbowl.png)\n",
        "\n",
        "The data loss is the sum of multiple terms. The loss is bowl-shaped (convex). NN's tend to produce non-convex. Non-differentiable loss functions: you can see that the kinks in the loss function (due to the mx operation) make the loss non-differentiable because at these kinks the gradient is not defined. However, the subgradient still exists and is used instead. \n",
        "\n",
        "The goal is to find weights to minimize the loss. \n",
        "\n",
        "Strategy 1# Random Search and keep track of what works best?\n",
        "\n",
        "```\n",
        "bestloss = float(\"inf\") # python assigns highest float value to inf\n",
        "for n in range(10000):\n",
        "W = np.random.randn(10, 3073) * 0.0001\n",
        "loss = L(X_train, Y_train, W)\n",
        "if loss < best_loss\n",
        "bestloss=loss\n",
        "bestW=W\n",
        "```\n",
        "\n",
        "15% accuracy.... **Core idea: iterative refinement:** finding the best set of weights $W$ is near impossible but refining a set of weights to be better is much less difficult. \n",
        "\n",
        "Strategy 2# Random Local Search\n",
        "\n",
        "Strategy #3 Following the Gradient\n",
        "\n",
        "Gradient : vector of partial derivatives. The gradient provides the direction of steepest ascent (if you do the dot product of the unit vector of that gradient with gradient). If we simply do 1 - the direction of steepest ascent we get the direction of steepest descent. Therefore, if we compute the best direction along which we should change our weights that is the direction of steepest descent. \n",
        "\n",
        "For 1-D (single variable) functions, the slope is the instantaneous rate of change of the function at any point. The gradient then is the slope for functions that take a vector of numbers (multivariable functions, therefore we compute the partial derivatives). \n",
        "The equation for the derivative:\n",
        "\n",
        "$\\frac{df(x)}{dx} = \\lim_{h\\ \\to 0} \\frac{f(x + h) - f(x)}{h}$\n",
        "\n",
        "###Computing the gradient\n",
        "\n",
        "There are 2 ways to compute the gradient: A slow, approximate but easy way (**numerical gradient**), and a fast, exact but more error prone way that requires calculus (**analytic gradient**). \n",
        "\n",
        "####Computing the gradient numerically with finite differences\n",
        "\n",
        "The above formula is the numerical gradient. Below is a function that takes a function f and a vector x to evaluate the gradient on and returns the gradient of f at x:\n",
        "\n",
        "```\n",
        "def numeric_grad(f, x):\n",
        "\"\"\"\n",
        "- f : function with single arg\n",
        "- x : points (np array) to evaluate the gradient at\n",
        "\"\"\"\n",
        "fx = f(x)\n",
        "grad = np.zeros(x.shape)\n",
        "h = 0.00001 # small value to nudge by\n",
        "#iterate over all indexes in x\n",
        "it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "while not it.finished:\n",
        "ix = it.multi_index\n",
        "old_value = x[ix]\n",
        "x[ix] = old_valute + h #inc by h\n",
        "fxh = f(x) # eval f(x + h)\n",
        "x[ix] = old_value #resture to prev value\n",
        "#compute partial derivative\n",
        "grad[ix] = (fxh-fx) / h #the slope\n",
        "it.iternext() # step to next dimension\n",
        "```\n",
        "\n",
        "The code iterates over all dimensions one by one, makes a small change i.e. h along that dimension and calculates the partial derivative of the loss function along that dimension by seeing how much that function changed.\n",
        "\n",
        "Mathematically the formula of the gradient requires the limit of h to go to 0, but in practice it is sufficient to be a very small number (1e-5).\n",
        "**NB:**in practice it is often better to compute the numeric gradient using the centered difference formula : $[f(x+h) - f(x-h)] / 2 h$\n",
        "\n",
        "We can use the function above to compute the gradient at any point for any function. \n",
        "\n",
        "```\n",
        "def cifar10_loss(W):\n",
        "return L(X_train, Y_train, W)\n",
        "\n",
        "W = np.random.randn(10, 3073) * 0.001\n",
        "df = numeric_grad(cifar10_loss, W)\n",
        "```\n",
        "We can now use the gradient to update the weights\n",
        "```\n",
        "loss_old = cifar10_loss(W)\n",
        "for stepsize in [-10, -5, -1]:\n",
        "step_size = 10 ** stepsize\n",
        "W_new = W - step_size * df\n",
        "loss_new = cifar10_loss(W_new)\n",
        "print('for step size %f new loss: %f' % (step_size, loss_new))\n",
        "```\n",
        "As mentioned we compute the new weights in the negative of the gradient because the gradient shows you the direction of steepest ascent and we just go in the opposite of that direction.\n",
        "\n",
        "The step size dictated by the learning rate is one of the most important hyperparameters (there are several techniques such as learning rate scheduling/annealing etc..). \n",
        "\n",
        "Evaluating the numerical gradient has a linear complexity in the number of parameters. We had 30730 params and thus had to perform 30,731 evaluations of the loss to evaluate the gradient and to perform a single parameter update. \n",
        "\n",
        "####Computing the gradient analytically with Calculus\n",
        "\n",
        "The numerical gradient is very simple to compute using the finite difference approximation, the downside is that it is approximate (we pick a small value of h, while the true gradient is as h approaches 0), and it is computationally expensive to compute. The second way is using calculus, allowing us to derive a direct formula for the gradient (no approximations) that is also very fast to compute. It can be more error prone to implement, that is why its common to compute the analytic gradient and compare it to the numerical gradient to check the correctness. This is called a **gradient check**.\n",
        "\n",
        "Usings the SVM loss for a single datapoint:\n",
        "\n",
        "$L_i = \\sum_{j\\neq y_i} \\left[ \\max(0, w_j^Tx_i - w_{y_i}^Tx_i + \\Delta) \\right]$\n",
        "\n",
        "We can differentiate the function w.r.t to the weights. E.g. taking the gradient w.r.t $w_yi$ we get: \n",
        "\n",
        "$\\nabla_{w_{y_i}} L_i = - \\left( \\sum_{j\\neq y_i} \\mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \\Delta > 0) \\right) x_i$\n",
        "\n",
        "where 1 is the indicator function (that is one if the condition inside is true or 0 otherwise). This is simply counting the number of classes that didn't meet the desired margin (and hence contributed to the loss function) and then the data vector $x_i$ scaled by this number is the gradient. This is the gradient only w.r.t the row of $W$ that corresponds to the correct class. For rows where ${j\\neq y_i}$ the gradient is:\n",
        "\n",
        "$\\nabla_{w_j} L_i = \\mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \\Delta > 0) x_i$\n",
        "\n",
        "###Gradient Descent\n",
        "\n",
        "Now we have the gradient of the loss, we can simply repeatedly evaluate the gradient and update the parameters. \n",
        "\n",
        "```\n",
        "while True:\n",
        "weights_grad = evaluate_gradient(loss_fun, data, weights)\n",
        "weights += - step_size * weights_grad # param update\n",
        "```\n",
        "This is at the core of NN libraries.\n",
        "\n",
        "**Mini-batch gradient descent:** It is wasteful to compute the full loss function over the entire training set to perform a single param update. It is better to compute the gradient over batches of data. We use batches to perform param updates:\n",
        "```\n",
        "while True:\n",
        "data_batch = sample_training_data(data, batch_size=256)\n",
        "weights_grad = eval_grad(loss_fn, data_batch, weights)\n",
        "weights += - step_size * weights_grad\n",
        "```\n",
        "\n",
        "This works well when we have copies of images and we'd get the same loss so we'd be wasting computation. The mini-batch is a good approximation of the full and the gradient of the objective function is a good approximation as a result. We can achieve convergence much quicker. It is then better to perform more updates with fewer samples.\n",
        "\n",
        "The extreme case where the mini-batch contains only one example is called stochastic gradient descent. Sometimes people use the term SGD to mean Mini-batch GD. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szNn1K-qNm1q",
        "colab_type": "text"
      },
      "source": [
        "#Derivatives, Backprop and Vectorization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6LeOwOnq2Jy",
        "colab_type": "text"
      },
      "source": [
        "###Derivatives\n",
        "####Scalar Case\n",
        "Derivatives help us measure change, that is if we were to change variable x by h what is the resultant change in y. The **chain rule** tells us how to compute the derivative of the composition of functions. Say function z = g(f(x)). If we wanted to compute the effect of x on z, if x changes by hx then y will change by the derviative of x and y = the derivative of y w.r.t x. If y changes by hy then z will change by the derivative of z w.r.t y. \n",
        "\n",
        "So we take the derivative of the outer function then multiply by the derivative of the inner. \n",
        "\n",
        "Say $\\frac{d}{dx}[ln(sin(x))]$ = $f'(g(x))$ = $f' (g(x)) . g' (x)$\n",
        "\n",
        "Basically, the chain rule says we have to take the derivative of the outer function w.r.t the inner function. \n",
        "\n",
        "$f'(g(x)) = \\frac{1}{sin(x)}$ then we multiply this with the inner\n",
        "\n",
        "$g'(x) = cos(x)$ so the derivative is $\\frac{1}{sin(x)} . cos(x)$.\n",
        "\n",
        "So the chain rule is just finding the deriviative of composite functions. \n",
        "\n",
        "The chain rule says:\n",
        "$\\frac{d}{dx}[f(g(x))] = f'(g(x)) . g'(x)$\n",
        "\n",
        "Our function is composite because it is a function within a function. \n",
        "Say $f(x) cos(x)$ and $g(x) = x^2$ then $cos(x^2) = f(g(x)).$ g is the inner function and f the outer.\n",
        "\n",
        "Say now that $g(x) = 5- 6x$ inner function and $f(x) = x^5$ outer. We can use the chain rule: $\\frac{d}{dx}[f(g(x))] = f'(g(x)) . g'(x)$\n",
        "Lets find the derivatives of the inner and outer functions: \n",
        "$g'(x) = -6$ and $f'(x) = 5x^4$. That means:\n",
        "$5(5-6x)^4 . -6$ remember the first part of the expression is the inner and outer.\n",
        "$=-30(5-6x)^4$\n",
        "\n",
        "####Jacobian & Generalized Jacobian: Tensor in Tensor out\n",
        "\n",
        "It is the M x N matrix of partial derivatives. It tells us the amount by which $y_i$ will change if $x_j$ changed by a small amount. \n",
        "\n",
        "In deep learning we take tensors as inputs and tensors as outputs. The generalized jacobian tells the same relationship but for a tensor. \n",
        "\n",
        "####Backprop with Tensors\n",
        "\n",
        "A layer f is a function of tensor inputs x and weights w; the tensor output of the layer is then y = f(x, w). The layer f has a scalar loss L. During backprop we are given the $\\frac{dL}{dy}$ and want to compute $\\frac{dL}{dx}$ and $\\frac{dL}{dw}$. We know from the chain rule that: $\\frac{dL}{dx}= \\frac{dL}{dy} . \\frac{dy}{dx}$ and  $\\frac{dL}{dw}= \\frac{dL}{dy} . \\frac{dy}{dw}$. However, the jacobian is too large to fit in memory. Say f is a linear layer and takes a minibatch N vectors each of D dimension, and produces mini-batch of N vectors each of dimension M. Then x is N x D, w is D x M and y = f(x,w) = xw is a matrix of shape N x M. \n",
        "\n",
        "The Jacobian then has shape (N x M) x (N x D). A typical NN may have N = 64, and M = D = 4096; then  $\\frac{dy}{dx}$ consists of 64 * 4096 * 64 * 2096 scalar values; which is > 68 billion; using 32-bit floating point, this jacobian matrix will take 256gb of memory. \n",
        "\n",
        "However, it turns out that for most common NN layers, we can derive expressions that compute the product  $\\frac{dy}{dx}\\frac{dL}{dy}$ without forming the jacobian. Even better, we can derive this without computing an explicit expression for the jacobian. \n",
        "\n",
        "Say N = 1, D=2, M=3.\n",
        "$y = (y_{1,1}, y_{1,2}, y_{1,3}) = xw$\n",
        "$= (x_{1,1}, x_{1,2}) (w_{1,1}, w_{1,2}, w_{1,3}\n",
        "w_{2,1}, w_{2,2}, w_{2,3})T$\n",
        "$=(x_{1,1}w_{1,1}+x_{1,2}w_{1,2}\n",
        "x_{2,1}w_{2,1}+x_{2,2}w_{2,2}\n",
        "x_{3,1}w_{3,1}+x_{3,2}w_{3,2})$\n",
        "During backprop we assume we have access to $\\frac{dL}{dy}$ which has shape (1) x (N x M) then we can write:\n",
        "$\\frac{dL}{dy} = (dy_{1,1}, dy_{1,2}, dy_{1,3})$\n",
        "Our goal is to derive an expression for $\\frac{dL}{dx}$ in terms of x, w, and  $\\frac{dL}{dy}$ without forming the entire jacobian. We know each element of  $\\frac{dL}{dx}$ (jacobian) is a scalar giving the partial derivatives of L w.r.t the elements of x:\n",
        "\n",
        " $\\frac{dL}{dx} = (\\frac{dL}{dx_{1,1}} \\frac{dL}{dx_{1,2}})$\n",
        " \n",
        " Thinking in terms of one element at a time the chain rule tells us that:\n",
        " \n",
        "$ \\frac{dL}{dx_{1,1}} = \\frac{dL}{dy} \\frac{dy}{dx_{1,1}}$\n",
        "\n",
        "$ \\frac{dL}{dx_{1,2}} = \\frac{dL}{dy} \\frac{dy}{dx_{1,2}}$\n",
        "\n",
        "If we view $\\frac{dL}{dy}$ and $\\frac{dy}{dx_{1,1}}$ as matrices of shape N x M, then their generalized matrix product is simply the dot product $ \\frac{dL}{d .y} \\frac{dy}{dx_{1,1}}$.\n",
        "Now we compute:\n",
        "\n",
        "$\\frac{dy}{dx_{1,1}} = (\\frac{dy_{1,1}}{dx_{1,1}} \\frac{dy_{1,2}}{dx_{1,1}}, \\frac{dy_{1,3}}{dx_{1,1}} ) = (w_{1,1} w_{1,2} w_{1,3})$\n",
        "\n",
        "$\\frac{dy}{dx_{1,2}} = (\\frac{dy_{1,1}}{dx_{1,2}} \\frac{dy_{1,2}}{dx_{1,2}}, \\frac{dy_{1,3}}{dx_{1,2}} ) = (w_{2,1} w_{2,2} w_{2,3})$\n",
        "\n",
        "This gives the final expression for $\\frac{dL}{dx}$:\n",
        "\n",
        "$\\frac{dL}{dx} = (\\frac{dL}{dx_{1,1}} \\frac{dL}{dx_{1,2}})$\n",
        "\n",
        "$=(dy_{1,1}w_{1,1} + dy_{1,2}w_{1,2} + dy_{1,3}w_{1,3}$\n",
        "$dy_{1,1}w_{2,1} + dy_{1,2}w_{2,2} + dy_{1,3}w_{2,3})^{T}$\n",
        "\n",
        "$=\\frac{dL}{dy}x^{T}$\n",
        "\n",
        "This final result is interesting because it allows us to efficiently compute the $\\frac{dL}{dx}$ without forming the jacobian. \n",
        "\n",
        "\n",
        "NB: the jacobian has a nice property called local linearity. Zooming in on a point the neighbourhood around the specific point looks linear. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31th7xy4fMVz",
        "colab_type": "text"
      },
      "source": [
        "#Backprop\n",
        "http://cs231n.github.io/optimization-2/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFITOc34q53N",
        "colab_type": "text"
      },
      "source": [
        "Backprop : a way of computing gradients of expressions through recursive application of the chain rule. \n",
        "\n",
        "The core problem in this section is as follows: we have some function f(x) where x is a vector of inputs and we want to compute the gradient of f at x(i.e. $\\nabla f(x)$)\n",
        "\n",
        "Lets consider: $f(x,y,z)=(x+y)z$. This can be broken down into $q=x+y$ and $f=qz$. f is just a multiplication of q and z so, $\\frac{\\partial f}{\\partial q} = z, \\frac{\\partial f}{\\partial z} = q$ and q is the addition of x and y so $\\frac{\\partial q}{\\partial x} = 1, \\frac{\\partial q}{\\partial y} = 1$. We dont care about the gradient of the q. We want the gradient of f w.r.t its inputs x,y,z. The chain rule tells us the correct way to chain these gradient expression is through multiplication. That is $\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial x}$:\n",
        "\n",
        "```\n",
        "x = -2; y = 5; z = -4\n",
        "# perform the forward pass\n",
        "q = x + y # q becomes 3\n",
        "f = q * z # f becomes -12\n",
        "\n",
        "# perform the backward pass (backpropagation) in reverse order:\n",
        "# first backprop through f = q * z\n",
        "dfdz = q # df/dz = q, so gradient on z becomes 3\n",
        "dfdq = z # df/dq = z, so gradient on q becomes -4\n",
        "# now backprop through q = x + y\n",
        "dfdx = 1.0 * dfdq # dq/dx = 1. And the multiplication here is the chain rule!\n",
        "dfdy = 1.0 * dfdq # dq/dy = 1\n",
        "```\n",
        "\n",
        "We are left with the gradients [dfdx, dfdy, dfdz], which tells us the sensitivty of the variables x,y,z on f. \n",
        "\n",
        "####Intuition of backprop\n",
        "\n",
        "Backprop is a local process, every gate in a circuit diagram gets some inputs and compute two things: 1. its output value and 2. the local gradient of its inputs w.r.t its output value. They do this without awareness of any other details of the full circuit. Once the forward pass is over it will learn about the gradient of its output value on the final output of the entire circuit. The chain rule says take that gradient and multiply it into every gradient for all its inputs.\n",
        "\n",
        "![alt text](https://xiandong79.github.io/downloads/circuit_diagram.png)\n",
        "\n",
        "The add gate receives an input -2, 5 and computes output 3. Since its computing addition, its local gradient for both of its inputs is +1. The rest of the circuit computes the final value -12. During the backward pass, the chain rule is applied recursively through the circuit, the add gate (which is an input to the multiply gate) leans that the gradient for its output was -4. The circuit wants a higher value, then the circuit wants the output of the add gate to be lower (due to the negative sign) and with force of 4. The add gate takes that gradient and multiplies it to all of the local gradients for its inputs (making the gradient on both x and y 1 * -4 = -4). This has the desired effect, if x,y were to decrease (in response to their negative gradient) then the add gate's output would decrease, in turn making the multiply gate's output increase. \n",
        "\n",
        "Backprop = gate's talking to each other.\n",
        "\n",
        "####Signmoid example\n",
        "Any differentiable function can be a gate, we can group multiple gates into a single gate, or decompose a function into multiple gates. \n",
        "\n",
        "Example: $f(w,x) = \\frac{1}{1+e^{-(w_0x_0 + w_1x_1 + w_2)}}$\n",
        "\n",
        "This is a 2-D neuron with inputs x and weights w that uses a sigmoid activation function. The function comprises multiple gates:\n",
        "\n",
        "$f(x) = \\frac{1}{x} \n",
        "\\hspace{1in} \\rightarrow \\hspace{1in} \n",
        "\\frac{df}{dx} = -1/x^2 \n",
        "\\\\\\\\\n",
        "f_c(x) = c + x\n",
        "\\hspace{1in} \\rightarrow \\hspace{1in} \n",
        "\\frac{df}{dx} = 1 \n",
        "\\\\\\\\\n",
        "f(x) = e^x\n",
        "\\hspace{1in} \\rightarrow \\hspace{1in} \n",
        "\\frac{df}{dx} = e^x\n",
        "\\\\\\\\\n",
        "f_a(x) = ax\n",
        "\\hspace{1in} \\rightarrow \\hspace{1in} \n",
        "\\frac{df}{dx} = a$\n",
        "\n",
        "Where functions $f_c, f_a$ translate the input by a constant c and scale the input by constant a. These are special cases of addition and multiplication, they are new unary gates, since we dont need the gradients for c,a. \n",
        "\n",
        "![alt text](https://xiandong79.github.io/downloads/circuit.png)\n",
        "\n",
        "We see above a long chain of functions that operates on the result of the dot product of w,x. This is the sigmoid function. The derivative of the sigmoid w.r.t to its input simplifies if you perform the derivation (after adding and subtracting 1 in the numerator):\n",
        "\n",
        "$\\sigma(x) = \\frac{1}{1+e^{-x}} \\\\\\\\\n",
        "\\rightarrow \\hspace{0.3in} \\frac{d\\sigma(x)}{dx} = \\frac{e^{-x}}{(1+e^{-x})^2} = \\left( \\frac{1 + e^{-x} - 1}{1 + e^{-x}} \\right) \\left( \\frac{1}{1+e^{-x}} \\right) \n",
        "= \\left( 1 - \\sigma(x) \\right) \\sigma(x)$\n",
        "\n",
        "The gradient simplies nicely. E.g. the sigmoid recieves 1.0 as input and computes the output as 0.73. The derivation shows the local gradient would simply be (1 - 0.73) * 0.73 which is roughly 0.2. In practice its good to group these operations into a single gate. \n",
        "\n",
        "```\n",
        "w = [2,-3,-3] # assume some random weights and data\n",
        "x = [-1, -2]\n",
        "\n",
        "# forward pass\n",
        "dot = w[0]*x[0] + w[1]*x[1] + w[2]\n",
        "f = 1.0 / (1 + math.exp(-dot)) # sigmoid function\n",
        "\n",
        "# backward pass through the neuron (backpropagation)\n",
        "ddot = (1 - f) * f # gradient on dot variable, using the sigmoid gradient derivation\n",
        "dx = [w[0] * ddot, w[1] * ddot] # backprop into x\n",
        "dw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # backprop into w\n",
        "# we're done! we have the gradients on the inputs to the circuit\n",
        "```\n",
        "\n",
        "###Backprop in practice\n",
        "\n",
        "Another example: \n",
        "\n",
        "$f(x,y) = \\frac{x + \\sigma(y)}{\\sigma(x) + (x+y)^2}$\n",
        "\n",
        "Its good to keep track of the intermediate variables. So cache forward pass variables. \n",
        "\n",
        "![alt text](https://xiandong79.github.io/downloads/circuit2.png)\n",
        "\n",
        "Looking at the above we can see:\n",
        "\n",
        "The add gate always takes the gradient on its output and distributes it equally to all of its inputs. The local gradient for the add op is simply +1.0 so the gradients on all inputs will exactly equal the gradients on the output because it is multiplied by x1.0. Note that the + gate routed the gradient of 2.00 to both of its inputs, equally and unchanged. \n",
        "\n",
        "The max gate routes the gradient. The max gate distributes the gradient unchanged to one of its input (the one with the highest value during forward pass). This is because the local gradient for a max gate is 1.0 for the highest value and 0 for all others. In the above, the max routes 2.00 grad to z. \n",
        "\n",
        "The multiply gate: its local gradients are the input values (except switched) which is multiplied by the gradient on its output during the chain rule. The gradient on x is -8.00, which is -4.00 x 2.00. \n",
        "\n",
        "Notice: if one input to the multiply gate is very small and other is very big then the multiply gate will do something unintuitive: it will asign a huge gradient to the small and a tiny to the large. In linear classifier where the weights are dot producted with inputs, this implies the scale of the data has an effect on the magnitude of the gradient for the weights. E.g. if you multiplied all input data by 1000 during preprocessing then the grad on the weights will be 1000x larger, and you'd have a lower learning rate.\n",
        "\n",
        "###Gradients for vectorized ops\n",
        "\n",
        "The concepts above extend to matrix/vector ops. \n",
        "\n",
        "**Matrix-Matrix multiply gradient**. The most tricky op is mat-mat multipication:\n",
        "\n",
        "```\n",
        "W = np.random.randn(5, 10)\n",
        "X = np.random.randn(10, 3)\n",
        "D = W.dot(X)\n",
        "\n",
        "# now suppose we had the gradient on D from above in the circuit\n",
        "dD = np.random.randn(*D.shape) # same shape as D\n",
        "dW = dD.dot(X.T) \n",
        "dX = W.T.dot(dD)\n",
        "\n",
        "```\n",
        "\n",
        "For instance, we know that the gradient on the weights dW must be of the same size as W after it is computed, and that it must depend on matrix multiplication of X and dD (as is the case when both X,W are single numbers and not matrices). There is always exactly one way of achieving this so that the dimensions work out. For example, X is of size [10 x 3] and dD of size [5 x 3], so if we want dW and W has shape [5 x 10], then the only way of achieving this is with dD.dot(X.T), as shown above.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxzwmsy9r2v9",
        "colab_type": "text"
      },
      "source": [
        "http://cs231n.stanford.edu/vecDerivs.pdf\n",
        "\n",
        "Shows how the derivative of y w.r.t x is just the dot product of the weights and the input. Since the y is a summation of the dot products of weights and inputs. The derivative \n",
        "$\\frac{\\partial y_3}{\\partial x_7} = \\frac{\\partial}{\\partial x_7}[W_{3,1}x_1 + W_{3,2}x_2 + ... + W_{3,D}x_D]$\n",
        "\n",
        "Since none of the other terms in the summation include $x_7$ their derivatives w.r.t to $x_7$ are all 0. Thus:\n",
        "\n",
        "$= 0 + 0 + .. +  \\frac{\\partial}{\\partial x_7}[W_{3,7}x_7]+...+0$\n",
        "\n",
        "$=\\frac{\\partial}{\\partial x_7}[W_{3,7}x_7]$\n",
        "$=W_{3,7$\n",
        "\n",
        "So the vector of y's is just equal to $Wx$.\n",
        "If you do the same process you will find that for all i and j:\n",
        "\n",
        "$\\frac{\\partial y_i}{\\partial x_j} = W_{i,j}$ \n",
        "\n",
        "Thus the matrix of partial derivatives (Jacobian) is equivalent to your weight matrix $W$. Thus we can conclude that:\n",
        "\n",
        "$\\vec{y}=W\\vec{x}$ and we have $\\frac{d \\vec{y} } {d \\vec{x}}$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4YXMXoHr3EU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
