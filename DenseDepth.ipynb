{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DenseDepth.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paruliansaragi/Notebooks/blob/master/DenseDepth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Exx_d42jaf2L",
        "colab_type": "code",
        "outputId": "1b28f2be-7a53-4494-e4c6-4a7afe65a837",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "!git clone https://github.com/ialhashim/DenseDepth.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DenseDepth'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 143 (delta 0), reused 0 (delta 0), pack-reused 140\u001b[K\n",
            "Receiving objects: 100% (143/143), 11.46 MiB | 37.37 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjTK0BfOqLZF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv nyu.h5 DenseDepth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmEvl51KbcuS",
        "colab_type": "code",
        "outputId": "4f834f5b-b1d1-49e9-e2dc-486e7da27cb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd DenseDepth"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DenseDepth\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0y_EGSco67l",
        "colab_type": "code",
        "outputId": "946b5526-9fac-4d41-8bdb-68136477e86f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "source": [
        "!python test.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "Loading model...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "\n",
            "Model loaded (nyu.h5).\n",
            "\n",
            "Loaded (12) images of size (480, 640, 3).\n",
            "<Figure size 1000x500 with 1 Axes>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9i3tb5rqbp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import argparse\n",
        "\n",
        "# Kerasa / TensorFlow\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '5'\n",
        "from keras.models import load_model\n",
        "from layers import BilinearUpSampling2D\n",
        "from loss import depth_loss_function\n",
        "from utils import predict, load_images, display_images\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Custom object needed for inference and training\n",
        "custom_objects = {'BilinearUpSampling2D': BilinearUpSampling2D, 'depth_loss_function': depth_loss_function}\n",
        "\n",
        "print('Loading model...')\n",
        "\n",
        "# Load model into GPU / CPU\n",
        "model = load_model('nyu.h5', custom_objects=custom_objects, compile=False)\n",
        "\n",
        "print('\\nModel loaded ({0}).'.format('nyu.h5'))\n",
        "\n",
        "# Input images\n",
        "inputs = load_images( glob.glob('test_data/*.jpg') )\n",
        "print('\\nLoaded ({0}) images of size {1}.'.format(inputs.shape[0], inputs.shape[1:]))\n",
        "\n",
        "# Compute results\n",
        "outputs = predict(model, inputs)\n",
        "\n",
        "# Display results\n",
        "viz = display_images(outputs.copy(), inputs.copy())\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(viz)\n",
        "cv2.imwrite('outputs.png',viz)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKwSTJl81iQN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download_images(outputs, inputs=None, gt=None, is_colormap=True, is_rescale=True):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import skimage\n",
        "    from skimage.transform import resize\n",
        "\n",
        "    plasma = plt.get_cmap('plasma')\n",
        "\n",
        "    shape = (outputs[0].shape[0], outputs[0].shape[1], 3)\n",
        "    \n",
        "    all_images = []\n",
        "\n",
        "    for i in range(outputs.shape[0]):\n",
        "        imgs = []\n",
        "        \n",
        "        if isinstance(inputs, (list, tuple, np.ndarray)):\n",
        "            x = to_multichannel(inputs[i])\n",
        "            x = resize(x, shape, preserve_range=True, mode='reflect', anti_aliasing=True )\n",
        "            imgs.append(x)\n",
        "\n",
        "        if isinstance(gt, (list, tuple, np.ndarray)):\n",
        "            x = to_multichannel(gt[i])\n",
        "            x = resize(x, shape, preserve_range=True, mode='reflect', anti_aliasing=True )\n",
        "            imgs.append(x)\n",
        "\n",
        "        if is_colormap:\n",
        "            rescaled = outputs[i][:,:,0]\n",
        "            if is_rescale:\n",
        "                rescaled = rescaled - np.min(rescaled)\n",
        "                rescaled = rescaled / np.max(rescaled)\n",
        "            imgs.append(plasma(rescaled)[:,:,:3])\n",
        "        else:\n",
        "            imgs.append(to_multichannel(outputs[i]))\n",
        "\n",
        "        img_set = np.hstack(imgs)\n",
        "        all_images.append(img_set)\n",
        "\n",
        "    all_images = np.stack(all_images)\n",
        "    cv2.imwrite('outputs.png',all_images)\n",
        "    \n",
        "    return skimage.util.montage(all_images, multichannel=True, fill=(0,0,0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9PT7EErxjUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kyQcTipxmtN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv ../1.jpg test_data\n",
        "!mv ../15.jpg test_data\n",
        "!mv ../10.jpg test_data\n",
        "!mv ../2.jpg test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bO1hbH1Ax1Kp",
        "colab_type": "code",
        "outputId": "7d2cc627-690f-43fd-cd62-0dcf819f9a28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "glob.glob('test_data/*.jpg')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['test_data/15.jpg', 'test_data/10.jpg', 'test_data/2.jpg', 'test_data/1.jpg']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3pNJrkWvmrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def load_image(file_name):\n",
        "  x = np.clip(np.asarray(Image.open( file_name ), dtype=float) / 255, 0, 1)\n",
        "  x = cv2.resize(x, (640, 480))\n",
        "  #loaded_images.append(x)\n",
        "  return x #np.stack(loaded_images, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b172KxTu6E2",
        "colab_type": "code",
        "outputId": "7e8c9e45-5869-4e2e-b4eb-cd3afa9e1914",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Input images\n",
        "inputs = load_image('./test_data/10.jpg' )\n",
        "inputs = np.expand_dims(inputs, 0) # add batch size dimension\n",
        "print('\\nLoaded ({0}) images of size {1}.'.format(inputs.shape[0], inputs.shape[1:]))\n",
        "\n",
        "# Compute results\n",
        "outputs = predict(model, inputs)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded (1) images of size (480, 640, 3).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJlxU40hwJ-G",
        "colab_type": "code",
        "outputId": "9d9584fd-c95a-47bf-984b-1a10a3c07335",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "inputs.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 480, 640, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYWpQFR3wmPD",
        "colab_type": "code",
        "outputId": "a6fdf03f-2f78-4a07-b9a8-0642f8f2e3c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "outputs.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 240, 320, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgaYuUHywf4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "viz = display_images(outputs.copy(), inputs.copy())\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(viz)\n",
        "cv2.imwrite('outputs.png',viz)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2VkoQoyhNc3",
        "colab_type": "code",
        "outputId": "aad1220d-0862-4ee6-9adf-58e7bac5262f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "!wget https://s3-eu-west-1.amazonaws.com/densedepth/nyu_data.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-22 07:52:36--  https://s3-eu-west-1.amazonaws.com/densedepth/nyu_data.zip\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.97.67\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.97.67|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4399138715 (4.1G) [application/zip]\n",
            "Saving to: ‘nyu_data.zip’\n",
            "\n",
            "nyu_data.zip        100%[===================>]   4.10G  66.6MB/s    in 64s     \n",
            "\n",
            "2019-05-22 07:53:40 (65.8 MB/s) - ‘nyu_data.zip’ saved [4399138715/4399138715]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3hLLuPNiLR-",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Loss\n",
        "\n",
        "import torch\n",
        "from math import exp\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def gaussian(window_size, sigma):\n",
        "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
        "    return gauss/gauss.sum()\n",
        "\n",
        "def create_window(window_size, channel=1):\n",
        "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
        "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
        "    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
        "    return window\n",
        "\n",
        "def ssim(img1, img2, val_range, window_size=11, window=None, size_average=True, full=False):\n",
        "    L = val_range\n",
        "\n",
        "    padd = 0\n",
        "    (_, channel, height, width) = img1.size()\n",
        "    if window is None:\n",
        "        real_size = min(window_size, height, width)\n",
        "        window = create_window(real_size, channel=channel).to(img1.device)\n",
        "\n",
        "    mu1 = F.conv2d(img1, window, padding=padd, groups=channel)\n",
        "    mu2 = F.conv2d(img2, window, padding=padd, groups=channel)\n",
        "\n",
        "    mu1_sq = mu1.pow(2)\n",
        "    mu2_sq = mu2.pow(2)\n",
        "    mu1_mu2 = mu1 * mu2\n",
        "\n",
        "    sigma1_sq = F.conv2d(img1 * img1, window, padding=padd, groups=channel) - mu1_sq\n",
        "    sigma2_sq = F.conv2d(img2 * img2, window, padding=padd, groups=channel) - mu2_sq\n",
        "    sigma12 = F.conv2d(img1 * img2, window, padding=padd, groups=channel) - mu1_mu2\n",
        "\n",
        "    C1 = (0.01 * L) ** 2\n",
        "    C2 = (0.03 * L) ** 2\n",
        "\n",
        "    v1 = 2.0 * sigma12 + C2\n",
        "    v2 = sigma1_sq + sigma2_sq + C2\n",
        "    cs = torch.mean(v1 / v2)  # contrast sensitivity\n",
        "\n",
        "    ssim_map = ((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)\n",
        "\n",
        "    if size_average:\n",
        "        ret = ssim_map.mean()\n",
        "    else:\n",
        "        ret = ssim_map.mean(1).mean(1).mean(1)\n",
        "\n",
        "    if full:\n",
        "        return ret, cs\n",
        "\n",
        "    return ret\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oD3chE4IhNTZ",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Data\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import random\n",
        "\n",
        "def _is_pil_image(img):\n",
        "    return isinstance(img, Image.Image)\n",
        "\n",
        "def _is_numpy_image(img):\n",
        "    return isinstance(img, np.ndarray) and (img.ndim in {2, 3})\n",
        "\n",
        "class RandomHorizontalFlip(object):\n",
        "    def __call__(self, sample):\n",
        "        image, depth = sample['image'], sample['depth']\n",
        "\n",
        "        if not _is_pil_image(image):\n",
        "            raise TypeError(\n",
        "                'img should be PIL Image. Got {}'.format(type(image)))\n",
        "        if not _is_pil_image(depth):\n",
        "            raise TypeError(\n",
        "                'img should be PIL Image. Got {}'.format(type(depth)))\n",
        "\n",
        "        if random.random() < 0.5:\n",
        "            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "            depth = depth.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "        return {'image': image, 'depth': depth}\n",
        "\n",
        "class RandomChannelSwap(object):\n",
        "    def __init__(self, probability):\n",
        "        from itertools import permutations\n",
        "        self.probability = probability\n",
        "        self.indices = list(permutations(range(3), 3))\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, depth = sample['image'], sample['depth']\n",
        "        if not _is_pil_image(image): raise TypeError('img should be PIL Image. Got {}'.format(type(image)))\n",
        "        if not _is_pil_image(depth): raise TypeError('img should be PIL Image. Got {}'.format(type(depth)))\n",
        "        if random.random() < self.probability:\n",
        "            image = np.asarray(image)\n",
        "            image = Image.fromarray(image[...,list(self.indices[random.randint(0, len(self.indices) - 1)])])\n",
        "        return {'image': image, 'depth': depth}\n",
        "\n",
        "def loadZipToMem(zip_file):\n",
        "    # Load zip file into memory\n",
        "    print('Loading dataset zip file...', end='')\n",
        "    from zipfile import ZipFile\n",
        "    input_zip = ZipFile(zip_file)\n",
        "    data = {name: input_zip.read(name) for name in input_zip.namelist()}\n",
        "    nyu2_train = list((row.split(',') for row in (data['data/nyu2_train.csv']).decode(\"utf-8\").split('\\n') if len(row) > 0))\n",
        "\n",
        "    from sklearn.utils import shuffle\n",
        "    nyu2_train = shuffle(nyu2_train, random_state=0)\n",
        "\n",
        "    #if True: nyu2_train = nyu2_train[:40]\n",
        "\n",
        "    print('Loaded ({0}).'.format(len(nyu2_train)))\n",
        "    return data, nyu2_train\n",
        "\n",
        "class depthDatasetMemory(Dataset):\n",
        "    def __init__(self, data, nyu2_train, transform=None):\n",
        "        self.data, self.nyu_dataset = data, nyu2_train\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.nyu_dataset[idx]\n",
        "        image = Image.open( BytesIO(self.data[sample[0]]) )\n",
        "        depth = Image.open( BytesIO(self.data[sample[1]]) )\n",
        "        sample = {'image': image, 'depth': depth}\n",
        "        if self.transform: sample = self.transform(sample)\n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.nyu_dataset)\n",
        "\n",
        "class ToTensor(object):\n",
        "    def __init__(self,is_test=False):\n",
        "        self.is_test = is_test\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, depth = sample['image'], sample['depth']\n",
        "        \n",
        "        image = self.to_tensor(image)\n",
        "\n",
        "        depth = depth.resize((320, 240))\n",
        "\n",
        "        if self.is_test:\n",
        "            depth = self.to_tensor(depth).float() / 1000\n",
        "        else:            \n",
        "            depth = self.to_tensor(depth).float() * 1000\n",
        "        \n",
        "        # put in expected range\n",
        "        depth = torch.clamp(depth, 10, 1000)\n",
        "\n",
        "        return {'image': image, 'depth': depth}\n",
        "\n",
        "    def to_tensor(self, pic):\n",
        "        if not(_is_pil_image(pic) or _is_numpy_image(pic)):\n",
        "            raise TypeError(\n",
        "                'pic should be PIL Image or ndarray. Got {}'.format(type(pic)))\n",
        "\n",
        "        if isinstance(pic, np.ndarray):\n",
        "            img = torch.from_numpy(pic.transpose((2, 0, 1)))\n",
        "\n",
        "            return img.float().div(255)\n",
        "\n",
        "        # handle PIL Image\n",
        "        if pic.mode == 'I':\n",
        "            img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n",
        "        elif pic.mode == 'I;16':\n",
        "            img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n",
        "        else:\n",
        "            img = torch.ByteTensor(\n",
        "                torch.ByteStorage.from_buffer(pic.tobytes()))\n",
        "        # PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK\n",
        "        if pic.mode == 'YCbCr':\n",
        "            nchannel = 3\n",
        "        elif pic.mode == 'I;16':\n",
        "            nchannel = 1\n",
        "        else:\n",
        "            nchannel = len(pic.mode)\n",
        "        img = img.view(pic.size[1], pic.size[0], nchannel)\n",
        "\n",
        "        img = img.transpose(0, 1).transpose(0, 2).contiguous()\n",
        "        if isinstance(img, torch.ByteTensor):\n",
        "            return img.float().div(255)\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "def getNoTransform(is_test=False):\n",
        "    return transforms.Compose([\n",
        "        ToTensor(is_test=is_test)\n",
        "    ])\n",
        "\n",
        "def getDefaultTrainTransform():\n",
        "    return transforms.Compose([\n",
        "        RandomHorizontalFlip(),\n",
        "        RandomChannelSwap(0.5),\n",
        "        ToTensor()\n",
        "    ])\n",
        "\n",
        "def getTrainingTestingData(batch_size):\n",
        "    data, nyu2_train = loadZipToMem('nyu_data.zip')\n",
        "\n",
        "    transformed_training = depthDatasetMemory(data, nyu2_train, transform=getDefaultTrainTransform())\n",
        "    transformed_testing = depthDatasetMemory(data, nyu2_train, transform=getNoTransform())\n",
        "\n",
        "    return DataLoader(transformed_training, batch_size, shuffle=True), DataLoader(transformed_testing, batch_size, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJ-w_r5EiR_P",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Utils\n",
        "import matplotlib\n",
        "import matplotlib.cm\n",
        "import numpy as np\n",
        "\n",
        "def DepthNorm(depth, maxDepth=1000.0): \n",
        "    return maxDepth / depth\n",
        "\n",
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def colorize(value, vmin=10, vmax=1000, cmap='plasma'):\n",
        "    value = value.cpu().numpy()[0,:,:]\n",
        "\n",
        "    # normalize\n",
        "    vmin = value.min() if vmin is None else vmin\n",
        "    vmax = value.max() if vmax is None else vmax\n",
        "    if vmin!=vmax:\n",
        "        value = (value - vmin) / (vmax - vmin) # vmin..vmax\n",
        "    else:\n",
        "        # Avoid 0-division\n",
        "        value = value*0.\n",
        "    # squeeze last dim if it exists\n",
        "    #value = value.squeeze(axis=0)\n",
        "\n",
        "    cmapper = matplotlib.cm.get_cmap(cmap)\n",
        "    value = cmapper(value,bytes=True) # (nxmx4)\n",
        "\n",
        "    img = value[:,:,:3]\n",
        "\n",
        "    return img.transpose((2,0,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G775nBGiiEP4",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class UpSample(nn.Sequential):\n",
        "    def __init__(self, skip_input, output_features):\n",
        "        super(UpSample, self).__init__()        \n",
        "        self.convA = nn.Conv2d(skip_input, output_features, kernel_size=3, stride=1, padding=1)\n",
        "        self.leakyreluA = nn.LeakyReLU(0.2)\n",
        "        self.convB = nn.Conv2d(output_features, output_features, kernel_size=3, stride=1, padding=1)\n",
        "        self.leakyreluB = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self, x, concat_with):\n",
        "        up_x = F.interpolate(x, size=[concat_with.size(2), concat_with.size(3)], mode='bilinear', align_corners=True)\n",
        "        return self.leakyreluB( self.convB( self.leakyreluA(self.convA( torch.cat([up_x, concat_with], dim=1) ) ) )  )\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, num_features=2208, decoder_width = 0.5):\n",
        "        super(Decoder, self).__init__()\n",
        "        features = int(num_features * decoder_width)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(num_features, features, kernel_size=1, stride=1, padding=1)\n",
        "\n",
        "        self.up1 = UpSample(skip_input=features//1 + 384, output_features=features//2)\n",
        "        self.up2 = UpSample(skip_input=features//2 + 192, output_features=features//4)\n",
        "        self.up3 = UpSample(skip_input=features//4 +  96, output_features=features//8)\n",
        "        self.up4 = UpSample(skip_input=features//8 +  96, output_features=features//16)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(features//16, 1, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, features):\n",
        "        x_block0, x_block1, x_block2, x_block3, x_block4 = features[3], features[4], features[6], features[8], features[11]\n",
        "        x_d0 = self.conv2(x_block4)\n",
        "        x_d1 = self.up1(x_d0, x_block3)\n",
        "        x_d2 = self.up2(x_d1, x_block2)\n",
        "        x_d3 = self.up3(x_d2, x_block1)\n",
        "        x_d4 = self.up4(x_d3, x_block0)\n",
        "        return self.conv3(x_d4)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()       \n",
        "        import torchvision.models as models\n",
        "        self.original_model = models.densenet161( pretrained=True )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = [x]\n",
        "        for k, v in self.original_model.features._modules.items(): features.append( v(features[-1]) )\n",
        "        return features\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decoder( self.encoder(x) )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09iAvc2NsBUo",
        "colab_type": "code",
        "outputId": "89e160f5-e439-4b56-a85d-d5b1343d3a2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import multiprocessing\n",
        "pool = multiprocessing.Pool()\n",
        "pool._processes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVFN0etFsFiW",
        "colab_type": "code",
        "outputId": "76451540-36ea-42eb-95f3-6e0d0daa257b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "multiprocessing.cpu_count()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ4AqsZgjFqL",
        "colab_type": "code",
        "outputId": "42b47565-627d-4f4b-af30-b29d304c3269",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/57/2f0a46538295b8e7f09625da6dd24c23f9d0d7ef119ca1c33528660130d5/tensorboardX-1.7-py2.py3-none-any.whl (238kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 21.6MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 6.2MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 61kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 71kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81kB 10.3MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 92kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 102kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 122kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 133kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 143kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 153kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 163kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 174kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 184kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 194kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 204kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 215kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 225kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 235kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.16.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX) (41.0.1)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-1.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IxQ-CV7jVJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model()\n",
        "loss = ssim()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNL4xszpiZe7",
        "colab_type": "code",
        "outputId": "1b7ad45a-fff3-46cb-f4a5-1f26fddf03e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1583
        }
      },
      "source": [
        "import time\n",
        "import argparse\n",
        "import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils as utils\n",
        "import torchvision.utils as vutils    \n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "def main():\n",
        "    # Arguments\n",
        "#     parser = argparse.ArgumentParser(description='High Quality Monocular Depth Estimation via Transfer Learning')\n",
        "#     parser.add_argument('--epochs', default=20, type=int, help='number of total epochs to run')\n",
        "#     parser.add_argument('--lr', '--learning-rate', default=0.0001, type=float, help='initial learning rate')\n",
        "#     parser.add_argument('--bs', default=4, type=int, help='batch size')\n",
        "#     args = parser.parse_args()\n",
        "\n",
        "    lr = 0.0001\n",
        "    epochs = 20\n",
        "    bs = 4\n",
        "    \n",
        "    # Create model\n",
        "    model = Model().cuda()\n",
        "    print('Model created.')\n",
        "\n",
        "    # Training parameters\n",
        "    optimizer = torch.optim.Adam( model.parameters(),  )\n",
        "    batch_size = bs\n",
        "    prefix = 'densenet_' + str(batch_size)\n",
        "\n",
        "    # Load data\n",
        "    train_loader, test_loader = getTrainingTestingData(batch_size=batch_size)\n",
        "\n",
        "    # Logging\n",
        "    writer = SummaryWriter(comment='{}-lr{}-e{}-bs{}'.format(prefix, lr, epochs, bs), flush_secs=30)\n",
        "\n",
        "    # Loss\n",
        "    l1_criterion = nn.L1Loss()\n",
        "\n",
        "    # Start training...\n",
        "    for epoch in range(epochs):\n",
        "        batch_time = AverageMeter()\n",
        "        losses = AverageMeter()\n",
        "        N = len(train_loader)\n",
        "\n",
        "        # Switch to train mode\n",
        "        model.train()\n",
        "\n",
        "        end = time.time()\n",
        "\n",
        "        for i, sample_batched in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Prepare sample and target\n",
        "            image = torch.autograd.Variable(sample_batched['image'].cuda())\n",
        "            depth = torch.autograd.Variable(sample_batched['depth'].cuda(non_blocking=True))\n",
        "\n",
        "            # Normalize depth\n",
        "            depth_n = DepthNorm( depth )\n",
        "\n",
        "            # Predict\n",
        "            output = model(image)\n",
        "\n",
        "            # Compute the loss\n",
        "            l_depth = l1_criterion(output, depth_n)\n",
        "            l_ssim = torch.clamp((1 - ssim(output, depth_n, val_range = 1000.0 / 10.0)) * 0.5, 0, 1)\n",
        "\n",
        "            loss = (1.0 * l_ssim) + (0.1 * l_depth)\n",
        "\n",
        "            # Update step\n",
        "            losses.update(loss.data.item(), image.size(0))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "            eta = str(datetime.timedelta(seconds=int(batch_time.val*(N - i))))\n",
        "        \n",
        "            # Log progress\n",
        "            niter = epoch*N+i\n",
        "            if i % 5 == 0:\n",
        "                # Print to console\n",
        "                print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                'Time {batch_time.val:.3f} ({batch_time.sum:.3f})\\t'\n",
        "                'ETA {eta}\\t'\n",
        "                'Loss {loss.val:.4f} ({loss.avg:.4f})'\n",
        "                .format(epoch, i, N, batch_time=batch_time, loss=losses, eta=eta))\n",
        "\n",
        "                # Log to tensorboard\n",
        "                writer.add_scalar('Train/Loss', losses.val, niter)\n",
        "\n",
        "            if i % 300 == 0:\n",
        "                LogProgress(model, writer, test_loader, niter)\n",
        "\n",
        "        # Record epoch's intermediate results\n",
        "        LogProgress(model, writer, test_loader, niter)\n",
        "        writer.add_scalar('Train/Loss.avg', losses.avg, epoch)\n",
        "\n",
        "def LogProgress(model, writer, test_loader, epoch):\n",
        "    model.eval()\n",
        "    sequential = test_loader\n",
        "    sample_batched = next(iter(sequential))\n",
        "    image = torch.autograd.Variable(sample_batched['image'].cuda())\n",
        "    depth = torch.autograd.Variable(sample_batched['depth'].cuda(non_blocking=True))\n",
        "    if epoch == 0: writer.add_image('Train.1.Image', vutils.make_grid(image.data, nrow=6, normalize=True), epoch)\n",
        "    if epoch == 0: writer.add_image('Train.2.Depth', colorize(vutils.make_grid(depth.data, nrow=6, normalize=False)), epoch)\n",
        "    output = DepthNorm( model(image) )\n",
        "    writer.add_image('Train.3.Ours', colorize(vutils.make_grid(output.data, nrow=6, normalize=False)), epoch)\n",
        "    writer.add_image('Train.3.Diff', colorize(vutils.make_grid(torch.abs(output-depth).data, nrow=6, normalize=False)), epoch)\n",
        "    del image\n",
        "    del depth\n",
        "    del output\n",
        "\n",
        "main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/densenet161-8d451a50.pth\" to /root/.cache/torch/checkpoints/densenet161-8d451a50.pth\n",
            "100%|██████████| 115730790/115730790 [00:00<00:00, 164320765.94it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model created.\n",
            "Loading dataset zip file...Loaded (50688).\n",
            "Epoch: [0][0/12672]\tTime 1.273 (1.273)\tETA 4:28:48\tLoss 0.8512 (0.8512)\n",
            "Epoch: [0][5/12672]\tTime 0.849 (6.717)\tETA 2:59:17\tLoss 1.1003 (2.4798)\n",
            "Epoch: [0][10/12672]\tTime 0.838 (10.922)\tETA 2:56:53\tLoss 0.6818 (1.9327)\n",
            "Epoch: [0][15/12672]\tTime 0.853 (15.143)\tETA 2:59:53\tLoss 1.9527 (1.8694)\n",
            "Epoch: [0][20/12672]\tTime 0.850 (19.377)\tETA 2:59:12\tLoss 0.5870 (9.6349)\n",
            "Epoch: [0][25/12672]\tTime 0.840 (23.581)\tETA 2:57:01\tLoss 3.2345 (8.2561)\n",
            "Epoch: [0][30/12672]\tTime 0.850 (27.855)\tETA 2:59:01\tLoss 9.6748 (7.4479)\n",
            "Epoch: [0][35/12672]\tTime 0.884 (32.208)\tETA 3:06:13\tLoss 2.8011 (6.5837)\n",
            "Epoch: [0][40/12672]\tTime 0.858 (36.587)\tETA 3:00:34\tLoss 2.5015 (6.0324)\n",
            "Epoch: [0][45/12672]\tTime 0.856 (40.920)\tETA 3:00:14\tLoss 0.3876 (5.4822)\n",
            "Epoch: [0][50/12672]\tTime 0.889 (45.306)\tETA 3:07:04\tLoss 0.2219 (4.9958)\n",
            "Epoch: [0][55/12672]\tTime 0.881 (49.696)\tETA 3:05:17\tLoss 0.3111 (4.5726)\n",
            "Epoch: [0][60/12672]\tTime 0.914 (54.127)\tETA 3:12:04\tLoss 0.1756 (4.2218)\n",
            "Epoch: [0][65/12672]\tTime 0.916 (58.703)\tETA 3:12:26\tLoss 0.1824 (3.9237)\n",
            "Epoch: [0][70/12672]\tTime 0.913 (63.275)\tETA 3:11:46\tLoss 0.3153 (3.6676)\n",
            "Epoch: [0][75/12672]\tTime 0.911 (67.806)\tETA 3:11:20\tLoss 0.1782 (3.4405)\n",
            "Epoch: [0][80/12672]\tTime 0.913 (72.332)\tETA 3:11:42\tLoss 0.2543 (3.2424)\n",
            "Epoch: [0][85/12672]\tTime 0.913 (76.850)\tETA 3:11:31\tLoss 0.3284 (3.0701)\n",
            "Epoch: [0][90/12672]\tTime 0.909 (81.390)\tETA 3:10:36\tLoss 0.2524 (2.9158)\n",
            "Epoch: [0][95/12672]\tTime 0.914 (85.911)\tETA 3:11:33\tLoss 0.1964 (2.7759)\n",
            "Epoch: [0][100/12672]\tTime 0.911 (90.495)\tETA 3:10:55\tLoss 0.3470 (2.6568)\n",
            "Epoch: [0][105/12672]\tTime 0.912 (95.012)\tETA 3:10:59\tLoss 0.2105 (2.5428)\n",
            "Epoch: [0][110/12672]\tTime 0.898 (99.485)\tETA 3:08:00\tLoss 0.2961 (2.4418)\n",
            "Epoch: [0][115/12672]\tTime 0.896 (103.975)\tETA 3:07:32\tLoss 0.2773 (2.3476)\n",
            "Epoch: [0][120/12672]\tTime 0.904 (108.455)\tETA 3:09:07\tLoss 0.2053 (2.2602)\n",
            "Epoch: [0][125/12672]\tTime 0.909 (113.025)\tETA 3:10:10\tLoss 0.1944 (2.1800)\n",
            "Epoch: [0][130/12672]\tTime 0.885 (117.494)\tETA 3:04:57\tLoss 0.1523 (2.1049)\n",
            "Epoch: [0][135/12672]\tTime 0.881 (121.941)\tETA 3:04:07\tLoss 0.2426 (2.0358)\n",
            "Epoch: [0][140/12672]\tTime 0.889 (126.377)\tETA 3:05:44\tLoss 0.2514 (1.9737)\n",
            "Epoch: [0][145/12672]\tTime 0.888 (130.789)\tETA 3:05:28\tLoss 0.1625 (1.9132)\n",
            "Epoch: [0][150/12672]\tTime 0.894 (135.234)\tETA 3:06:35\tLoss 0.3043 (1.8611)\n",
            "Epoch: [0][155/12672]\tTime 0.890 (139.697)\tETA 3:05:44\tLoss 0.2435 (1.8093)\n",
            "Epoch: [0][160/12672]\tTime 0.901 (144.135)\tETA 3:07:48\tLoss 0.3208 (1.7610)\n",
            "Epoch: [0][165/12672]\tTime 0.897 (148.599)\tETA 3:06:56\tLoss 0.2410 (1.7143)\n",
            "Epoch: [0][170/12672]\tTime 0.897 (153.081)\tETA 3:06:48\tLoss 0.3598 (1.6718)\n",
            "Epoch: [0][175/12672]\tTime 0.891 (157.545)\tETA 3:05:40\tLoss 0.1701 (1.6291)\n",
            "Epoch: [0][180/12672]\tTime 0.897 (162.024)\tETA 3:06:41\tLoss 0.2328 (1.5905)\n",
            "Epoch: [0][185/12672]\tTime 0.896 (166.504)\tETA 3:06:30\tLoss 0.2399 (1.5533)\n",
            "Epoch: [0][190/12672]\tTime 0.900 (170.983)\tETA 3:07:12\tLoss 0.2326 (1.5193)\n",
            "Epoch: [0][195/12672]\tTime 0.891 (175.447)\tETA 3:05:13\tLoss 0.1430 (1.4860)\n",
            "Epoch: [0][200/12672]\tTime 0.898 (179.907)\tETA 3:06:43\tLoss 0.1798 (1.4544)\n",
            "Epoch: [0][205/12672]\tTime 0.899 (184.388)\tETA 3:06:51\tLoss 0.2879 (1.4265)\n",
            "Epoch: [0][210/12672]\tTime 0.897 (188.887)\tETA 3:06:23\tLoss 0.2030 (1.3987)\n",
            "Epoch: [0][215/12672]\tTime 0.906 (193.438)\tETA 3:08:08\tLoss 0.1439 (1.3713)\n",
            "Epoch: [0][220/12672]\tTime 0.876 (197.929)\tETA 3:01:48\tLoss 0.2652 (1.3456)\n",
            "Epoch: [0][225/12672]\tTime 0.900 (202.400)\tETA 3:06:41\tLoss 0.3160 (1.3206)\n",
            "Epoch: [0][230/12672]\tTime 0.887 (206.843)\tETA 3:03:59\tLoss 0.1317 (1.2959)\n",
            "Epoch: [0][235/12672]\tTime 0.889 (211.300)\tETA 3:04:11\tLoss 0.1943 (1.2730)\n",
            "Epoch: [0][240/12672]\tTime 0.879 (215.740)\tETA 3:02:02\tLoss 0.1738 (1.2512)\n",
            "Epoch: [0][245/12672]\tTime 0.891 (220.194)\tETA 3:04:29\tLoss 0.2194 (1.2296)\n",
            "Epoch: [0][250/12672]\tTime 0.882 (224.644)\tETA 3:02:31\tLoss 0.2495 (1.2098)\n",
            "Epoch: [0][255/12672]\tTime 0.881 (229.082)\tETA 3:02:25\tLoss 0.1719 (1.1898)\n",
            "Epoch: [0][260/12672]\tTime 0.890 (233.531)\tETA 3:04:05\tLoss 0.2067 (1.1715)\n",
            "Epoch: [0][265/12672]\tTime 0.888 (237.981)\tETA 3:03:33\tLoss 0.1647 (1.1530)\n",
            "Epoch: [0][270/12672]\tTime 0.886 (242.444)\tETA 3:03:07\tLoss 0.2217 (1.1365)\n",
            "Epoch: [0][275/12672]\tTime 0.889 (246.904)\tETA 3:03:35\tLoss 0.2749 (1.1204)\n",
            "Epoch: [0][280/12672]\tTime 0.898 (251.360)\tETA 3:05:27\tLoss 0.2731 (1.1041)\n",
            "Epoch: [0][285/12672]\tTime 0.898 (255.836)\tETA 3:05:17\tLoss 0.1122 (1.0880)\n",
            "Epoch: [0][290/12672]\tTime 0.891 (260.285)\tETA 3:03:49\tLoss 0.2126 (1.0722)\n",
            "Epoch: [0][295/12672]\tTime 0.894 (264.736)\tETA 3:04:29\tLoss 0.2329 (1.0576)\n",
            "Epoch: [0][300/12672]\tTime 0.906 (269.230)\tETA 3:06:54\tLoss 0.1959 (1.0440)\n",
            "Epoch: [0][305/12672]\tTime 0.903 (274.236)\tETA 3:06:04\tLoss 0.2540 (1.0301)\n",
            "Epoch: [0][310/12672]\tTime 0.896 (278.773)\tETA 3:04:38\tLoss 0.2422 (1.0176)\n",
            "Epoch: [0][315/12672]\tTime 0.892 (283.211)\tETA 3:03:36\tLoss 0.1969 (1.0050)\n",
            "Epoch: [0][320/12672]\tTime 0.896 (287.664)\tETA 3:04:33\tLoss 0.2613 (0.9925)\n",
            "Epoch: [0][325/12672]\tTime 0.885 (292.125)\tETA 3:02:07\tLoss 0.1799 (0.9811)\n",
            "Epoch: [0][330/12672]\tTime 0.888 (296.579)\tETA 3:02:41\tLoss 0.2560 (0.9696)\n",
            "Epoch: [0][335/12672]\tTime 0.883 (301.051)\tETA 3:01:34\tLoss 0.2258 (0.9584)\n",
            "Epoch: [0][340/12672]\tTime 0.902 (305.528)\tETA 3:05:17\tLoss 0.1150 (0.9469)\n",
            "Epoch: [0][345/12672]\tTime 0.892 (309.996)\tETA 3:03:14\tLoss 0.3801 (0.9394)\n",
            "Epoch: [0][350/12672]\tTime 0.896 (314.476)\tETA 3:04:02\tLoss 0.2663 (0.9298)\n",
            "Epoch: [0][355/12672]\tTime 0.903 (318.957)\tETA 3:05:19\tLoss 0.1843 (0.9200)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-ba1e3960184f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-ba1e3960184f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;31m# Update step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nPJLGprjDt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}