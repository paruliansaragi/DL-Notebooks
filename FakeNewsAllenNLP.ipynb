{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FakeNewsAllenNLP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paruliansaragi/DL-Notebooks/blob/master/FakeNewsAllenNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLHIG0LTUMHb",
        "colab_type": "code",
        "outputId": "50756fd7-9c45-4822-bf6f-32f52bd65d34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2915
        }
      },
      "source": [
        "!pip install allennlp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting allennlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/c8/10342a6068a8d156a5947e03c95525d559e71ad62de0f2585ab922e14533/allennlp-0.8.3-py3-none-any.whl (5.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.6MB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.0.1.post2)\n",
            "Collecting parsimonious>=0.8.0 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 20.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.3.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.9.128)\n",
            "Collecting moto>=1.3.4 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/40/cec89fa5c13108eb1c8de435633f8b7639e0e43fcbcdc8ac52633efeeabe/moto-1.3.7-py2.py3-none-any.whl (552kB)\n",
            "\u001b[K    100% |████████████████████████████████| 552kB 23.2MB/s \n",
            "\u001b[?25hCollecting overrides (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/de/55/3100c6d14c1ed177492fcf8f07c4a7d2d6c996c0a7fc6a9a0a41308e7eec/overrides-1.9.tar.gz\n",
            "Collecting awscli>=1.11.91 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/41/f03fa1b10c7619262da75d34ce93067c9f0dc274dbd482200250319d9bef/awscli-1.16.140-py2.py3-none-any.whl (1.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.5MB 15.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2018.9)\n",
            "Collecting tensorboardX>=1.2 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/76/89dd44458eb976347e5a6e75eb79fecf8facd46c1ce259bad54e0044ea35/tensorboardX-1.6-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 28.5MB/s \n",
            "\u001b[?25hCollecting word2number>=1.1 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Collecting numpydoc>=0.8.0 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/95/a8/b4706a6270f0475541c5c1ee3373c7a3b793936ec1f517f1a1dab4f896c0/numpydoc-0.8.0.tar.gz\n",
            "Collecting ftfy (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/86/df789c5834f15ae1ca53a8d4c1fc4788676c2e32112f6a786f2625d9c6e6/ftfy-5.5.1-py3-none-any.whl (43kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 20.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.8.0)\n",
            "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.0.2)\n",
            "Collecting responses>=0.7 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/5a/b887e89925f1de7890ef298a74438371ed4ed29b33def9e6d02dc6036fd8/responses-0.10.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.0)\n",
            "Collecting conllu==0.11 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/d4/2c/856344d9b69baf5b374c395b4286626181a80f0c2b2f704914d18a1cea47/conllu-0.11-py2.py3-none-any.whl\n",
            "Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.6)\n",
            "Collecting unidecode (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/39/53096f9217b057cb049fe872b7fc7ce799a1a89b76cf917d9639e7a558b5/Unidecode-1.0.23-py2.py3-none-any.whl (237kB)\n",
            "\u001b[K    100% |████████████████████████████████| 245kB 28.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.0)\n",
            "Collecting jsonnet>=0.10.0; sys_platform != \"win32\" (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/dc/3abd3971869a741d7acdba166d71d4f9366b6b53028dfd56f95de356af0f/jsonnet-0.12.1.tar.gz (240kB)\n",
            "\u001b[K    100% |████████████████████████████████| 245kB 29.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.14.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.20.3)\n",
            "Requirement already satisfied: spacy<2.2,>=2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.0.18)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.18.4)\n",
            "Collecting pytorch-pretrained-bert>=0.6.0 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/3c/d5fa084dd3a82ffc645aba78c417e6072ff48552e3301b1fa3bd711e03d4/pytorch_pretrained_bert-0.6.1-py3-none-any.whl (114kB)\n",
            "\u001b[K    100% |████████████████████████████████| 122kB 28.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.0.3)\n",
            "Collecting flaky (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/02/42/cca66659a786567c8af98587d66d75e7d2b6e65662f8daab75db708ac35b/flaky-3.5.3-py2.py3-none-any.whl\n",
            "Collecting flask-cors>=3.0.7 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/65/cb/683f71ff8daa3aea0a5cbb276074de39f9ab66d3fbb8ad5efb5bb83e90d2/Flask_Cors-3.0.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.28.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->allennlp) (1.11.0)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.128 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (1.12.128)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.2.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.9.4)\n",
            "Collecting python-jose<3.0.0 (from moto>=1.3.4->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/bf/5c/5fa238c0c5b0656994b52721dd8b1d7bf52ebd8786518dde794f44de86b6/python_jose-2.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Jinja2>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->allennlp) (2.10)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->allennlp) (2.5.3)\n",
            "Collecting xmltodict (from moto>=1.3.4->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/28/fd/30d5c1d3ac29ce229f6bdc40bbc20b28f716e8b363140c26eff19122d8a5/xmltodict-0.12.0-py2.py3-none-any.whl\n",
            "Collecting cryptography>=2.3.0 (from moto>=1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/12/b0409a94dad366d98a8eee2a77678c7a73aafd8c0e4b835abea634ea3896/cryptography-2.6.1-cp34-abi3-manylinux1_x86_64.whl (2.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.3MB 13.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto>=2.36.0 in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->allennlp) (2.49.0)\n",
            "Requirement already satisfied: werkzeug in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->allennlp) (0.15.2)\n",
            "Collecting pyaml (from moto>=1.3.4->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/c5/e1/1523fb1dab744e2c6b1f02446f2139a78726c18c062a8ddd53875abb20f8/pyaml-18.11.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: mock in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->allennlp) (2.0.0)\n",
            "Collecting aws-xray-sdk<0.96,>=0.93 (from moto>=1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/a5/da7887285564f9e0ae5cd25a453cca36e2cd43d8ccc9effde260b4d80904/aws_xray_sdk-0.95-py2.py3-none-any.whl (52kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 24.6MB/s \n",
            "\u001b[?25hCollecting docker>=2.5.1 (from moto>=1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/68/c3afca1a5aa8d2997ec3b8ee822a4d752cf85907b321f07ea86888545152/docker-3.7.2-py2.py3-none-any.whl (134kB)\n",
            "\u001b[K    100% |████████████████████████████████| 143kB 30.6MB/s \n",
            "\u001b[?25hCollecting jsondiff==1.1.1 (from moto>=1.3.4->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/bd/5f/13e28a2f9abeda2ffb3f44f2f809b01b52bc02cdb63816e05b8c9cbbdfc5/jsondiff-1.1.1.tar.gz\n",
            "Requirement already satisfied: PyYAML<=3.13,>=3.10 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (3.13)\n",
            "Collecting rsa<=3.5.0,>=3.1.2 (from awscli>=1.11.91->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/ae/baedc9cb175552e95f3395c43055a6a5e125ae4d48a1d7a924baca83e92e/rsa-3.4.2-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 9.5MB/s \n",
            "\u001b[?25hCollecting colorama<=0.3.9,>=0.2.5 (from awscli>=1.11.91->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/db/c8/7dcf9dbcb22429512708fe3a547f8b6101c0d02137acbd892505aee57adf/colorama-0.3.9-py2.py3-none-any.whl\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (0.14)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.7.1)\n",
            "Requirement already satisfied: sphinx>=1.2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (1.8.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.7)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (7.0)\n",
            "Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp) (0.4.15)\n",
            "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->allennlp) (1.35)\n",
            "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->allennlp) (0.2.9)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->allennlp) (0.9.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->allennlp) (2.0.2)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->allennlp) (2.0.1)\n",
            "Requirement already satisfied: thinc<6.13.0,>=6.12.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->allennlp) (6.12.1)\n",
            "Requirement already satisfied: regex==2018.01.10 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->allennlp) (2018.1.10)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->allennlp) (1.0.2)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (40.9.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.1.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (7.0.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.0)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2019.3.9)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (1.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.3.1)\n",
            "Collecting pycryptodome<4.0.0,>=3.3.1 (from python-jose<3.0.0->moto>=1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/fc/b09816d7b2d79d6454f75b40def94a89ed785d8d8d07840563f1084c6ecd/pycryptodome-3.8.1-cp36-cp36m-manylinux1_x86_64.whl (9.7MB)\n",
            "\u001b[K    100% |████████████████████████████████| 9.7MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: future<1.0 in /usr/local/lib/python3.6/dist-packages (from python-jose<3.0.0->moto>=1.3.4->allennlp) (0.16.0)\n",
            "Collecting ecdsa<1.0 (from python-jose<3.0.0->moto>=1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/f4/73669d51825516ce8c43b816c0a6b64cd6eb71d08b99820c00792cb42222/ecdsa-0.13-py2.py3-none-any.whl (86kB)\n",
            "\u001b[K    100% |████████████████████████████████| 92kB 27.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.7.3->moto>=1.3.4->allennlp) (1.1.1)\n",
            "Collecting asn1crypto>=0.21.0 (from cryptography>=2.3.0->moto>=1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/cd/35485615f45f30a510576f1a56d1e0a7ad7bd8ab5ed7cdc600ef7cd06222/asn1crypto-0.24.0-py2.py3-none-any.whl (101kB)\n",
            "\u001b[K    100% |████████████████████████████████| 102kB 29.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.3.0->moto>=1.3.4->allennlp) (1.12.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.6/dist-packages (from mock->moto>=1.3.4->allennlp) (5.1.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from aws-xray-sdk<0.96,>=0.93->moto>=1.3.4->allennlp) (1.10.11)\n",
            "Collecting jsonpickle (from aws-xray-sdk<0.96,>=0.93->moto>=1.3.4->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/dc/12/8c44eabb501e2bc0aec0dd152b328074d98a50968d3a02be28f6037f0c6a/jsonpickle-1.1-py2.py3-none-any.whl\n",
            "Collecting docker-pycreds>=0.4.0 (from docker>=2.5.1->moto>=1.3.4->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting websocket-client>=0.32.0 (from docker>=2.5.1->moto>=1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/19/44753eab1fdb50770ac69605527e8859468f3c0fd7dc5a76dd9c4dbd7906/websocket_client-0.56.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K    100% |████████████████████████████████| 204kB 31.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<=3.5.0,>=3.1.2->awscli>=1.11.91->allennlp) (0.4.5)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (0.7.12)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (19.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (2.1.3)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (1.1.0)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (1.2.1)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (2.6.0)\n",
            "Requirement already satisfied: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy<2.2,>=2.0->allennlp) (0.4.3.2)\n",
            "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy<2.2,>=2.0->allennlp) (0.9.0.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.3.0->moto>=1.3.4->allennlp) (2.19)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy<2.2,>=2.0->allennlp) (0.9.0)\n",
            "Building wheels for collected packages: parsimonious, overrides, word2number, numpydoc, jsonnet, jsondiff\n",
            "  Building wheel for parsimonious (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/8d/52/86/e5a83b1797e7d263b458d2334edd2704c78508b3eea9323718\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "  Building wheel for numpydoc (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ea/55/7f/3e25d754760ccd62d6796e5b2cfe25629346f52ea00753d549\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/f0/47/51/a178b15274ed0db775a1ae9c799ce31e511609c3ab75a7dec5\n",
            "  Building wheel for jsondiff (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/68/08/07/69d839606fb7fdc778fa86476abc0a864693d45969a0c1936c\n",
            "Successfully built parsimonious overrides word2number numpydoc jsonnet jsondiff\n",
            "\u001b[31mawscli 1.16.140 has requirement botocore==1.12.130, but you'll have botocore 1.12.128 which is incompatible.\u001b[0m\n",
            "Installing collected packages: parsimonious, pycryptodome, ecdsa, python-jose, xmltodict, asn1crypto, cryptography, pyaml, jsonpickle, aws-xray-sdk, docker-pycreds, websocket-client, docker, jsondiff, responses, moto, overrides, rsa, colorama, awscli, tensorboardX, word2number, numpydoc, ftfy, conllu, unidecode, jsonnet, pytorch-pretrained-bert, flaky, flask-cors, allennlp\n",
            "  Found existing installation: rsa 4.0\n",
            "    Uninstalling rsa-4.0:\n",
            "      Successfully uninstalled rsa-4.0\n",
            "Successfully installed allennlp-0.8.3 asn1crypto-0.24.0 aws-xray-sdk-0.95 awscli-1.16.140 colorama-0.3.9 conllu-0.11 cryptography-2.6.1 docker-3.7.2 docker-pycreds-0.4.0 ecdsa-0.13 flaky-3.5.3 flask-cors-3.0.7 ftfy-5.5.1 jsondiff-1.1.1 jsonnet-0.12.1 jsonpickle-1.1 moto-1.3.7 numpydoc-0.8.0 overrides-1.9 parsimonious-0.8.1 pyaml-18.11.0 pycryptodome-3.8.1 python-jose-2.0.2 pytorch-pretrained-bert-0.6.1 responses-0.10.6 rsa-3.4.2 tensorboardX-1.6 unidecode-1.0.23 websocket-client-0.56.0 word2number-1.1 xmltodict-0.12.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "rsa"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIogNsTQUlpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWjv0-fJU7bU",
        "colab_type": "text"
      },
      "source": [
        "##AllenNLP\n",
        "\n",
        "DatasetReader: Extracts necessary information from data into a list of Instance objects\n",
        "\n",
        "Model: The model to be trained (with some caveats!)\n",
        "\n",
        "Iterator: Batches the data\n",
        "\n",
        "Trainer: Handles training and metric recording\n",
        "\n",
        "(Predictor: Generates predictions from raw strings)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeHuu4KjUsmZ",
        "colab_type": "code",
        "outputId": "4fefa331-60cc-49ec-df8b-96c6c59478b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from pathlib import Path\n",
        "from typing import *\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from functools import partial\n",
        "from overrides import overrides\n",
        "\n",
        "from allennlp.data import Instance\n",
        "from allennlp.data.token_indexers import TokenIndexer\n",
        "from allennlp.data.tokenizers import Token\n",
        "from allennlp.nn import util as nn_util"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObGCuDFSUuWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Config(dict):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        for k, v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "    \n",
        "    def set(self, key, val):\n",
        "        self[key] = val\n",
        "        setattr(self, key, val)\n",
        "        \n",
        "config = Config(\n",
        "    testing=True,\n",
        "    seed=1,\n",
        "    batch_size=64,\n",
        "    lr=3e-4,\n",
        "    epochs=20,\n",
        "    hidden_sz=64,\n",
        "    max_seq_len=100, # necessary to limit memory usage\n",
        "    max_vocab_size=100000,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR3eSP5QVDXq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "USE_GPU = torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBGD-vUeVFrg",
        "colab_type": "code",
        "outputId": "59d0b25d-6b74-4264-b4b7-fa8d2f0ba771",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!ls ~/.kaggle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcK4UUXKVPnX",
        "colab_type": "code",
        "outputId": "5601e812-1dfc-47b9-fe33-1a64c565a106",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!ls -l ~/.kaggle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 4\n",
            "-rw------- 1 root root 70 Apr  5 11:49 kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SlH1RqfVRoU",
        "colab_type": "code",
        "outputId": "e7f966d6-5442-44b7-93cc-f60f334e43d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        }
      },
      "source": [
        "!kaggle competitions download -c fake-news"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading train.csv to /content\n",
            " 95% 89.0M/94.1M [00:01<00:00, 43.1MB/s]\n",
            "100% 94.1M/94.1M [00:01<00:00, 62.9MB/s]\n",
            "Downloading test.csv to /content\n",
            " 71% 17.0M/24.0M [00:00<00:00, 19.4MB/s]\n",
            "100% 24.0M/24.0M [00:00<00:00, 40.8MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtGfcqSIVV9L",
        "colab_type": "code",
        "outputId": "6849f642-1a6b-42e6-adc4-b4c2357a3870",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "torch.manual_seed(config.seed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f8b56a9d4f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zzhpmMMVe0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_ROOT = './'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRucbcZcVjDd",
        "colab_type": "text"
      },
      "source": [
        "The DatasetReader is responsible for the following:\n",
        "\n",
        "Reading the data from disk\n",
        "\n",
        "Extracting relevant information from the data\n",
        "\n",
        "Converting the data into a list of Instances (we’ll discuss Instances in a second)\n",
        "\n",
        "\n",
        "You may be surprised to hear that there is no Dataset class in AllenNLP, unlike traditional PyTorch. DatasetReaders are different from Datasets in that they are not a collection of data themselves: they are a schema for converting data on disk into lists of instances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UccAWjLrVgsg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.data.vocabulary import Vocabulary\n",
        "from allennlp.data.dataset_readers import DatasetReader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKLR6vYgV13J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_cols = ['label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IUo1zxzViWY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.data.fields import TextField, MetadataField, ArrayField\n",
        "\n",
        "class FNDDatasetReader(DatasetReader):\n",
        "    def __init__(self, tokenizer: Callable[[str], List[str]]=lambda x: x.split(),\n",
        "                 token_indexers: Dict[str, TokenIndexer] = None,\n",
        "                 max_seq_len: Optional[int]=config.max_seq_len) -> None:\n",
        "        super().__init__(lazy=False)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.test = test\n",
        "    \n",
        "    '''The second central method for the DatasetReader is the text_to_instance method. \n",
        "    This method is slightly misleading: it handles not only text but also labels, metadata, \n",
        "    and anything else that your model will need later on.'''\n",
        "    @overrides\n",
        "    def text_to_instance(self, tokens: List[Token], id: str=None,\n",
        "                         labels: np.ndarray=None) -> Instance:\n",
        "        sentence_field = TextField(tokens, self.token_indexers)\n",
        "        fields = {\"tokens\": sentence_field}\n",
        "        \n",
        "        id_field = MetadataField(id)\n",
        "        fields[\"id\"] = id_field\n",
        "        \n",
        "        if labels is None:\n",
        "            labels = np.zeros(len(label_cols))\n",
        "        label_field = ArrayField(array=labels)\n",
        "        fields[\"label\"] = label_field\n",
        "\n",
        "        return Instance(fields)\n",
        "    \n",
        "    '''As you will probably already have guessed, the _read method is responsible \n",
        "    for 1: reading the data from disk into memory.\n",
        "    \n",
        "    The essence of this method is simple: take the data for a single example \n",
        "    and pack it into an Instance object. Here, we’re passing the labels and ids of each example.\n",
        "    \n",
        "    all you need to know about them in practice is that they are instantiated \n",
        "    with a dictionary mapping field names to “Field”s, which are our next topic.\n",
        "    '''\n",
        "    @overrides\n",
        "    def _read(self, df) -> Iterator[Instance]:\n",
        "        \n",
        "        if config.testing: df = df.head(1000)\n",
        "        for i, row in df.iterrows():\n",
        "          if 'label' in df:\n",
        "            yield self.text_to_instance(\n",
        "                [Token(x) for x in self.tokenizer(row[\"comment_text\"])],\n",
        "                row[\"id\"], row[label_cols].values,\n",
        "            )\n",
        "          else:\n",
        "            yield self.text_to_instance(\n",
        "                [Token(x) for x in self.tokenizer(row[\"comment_text\"])],\n",
        "                row[\"id\"],\n",
        "            )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iVzTNhDWmeb",
        "colab_type": "text"
      },
      "source": [
        "###Field\n",
        "\n",
        "Field objects in AllenNLP correspond to inputs to a model or fields in a batch that is fed into a model, depending on how you look at it. For each Field, the model will receive a single input (you can take a look at the forward method in the BaselineModel class in the example code to confirm). Each field handles converting the data into tensors, so if you need to do some fancy processing on your data when converting it into tensor form, you should probably write your own custom Field class.\n",
        "\n",
        "Types of Field:\n",
        "\n",
        "###TextField\n",
        "\n",
        "it converts a sequence of tokens into integers. Be careful here though, since this is all the TextField does. It doesn’t clean the text, tokenize the text, etc.. You’ll need to do that yourself.\n",
        "\n",
        "The TextField takes an additional argument on init: the token indexer. Though the TextField handles converting tokens to integers, you need to tell it how to do this. Why? Because you might want to use a character level model instead of a word-level model or do some even funkier splitting of tokens (like splitting on morphemes). Instead of specifying these attributes in the TextField, AllenNLP has you pass a separate object that handles these decisions instead. This is the principle of composition, and you’ll see how this makes modifying your code easy later.\n",
        "\n",
        "For now, we’ll use a simple word-level model so we use the standard SingleIdTokenIndexer.\n",
        "\n",
        "DatasetReaders read data from disk and return a list of Instances. Instances are composed of Fields which specify both the data in the instance and how to process it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdh7gmOyVzoW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
        "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
        "\n",
        "# the token indexer is responsible for mapping tokens to integers\n",
        "token_indexer = SingleIdTokenIndexer()\n",
        "\n",
        "def tokenizer(x: str):\n",
        "    return [w.text for w in\n",
        "            SpacyWordSplitter(language='en_core_web_sm', \n",
        "                              pos_tags=False).split_words(x)[:config.max_seq_len]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASAw4U0bXVVj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reader = FNDDatasetReader(\n",
        "    tokenizer=tokenizer,\n",
        "    token_indexers={\"tokens\": token_indexer}\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgCLxAZBX03V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "test = test.drop(['title', 'author'], axis=1)\n",
        "train = train.drop(['title', 'author'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f8CrcQ0Ya2X",
        "colab_type": "code",
        "outputId": "fbb176d5-2569-427d-85df-7b7392d7083c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20800</td>\n",
              "      <td>PALO ALTO, Calif.  —   After years of scorning...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20801</td>\n",
              "      <td>Russian warships ready to strike terrorists ne...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20802</td>\n",
              "      <td>Videos #NoDAPL: Native American Leaders Vow to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20803</td>\n",
              "      <td>If at first you don’t succeed, try a different...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20804</td>\n",
              "      <td>42 mins ago 1 Views 0 Comments 0 Likes 'For th...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id                                               text\n",
              "0  20800  PALO ALTO, Calif.  —   After years of scorning...\n",
              "1  20801  Russian warships ready to strike terrorists ne...\n",
              "2  20802  Videos #NoDAPL: Native American Leaders Vow to...\n",
              "3  20803  If at first you don’t succeed, try a different...\n",
              "4  20804  42 mins ago 1 Views 0 Comments 0 Likes 'For th..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt8nE6-sYYeq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = train.rename(columns={'text':'comment_text'})\n",
        "test = test.rename(columns={'text':'comment_text'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eppj_wrQY4Co",
        "colab_type": "code",
        "outputId": "9a5b2194-6450-4454-9c1e-f7acd3611198",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Ever get the feeling your life circles the rou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                       comment_text  label\n",
              "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...      1\n",
              "1   1  Ever get the feeling your life circles the rou...      0\n",
              "2   2  Why the Truth Might Get You Fired October 29, ...      1\n",
              "3   3  Videos 15 Civilians Killed In Single US Airstr...      1\n",
              "4   4  Print \\nAn Iranian woman has been sentenced to...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weO41lyQYmdu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = test.dropna()\n",
        "train = train.dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aXEMRh6YC8S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.to_csv('train1.csv')\n",
        "test.to_csv('test1.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_S9yF1ZYxN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('train1.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPSRG033YzrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = pd.read_csv('test1.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy6ow-f_bR-I",
        "colab_type": "code",
        "outputId": "20a5c608-f233-42c8-8a00-d7f4a466eb42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PALO ALTO, Calif.  —   After years of scorning...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Russian warships ready to strike terrorists ne...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Videos #NoDAPL: Native American Leaders Vow to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>If at first you don’t succeed, try a different...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>42 mins ago 1 Views 0 Comments 0 Likes 'For th...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        comment_text\n",
              "0  PALO ALTO, Calif.  —   After years of scorning...\n",
              "1  Russian warships ready to strike terrorists ne...\n",
              "2  Videos #NoDAPL: Native American Leaders Vow to...\n",
              "3  If at first you don’t succeed, try a different...\n",
              "4  42 mins ago 1 Views 0 Comments 0 Likes 'For th..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRH3tGeMXjVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = reader.read(train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXV7gbFjXZ9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_ds = reader.read(test)\n",
        "val_ds = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1HjxiH_XecZ",
        "colab_type": "code",
        "outputId": "d0e56dee-b7f0-4bfd-acb1-11680f05f591",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "train_ds[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<allennlp.data.instance.Instance at 0x7f8b045cb908>,\n",
              " <allennlp.data.instance.Instance at 0x7f8b041d9860>,\n",
              " <allennlp.data.instance.Instance at 0x7f8b0457a048>,\n",
              " <allennlp.data.instance.Instance at 0x7f8b045c9898>,\n",
              " <allennlp.data.instance.Instance at 0x7f8b040b8748>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4xLXk4od9RS",
        "colab_type": "code",
        "outputId": "7030b97e-7712-420d-99d0-b796d640f837",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "len(train_ds)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z469cNpdeB_E",
        "colab_type": "code",
        "outputId": "9461a4b9-dab0-41c0-9046-402394c9c308",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "len(test_ds)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USQ4RT-keDSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vars(train_ds[0].fields[\"tokens\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhAQY3JHeSMC",
        "colab_type": "text"
      },
      "source": [
        "Wait, aren’t the fields supposed to convert my data into tensors?\n",
        "\n",
        "This is one of the gotchas of text processing for deep learning: you can only convert fields into tensors after you know what the vocabulary is. To build the vocabulary, you need to pass through all the text. To build a vocabulary over the training examples, just run the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-Ke4ZFMeHEL",
        "colab_type": "code",
        "outputId": "b64b03fb-9f2c-47bf-cc9a-df62b21d9e64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "vocab = Vocabulary.from_instances(train_ds, max_vocab_size=config.max_vocab_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1000/1000 [00:00<00:00, 10552.80it/s]\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oZWNvWLeeoh",
        "colab_type": "text"
      },
      "source": [
        "Where do we tell the fields to use this vocabulary? This is not immediately intuitive, but the answer is the Iterator – which nicely leads us to our next topic: DataIterators.\n",
        "\n",
        "Neural networks in PyTorch are trained on mini batches of tensors, not lists of data. Therefore, datasets need to be batched and converted to tensors.\n",
        "\n",
        "This seems trivial at first glance, but there is a lot of subtlety here. To list just a few things we have to consider:\n",
        "\n",
        "- Sequences of different lengths need to be padded\n",
        "- To minimize padding, sequences of similar lengths can be put in the same batch\n",
        "- Tensors need to be sent to the GPU if using the GPU\n",
        "- Data needs to be shuffled at the end of each epoch during training, but we don’t want to shuffle in the midst of an epoch in order to cover all examples evenly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOmbPk8ae_ck",
        "colab_type": "text"
      },
      "source": [
        "Thankfully, AllenNLP has several convenient iterators that will take care of all of these problems behind the scenes. Therefore, you will rarely have to implement your own Iterators from scratch (unless you are doing something really tricky during batching)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FKRgw4jeMKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.data.iterators import BucketIterator\n",
        "\n",
        "iterator = BucketIterator(batch_size=config.batch_size, \n",
        "                          sorting_keys=[(\"tokens\", \"num_tokens\")],\n",
        "                         )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9ihKwvIfCiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iterator.index_with(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSz06Uf-fEUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch = next(iter(iterator(train_ds)))\n",
        "batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-AeVT_tfYiF",
        "colab_type": "text"
      },
      "source": [
        "The BucketIterator batches sequences of similar lengths together to minimize padding. To prevent the batches from becoming deterministic, a small amount of noise is added to the lengths. The sorting_keys keyword argument tells the iterator which field to reference when determining the text length of each instance. Remember, Iterators are responsible for numericalizing the text fields. We pass the vocabulary we built earlier so that the Iterator knows how to map the words to integers.\n",
        "\n",
        "Important Tip: Don’t forget to run iterator.index_with(vocab)!\n",
        "\n",
        "You may have noticed that the iterator does not take datasets as an argument. This is an important distinction between general iterators in PyTorch and iterators in AllenNLP. Whereas iterators are direct sources of batches in PyTorch, in AllenNLP, iterators are a schema for how to convert lists of Instances into mini batches of tensors. Therefore, you can’t directly iterate over a DataIterator in AllenNLP!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1Vy5WILfGzl",
        "colab_type": "code",
        "outputId": "c03235f4-23c3-44f5-fbf0-f9c38347d1ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        }
      },
      "source": [
        "batch[\"tokens\"][\"tokens\"]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   22,  8584,  8585,  ...,  8592,   385,  5129],\n",
              "        [  452,    12,   425,  ...,    14,     7,   176],\n",
              "        [  154,    12,    33,  ...,    81,     7,   329],\n",
              "        ...,\n",
              "        [ 1791,    24,  1539,  ...,    37,   153,    11],\n",
              "        [12755, 12756,   782,  ..., 12808, 12809, 12810],\n",
              "        [  256,   106,   107,  ...,  2743,  2744,  3094]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsOYnljHfUUK",
        "colab_type": "code",
        "outputId": "4977cad4-afc9-4e5e-dd84-6e0c147d7b77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "batch[\"tokens\"][\"tokens\"].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLYdwf3EfrBl",
        "colab_type": "text"
      },
      "source": [
        "###Model\n",
        "\n",
        "AllenNLP models are mostly just simple PyTorch models. The key difference is that AllenNLP models are required to return a dictionary for every forward pass and compute the loss function within the forward method during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EoduRqlfVNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
        "from allennlp.nn.util import get_text_field_mask\n",
        "from allennlp.models import Model\n",
        "from allennlp.modules.text_field_embedders import TextFieldEmbedder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ky4xudDegHuo",
        "colab_type": "text"
      },
      "source": [
        "This may seem a bit unusual, but this restriction allows you to use all sorts of creative methods of computing the loss while taking advantage of the AllenNLP Trainer (which we will get to later). For instance, you can apply masks to your loss function, weight the losses of different classes adaptively, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytqS_mcPgL42",
        "colab_type": "text"
      },
      "source": [
        "One amazing aspect of AllenNLP is that it has a whole host of convenient tools for constructing models for NLP. To utilize these components fully, AllenNLP models are generally composed from the following components:\n",
        "\n",
        "A token embedder\n",
        "\n",
        "An encoder\n",
        "\n",
        "(For seq-to-seq models) A decoder\n",
        "\n",
        "Therefore, at a high level our model can be written very simply as"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFmMqJDCf13f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaselineModel(Model):\n",
        "    def __init__(self, word_embeddings: TextFieldEmbedder,\n",
        "                 encoder: Seq2VecEncoder,\n",
        "                 out_sz: int=len(label_cols)):\n",
        "        super().__init__(vocab)\n",
        "        self.word_embeddings = word_embeddings\n",
        "        self.encoder = encoder\n",
        "        self.projection = nn.Linear(self.encoder.get_output_dim(), out_sz)\n",
        "        self.loss = nn.BCEWithLogitsLoss()\n",
        "        \n",
        "    def forward(self, tokens: Dict[str, torch.Tensor],\n",
        "                id: Any, label: torch.Tensor) -> torch.Tensor:\n",
        "        mask = get_text_field_mask(tokens)\n",
        "        embeddings = self.word_embeddings(tokens)\n",
        "        state = self.encoder(embeddings, mask)\n",
        "        class_logits = self.projection(state)\n",
        "        \n",
        "        output = {\"class_logits\": class_logits}\n",
        "        output[\"loss\"] = self.loss(class_logits, label)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy1LmInIgZ3O",
        "colab_type": "text"
      },
      "source": [
        "###The Embedder\n",
        "\n",
        "The embedder maps a sequence of token ids (or character ids) into a sequence of tensors.\n",
        "\n",
        "You’ll notice that there are two classes here for handling embeddings: the Embedding class and the BasicTextFieldEmbedder class. This is slightly clumsy but is necessary to map the fields of a batch to the appropriate embedding mechanism."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfdAZEatgER9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.modules.token_embedders import Embedding\n",
        "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
        "\n",
        "token_embedding = Embedding(num_embeddings=config.max_vocab_size + 2,\n",
        "                            embedding_dim=300, padding_index=0)\n",
        "# the embedder maps the input tokens to the appropriate embedding matrix\n",
        "word_embeddings: TextFieldEmbedder = BasicTextFieldEmbedder({\"tokens\": token_embedding})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSDBeSvYgs2r",
        "colab_type": "text"
      },
      "source": [
        "###The Encoder\n",
        "\n",
        "To classify each sentence, we need to convert the sequence of embeddings into a single vector. In AllenNLP, the model that handles this is referred to as a Seq2VecEncoder: a mapping from sequences to a single vector.\n",
        "\n",
        "Though AllenNLP provides many Seq2VecEncoders our of the box, for this example we’ll use a simple bidirectional LSTM. Don’t remember the semantics of LSTMs in PyTorch? Don’t worry: AllenNLP has you covered. AllenNLP provides a handy wrapper called the PytorchSeq2VecWrapper that wraps the LSTM so that it takes a sequence as input and returns the final hidden state, converting it into a Seq2VecEncoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLyMs2hYgbjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.modules.seq2vec_encoders import PytorchSeq2VecWrapper\n",
        "encoder: Seq2VecEncoder = PytorchSeq2VecWrapper(nn.LSTM(word_embeddings.get_output_dim(),\n",
        "                                                        config.hidden_sz, bidirectional=True, batch_first=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDuPmC2Lg4nv",
        "colab_type": "text"
      },
      "source": [
        "Now, we can build our model in 3 simple lines of code! (or 4 lines depending on how you count it)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNDoL0z3guOI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BaselineModel(\n",
        "    word_embeddings, \n",
        "    encoder, \n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4rxlatFg7Ho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if USE_GPU: model.cuda()\n",
        "else: model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyzCG13XhD0n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch = nn_util.move_to_device(batch, 0 if USE_GPU else -1)\n",
        "tokens = batch[\"tokens\"]\n",
        "labels = batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WU7N6JpkhF2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_tVvSlkhIlv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mask = get_text_field_mask(tokens)\n",
        "mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPyPMoCOhK5V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings = model.word_embeddings(tokens)\n",
        "state = model.encoder(embeddings, mask)\n",
        "class_logits = model.projection(state)\n",
        "class_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmmSR8Cfh1Oa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model(**batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S33Gn8zjl13",
        "colab_type": "code",
        "outputId": "c8a52b1e-8f4b-436a-9990-909d61c0a027",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "loss = model(**batch)[\"loss\"]\n",
        "loss"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6986, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbbSMAERjpd7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss.backward()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhyYbFxxhBxs",
        "colab_type": "text"
      },
      "source": [
        "###Trainer\n",
        "\n",
        "AllenNLP – thanks to the light restrictions it puts on its models and iterators – provides a Trainer class that removes the necessity of boilerplate code and gives us all sorts of functionality, including access to Tensorboard, one of the best visualization/debugging tools for training neural networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZA72PB5gg7-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=config.lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9gGVQWphYYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.training.trainer import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    iterator=iterator,\n",
        "    train_dataset=train_ds,\n",
        "    cuda_device=0 if USE_GPU else -1,\n",
        "    num_epochs=config.epochs,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bsm0yFMhZoV",
        "colab_type": "code",
        "outputId": "caacb8a6-5a03-428c-bb93-574b78117360",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2417
        }
      },
      "source": [
        "metrics = trainer.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "loss: 0.6329 ||:   6%|▋         | 1/16 [00:00<00:01,  9.85it/s]\u001b[A\n",
            "loss: 0.6077 ||:  19%|█▉        | 3/16 [00:00<00:01, 11.53it/s]\u001b[A\n",
            "loss: 0.6040 ||:  31%|███▏      | 5/16 [00:00<00:00, 13.07it/s]\u001b[A\n",
            "loss: 0.5888 ||:  50%|█████     | 8/16 [00:00<00:00, 14.71it/s]\u001b[A\n",
            "loss: 0.5763 ||:  69%|██████▉   | 11/16 [00:00<00:00, 16.38it/s]\u001b[A\n",
            "loss: 0.5629 ||:  88%|████████▊ | 14/16 [00:00<00:00, 17.91it/s]\u001b[A\n",
            "loss: 0.5532 ||: 100%|██████████| 16/16 [00:00<00:00, 19.70it/s]\u001b[A\n",
            "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "loss: 0.4479 ||:  12%|█▎        | 2/16 [00:00<00:00, 15.83it/s]\u001b[A\n",
            "loss: 0.4211 ||:  31%|███▏      | 5/16 [00:00<00:00, 17.64it/s]\u001b[A\n",
            "loss: 0.3980 ||:  50%|█████     | 8/16 [00:00<00:00, 19.41it/s]\u001b[A\n",
            "loss: 0.3874 ||:  69%|██████▉   | 11/16 [00:00<00:00, 20.67it/s]\u001b[A\n",
            "loss: 0.3684 ||:  88%|████████▊ | 14/16 [00:00<00:00, 21.63it/s]\u001b[A\n",
            "loss: 0.3531 ||: 100%|██████████| 16/16 [00:00<00:00, 22.82it/s]\u001b[A\n",
            "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "loss: 0.2039 ||:  12%|█▎        | 2/16 [00:00<00:00, 15.83it/s]\u001b[A\n",
            "loss: 0.1917 ||:  31%|███▏      | 5/16 [00:00<00:00, 17.93it/s]\u001b[A\n",
            "loss: 0.1777 ||:  50%|█████     | 8/16 [00:00<00:00, 19.59it/s]\u001b[A\n",
            "loss: 0.1817 ||:  69%|██████▉   | 11/16 [00:00<00:00, 20.57it/s]\u001b[A\n",
            "loss: 0.1694 ||:  88%|████████▊ | 14/16 [00:00<00:00, 21.62it/s]\u001b[A\n",
            "loss: 0.1619 ||: 100%|██████████| 16/16 [00:00<00:00, 22.95it/s]\u001b[A\n",
            "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "loss: 0.0846 ||:  12%|█▎        | 2/16 [00:00<00:00, 15.69it/s]\u001b[A\n",
            "loss: 0.0815 ||:  31%|███▏      | 5/16 [00:00<00:00, 17.64it/s]\u001b[A\n",
            "loss: 0.0810 ||:  50%|█████     | 8/16 [00:00<00:00, 19.26it/s]\u001b[A\n",
            "loss: 0.0758 ||:  69%|██████▉   | 11/16 [00:00<00:00, 20.84it/s]\u001b[A\n",
            "loss: 0.0802 ||:  88%|████████▊ | 14/16 [00:00<00:00, 21.75it/s]\u001b[A\n",
            "loss: 0.0765 ||: 100%|██████████| 16/16 [00:00<00:00, 23.06it/s]\u001b[A\n",
            "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "loss: 0.0337 ||:  12%|█▎        | 2/16 [00:00<00:00, 14.82it/s]\u001b[A\n",
            "loss: 0.0530 ||:  31%|███▏      | 5/16 [00:00<00:00, 16.83it/s]\u001b[A\n",
            "loss: 0.0428 ||:  50%|█████     | 8/16 [00:00<00:00, 18.67it/s]\u001b[A\n",
            "loss: 0.0380 ||:  69%|██████▉   | 11/16 [00:00<00:00, 20.21it/s]\u001b[A\n",
            "loss: 0.0335 ||:  88%|████████▊ | 14/16 [00:00<00:00, 21.41it/s]\u001b[A\n",
            "loss: 0.0316 ||: 100%|██████████| 16/16 [00:00<00:00, 22.78it/s]\u001b[A\n",
            "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "loss: 0.0186 ||:  12%|█▎        | 2/16 [00:00<00:00, 16.31it/s]\u001b[A\n",
            "loss: 0.0164 ||:  31%|███▏      | 5/16 [00:00<00:00, 17.73it/s]\u001b[A\n",
            "loss: 0.0144 ||:  50%|█████     | 8/16 [00:00<00:00, 19.35it/s]\u001b[A\n",
            "loss: 0.0242 ||:  69%|██████▉   | 11/16 [00:00<00:00, 20.74it/s]\u001b[A\n",
            "loss: 0.0225 ||:  88%|████████▊ | 14/16 [00:00<00:00, 21.80it/s]\u001b[A\n",
            "loss: 0.0210 ||: 100%|██████████| 16/16 [00:00<00:00, 22.78it/s]\u001b[A\n",
            "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "loss: 0.0136 ||:  12%|█▎        | 2/16 [00:00<00:00, 15.73it/s]\u001b[A\n",
            "loss: 0.0393 ||:  25%|██▌       | 4/16 [00:00<00:00, 16.57it/s]\u001b[A\n",
            "loss: 0.0272 ||:  44%|████▍     | 7/16 [00:00<00:00, 18.35it/s]\u001b[A\n",
            "loss: 0.0218 ||:  62%|██████▎   | 10/16 [00:00<00:00, 20.00it/s]\u001b[A\n",
            "loss: 0.0187 ||:  81%|████████▏ | 13/16 [00:00<00:00, 21.30it/s]\u001b[A\n",
            "loss: 0.0173 ||: 100%|██████████| 16/16 [00:00<00:00, 22.29it/s]\u001b[A\n",
            "\u001b[A\n",
            "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "loss: 0.0105 ||:  12%|█▎        | 2/16 [00:00<00:00, 16.14it/s]\u001b[A\n",
            "loss: 0.0086 ||:  31%|███▏      | 5/16 [00:00<00:00, 18.08it/s]\u001b[A\n",
            "loss: 0.0081 ||:  50%|█████     | 8/16 [00:00<00:00, 19.71it/s]\u001b[A\n",
            "loss: 0.0181 ||:  69%|██████▉   | 11/16 [00:00<00:00, 20.97it/s]\u001b[A\n",
            "loss: 0.0157 ||:  88%|████████▊ | 14/16 [00:00<00:00, 22.11it/s]\u001b[A\n",
            "loss: 0.0147 ||: 100%|██████████| 16/16 [00:00<00:00, 23.28it/s]\u001b[A\n",
            "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "loss: 0.0066 ||:  12%|█▎        | 2/16 [00:00<00:00, 14.93it/s]\u001b[A\n",
            "loss: 0.0281 ||:  31%|███▏      | 5/16 [00:00<00:00, 16.91it/s]\u001b[A\n",
            "loss: 0.0199 ||:  50%|█████     | 8/16 [00:00<00:00, 18.66it/s]\u001b[A\n",
            "loss: 0.0164 ||:  69%|██████▉   | 11/16 [00:00<00:00, 20.18it/s]\u001b[A\n",
            "loss: 0.0142 ||:  88%|████████▊ | 14/16 [00:00<00:00, 21.14it/s]\u001b[A\n",
            "loss: 0.0132 ||: 100%|██████████| 16/16 [00:00<00:00, 22.65it/s]\u001b[A\n",
            "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "loss: 0.0054 ||:  12%|█▎        | 2/16 [00:00<00:00, 16.24it/s]\u001b[A\n",
            "loss: 0.0267 ||:  31%|███▏      | 5/16 [00:00<00:00, 17.98it/s]\u001b[A\n",
            "loss: 0.0190 ||:  50%|█████     | 8/16 [00:00<00:00, 19.58it/s]\u001b[A\n",
            "loss: 0.0152 ||:  69%|██████▉   | 11/16 [00:00<00:00, 20.88it/s]\u001b[A\n",
            "loss: 0.0130 ||:  88%|████████▊ | 14/16 [00:00<00:00, 21.97it/s]\u001b[A\n",
            "loss: 0.0121 ||: 100%|██████████| 16/16 [00:00<00:00, 22.92it/s]\u001b[A\n",
            "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "loss: 0.0059 ||:  12%|█▎        | 2/16 [00:00<00:00, 15.32it/s]\u001b[A\n",
            "loss: 0.0052 ||:  31%|███▏      | 5/16 [00:00<00:00, 17.37it/s]\u001b[A\n",
            "loss: 0.0050 ||:  50%|█████     | 8/16 [00:00<00:00, 19.20it/s]\u001b[A\n",
            "loss: 0.0049 ||:  69%|██████▉   | 11/16 [00:00<00:00, 20.63it/s]\u001b[A\n",
            "loss: 0.0048 ||:  88%|████████▊ | 14/16 [00:00<00:00, 21.76it/s]\u001b[A\n",
            "loss: 0.0114 ||: 100%|██████████| 16/16 [00:00<00:00, 23.05it/s]\u001b[A\n",
            "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "loss: 0.0042 ||:  12%|█▎        | 2/16 [00:00<00:00, 15.34it/s]\u001b[A\n",
            "loss: 0.0041 ||:  31%|███▏      | 5/16 [00:00<00:00, 17.35it/s]\u001b[A\n",
            "loss: 0.0043 ||:  50%|█████     | 8/16 [00:00<00:00, 19.11it/s]\u001b[A\n",
            "loss: 0.0042 ||:  69%|██████▉   | 11/16 [00:00<00:00, 20.67it/s]\u001b[A\n",
            "loss: 0.0041 ||:  88%|████████▊ | 14/16 [00:00<00:00, 21.89it/s]\u001b[A\n",
            "loss: 0.0106 ||: 100%|██████████| 16/16 [00:00<00:00, 23.07it/s]\u001b[A\n",
            "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "loss: 0.0036 ||:  12%|█▎        | 2/16 [00:00<00:00, 15.28it/s]\u001b[A\n",
            "loss: 0.0245 ||:  31%|███▏      | 5/16 [00:00<00:00, 17.15it/s]\u001b[A\n",
            "loss: 0.0167 ||:  50%|█████     | 8/16 [00:00<00:00, 18.09it/s]\u001b[A\n",
            "loss: 0.0131 ||:  69%|██████▉   | 11/16 [00:00<00:00, 19.55it/s]\u001b[A\n",
            "loss: 0.0110 ||:  88%|████████▊ | 14/16 [00:00<00:00, 20.93it/s]\u001b[A\n",
            "loss: 0.0100 ||: 100%|██████████| 16/16 [00:00<00:00, 22.00it/s]\u001b[A\n",
            "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "loss: 0.0550 ||:  12%|█▎        | 2/16 [00:00<00:00, 16.16it/s]\u001b[A\n",
            "loss: 0.0240 ||:  31%|███▏      | 5/16 [00:00<00:00, 18.01it/s]\u001b[A\n",
            "loss: 0.0161 ||:  50%|█████     | 8/16 [00:00<00:00, 19.66it/s]\u001b[A\n",
            "loss: 0.0125 ||:  69%|██████▉   | 11/16 [00:00<00:00, 21.04it/s]\u001b[A\n",
            "loss: 0.0105 ||:  88%|████████▊ | 14/16 [00:00<00:00, 22.09it/s]\u001b[A\n",
            "loss: 0.0095 ||: 100%|██████████| 16/16 [00:00<00:00, 22.98it/s]\u001b[A\n",
            "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "loss: 0.0028 ||:  12%|█▎        | 2/16 [00:00<00:00, 16.74it/s]\u001b[A\n",
            "loss: 0.0233 ||:  31%|███▏      | 5/16 [00:00<00:00, 18.30it/s]\u001b[A\n",
            "loss: 0.0156 ||:  50%|█████     | 8/16 [00:00<00:00, 19.89it/s]\u001b[A\n",
            "loss: 0.0121 ||:  69%|██████▉   | 11/16 [00:00<00:00, 21.18it/s]\u001b[A\n",
            "loss: 0.0101 ||:  88%|████████▊ | 14/16 [00:00<00:00, 22.21it/s]\u001b[A\n",
            "loss: 0.0091 ||: 100%|██████████| 16/16 [00:00<00:00, 23.10it/s]\u001b[A\n",
            "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "loss: 0.0026 ||:  12%|█▎        | 2/16 [00:00<00:00, 16.04it/s]\u001b[A\n",
            "loss: 0.0229 ||:  31%|███▏      | 5/16 [00:00<00:00, 17.89it/s]\u001b[A\n",
            "loss: 0.0153 ||:  50%|█████     | 8/16 [00:00<00:00, 19.59it/s]\u001b[A\n",
            "loss: 0.0117 ||:  69%|██████▉   | 11/16 [00:00<00:00, 20.91it/s]\u001b[A\n",
            "loss: 0.0097 ||:  88%|████████▊ | 14/16 [00:00<00:00, 22.03it/s]\u001b[A\n",
            "loss: 0.0088 ||: 100%|██████████| 16/16 [00:00<00:00, 23.30it/s]\u001b[A\n",
            "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "loss: 0.0024 ||:  12%|█▎        | 2/16 [00:00<00:00, 15.23it/s]\u001b[A\n",
            "loss: 0.0226 ||:  31%|███▏      | 5/16 [00:00<00:00, 16.91it/s]\u001b[A\n",
            "loss: 0.0149 ||:  50%|█████     | 8/16 [00:00<00:00, 18.73it/s]\u001b[A\n",
            "loss: 0.0123 ||:  69%|██████▉   | 11/16 [00:00<00:00, 20.26it/s]\u001b[A\n",
            "loss: 0.0101 ||:  88%|████████▊ | 14/16 [00:00<00:00, 21.54it/s]\u001b[A\n",
            "loss: 0.0092 ||: 100%|██████████| 16/16 [00:00<00:00, 22.80it/s]\u001b[A\n",
            "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "loss: 0.0026 ||:  12%|█▎        | 2/16 [00:00<00:00, 16.00it/s]\u001b[A\n",
            "loss: 0.0028 ||:  31%|███▏      | 5/16 [00:00<00:00, 17.87it/s]\u001b[A\n",
            "loss: 0.0034 ||:  50%|█████     | 8/16 [00:00<00:00, 19.51it/s]\u001b[A\n",
            "loss: 0.0125 ||:  69%|██████▉   | 11/16 [00:00<00:00, 20.90it/s]\u001b[A\n",
            "loss: 0.0105 ||:  88%|████████▊ | 14/16 [00:00<00:00, 21.99it/s]\u001b[A\n",
            "loss: 0.0097 ||: 100%|██████████| 16/16 [00:00<00:00, 23.14it/s]\u001b[A\n",
            "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "loss: 0.0037 ||:  12%|█▎        | 2/16 [00:00<00:00, 15.40it/s]\u001b[A\n",
            "loss: 0.0034 ||:  31%|███▏      | 5/16 [00:00<00:00, 17.35it/s]\u001b[A\n",
            "loss: 0.0031 ||:  50%|█████     | 8/16 [00:00<00:00, 19.00it/s]\u001b[A\n",
            "loss: 0.0120 ||:  69%|██████▉   | 11/16 [00:00<00:00, 20.34it/s]\u001b[A\n",
            "loss: 0.0105 ||:  81%|████████▏ | 13/16 [00:00<00:00, 19.78it/s]\u001b[A\n",
            "loss: 0.0089 ||: 100%|██████████| 16/16 [00:00<00:00, 21.12it/s]\u001b[A\n",
            "\u001b[A\n",
            "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "loss: 0.0021 ||:  12%|█▎        | 2/16 [00:00<00:00, 15.44it/s]\u001b[A\n",
            "loss: 0.0020 ||:  31%|███▏      | 5/16 [00:00<00:00, 17.49it/s]\u001b[A\n",
            "loss: 0.0145 ||:  50%|█████     | 8/16 [00:00<00:00, 19.07it/s]\u001b[A\n",
            "loss: 0.0110 ||:  69%|██████▉   | 11/16 [00:00<00:00, 20.49it/s]\u001b[A\n",
            "loss: 0.0091 ||:  88%|████████▊ | 14/16 [00:00<00:00, 21.78it/s]\u001b[A\n",
            "loss: 0.0081 ||: 100%|██████████| 16/16 [00:00<00:00, 23.01it/s]\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B359lcyOljSL",
        "colab_type": "text"
      },
      "source": [
        "###AllenNLP Predictors\n",
        "\n",
        "AllenNLP’s predictors aren’t very easy to use and don’t feel as polished as other parts of the API. Instead of toiling through the predictor API in AllenNLP, I propose a simpler solution: let’s write our own predictor. Thanks to the great tools in AllenNLP this is pretty easy and instructive!\n",
        "\n",
        "Our predictor will simply extract the model logits from each batch and concatenate them to form a single matrix containing predictions for all the Instances in the dataset. \n",
        "\n",
        "we’re using iterators to batch our data easily and exploiting the semantics of the model output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKEMGnIEha84",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.data.iterators import DataIterator\n",
        "from tqdm import tqdm\n",
        "from scipy.special import expit # the sigmoid function\n",
        "\n",
        "def tonp(tsr): return tsr.detach().cpu().numpy()\n",
        "\n",
        "class Predictor:\n",
        "    def __init__(self, model: Model, iterator: DataIterator,\n",
        "                 cuda_device: int=-1) -> None:\n",
        "        self.model = model\n",
        "        self.iterator = iterator\n",
        "        self.cuda_device = cuda_device\n",
        "        \n",
        "    def _extract_data(self, batch) -> np.ndarray:\n",
        "        out_dict = self.model(**batch)\n",
        "        return expit(tonp(out_dict[\"class_logits\"]))\n",
        "    \n",
        "    def predict(self, ds: Iterable[Instance]) -> np.ndarray:\n",
        "        pred_generator = self.iterator(ds, num_epochs=1, shuffle=False)\n",
        "        self.model.eval()\n",
        "        pred_generator_tqdm = tqdm(pred_generator,\n",
        "                                   total=self.iterator.get_num_batches(ds))\n",
        "        preds = []\n",
        "        with torch.no_grad():\n",
        "            for batch in pred_generator_tqdm:\n",
        "                batch = nn_util.move_to_device(batch, self.cuda_device)\n",
        "                preds.append(self._extract_data(batch))\n",
        "        return np.concatenate(preds, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-joJZ9vltS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.data.iterators import BasicIterator\n",
        "# iterate over the dataset without changing its order\n",
        "seq_iterator = BasicIterator(batch_size=64)\n",
        "seq_iterator.index_with(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVp5Ary2l1yg",
        "colab_type": "code",
        "outputId": "eb9fa075-908c-4353-d695-6836d5989331",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "predictor = Predictor(model, seq_iterator, cuda_device=0 if USE_GPU else -1)\n",
        "train_preds = predictor.predict(train_ds) \n",
        "test_preds = predictor.predict(test_ds)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 38%|███▊      | 6/16 [00:00<00:00, 53.54it/s]\u001b[A\u001b[A\n",
            "\n",
            " 75%|███████▌  | 12/16 [00:00<00:00, 52.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 16/16 [00:00<00:00, 51.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 38%|███▊      | 6/16 [00:00<00:00, 53.74it/s]\u001b[A\u001b[A\n",
            "\n",
            " 75%|███████▌  | 12/16 [00:00<00:00, 53.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 16/16 [00:00<00:00, 51.73it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwv6nFKJqaDU",
        "colab_type": "code",
        "outputId": "b8031bfe-d25f-4398-8114-7dee027d1eff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "train['comment_text'][3]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Videos 15 Civilians Killed In Single US Airstrike Have Been Identified The rate at which civilians are being killed by American airstrikes in Afghanistan is now higher than it was in 2014 when the US was engaged in active combat operations.   Photo of Hellfire missiles being loaded onto a US military Reaper drone in Afghanistan by Staff Sgt. Brian Ferguson/U.S. Air Force. \\nThe Bureau has been able to identify 15 civilians killed in a single US drone strike in Afghanistan last month – the biggest loss of civilian life in one strike since the attack on the Medecins Sans Frontieres hospital (MSF) last October. \\nThe US claimed it had conducted a “counter-terrorism” strike against Islamic State (IS) fighters when it hit Nangarhar province with missiles on September 28. But the next day the United Nations issued an unusually rapid and strong statement saying the strike had killed 15 civilians and injured 13 others who had gathered at a house to celebrate a tribal elder’s return from a pilgrimage to Mecca. \\nThe Bureau spoke to a man named Haji Rais who said he was the owner of the house that was targeted. He said 15 people were killed and 19 others injured, and provided their names (listed below). The Bureau was able to independently verify the identities of those who died. \\nRais’ son, a headmaster at a local school, was among them. Another man, Abdul Hakim, lost three of his sons in the attack. \\nRais said he had no involvement with IS and denied US claims that IS members had visited his house before the strike. He said: “I did not even speak to those sort of people on the phone let alone receiving them in my house.” \\nThe deaths amount to the biggest confirmed loss of civilian life in a single American strike in Afghanistan since the attack on the MSF hospital in Kunduz last October, which killed at least 42 people. \\nThe Nangarhar strike was not the only US attack to kill civilians in September. The Bureau’s data indicates that as many as 45 civilians and allied soldiers were killed in four American strikes in Afghanistan and Somalia that month. \\nOn September 18 a pair of strikes killed eight Afghan policemen in Tarinkot, the capital of Urozgan provice. US jets reportedly hit a police checkpoint, killing one officer, before returning to target first responders. The use of this tactic – known as a “double-tap” strike – is controversial because they often hit civilian rescuers. \\nThe US told the Bureau it had conducted the strike against individuals firing on and posing a threat to Afghan forces. The email did not directly address the allegations of Afghan policemen being killed. \\nAt the end of the month in Somalia, citizens burnt US flags on the streets of the north-central city of Galcayo after it emerged a drone attack may have unintentionally killed 22 Somali soldiers and civilians. The strike occurred on the same day as the one in Nangarhar. \\nIn both the Somali and Afghan incidents, the US at first denied that any non-combatants had been killed. It is now investigating both the strikes in Nangarhar and Galcayo. \\nThe rate at which civilians are being killed by American airstrikes in Afghanistan is now higher than it was in 2014 when the US was engaged in active combat operations. Name'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 284
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xS7Sx-5npVjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trial_tok = ['Videos 15 Civilians Killed In Single US Airstrike Have Been Identified The rate at which civilians are being killed by American airstrikes in Afghanistan is now higher than it was in 2014 when the US was engaged in active combat operations.   Photo of Hellfire missiles being loaded onto a US military Reaper drone in Afghanistan by Staff Sgt. Brian Ferguson/U.S. Air Force. \\nThe Bureau has been able to identify 15 civilians killed in a single US drone strike in Afghanistan last month – the biggest loss of civilian life in one strike since the attack on the Medecins Sans Frontieres hospital (MSF) last October. \\nThe US claimed it had conducted a “counter-terrorism” strike against Islamic State (IS) fighters when it hit Nangarhar province with missiles on September 28. But the next day the United Nations issued an unusually rapid and strong statement saying the strike had killed 15 civilians and injured 13 others who had gathered at a house to celebrate a tribal elder’s return from a pilgrimage to Mecca. \\nThe Bureau spoke to a man named Haji Rais who said he was the owner of the house that was targeted. He said 15 people were killed and 19 others injured, and provided their names (listed below). The Bureau was able to independently verify the identities of those who died. \\nRais’ son, a headmaster at a local school, was among them. Another man, Abdul Hakim, lost three of his sons in the attack. \\nRais said he had no involvement with IS and denied US claims that IS members had visited his house before the strike. He said: “I did not even speak to those sort of people on the phone let alone receiving them in my house.” \\nThe deaths amount to the biggest confirmed loss of civilian life in a single American strike in Afghanistan since the attack on the MSF hospital in Kunduz last October, which killed at least 42 people. \\nThe Nangarhar strike was not the only US attack to kill civilians in September. The Bureau’s data indicates that as many as 45 civilians and allied soldiers were killed in four American strikes in Afghanistan and Somalia that month. \\nOn September 18 a pair of strikes killed eight Afghan policemen in Tarinkot, the capital of Urozgan provice. US jets reportedly hit a police checkpoint, killing one officer, before returning to target first responders. The use of this tactic – known as a “double-tap” strike – is controversial because they often hit civilian rescuers. \\nThe US told the Bureau it had conducted the strike against individuals firing on and posing a threat to Afghan forces. The email did not directly address the allegations of Afghan policemen being killed. \\nAt the end of the month in Somalia, citizens burnt US flags on the streets of the north-central city of Galcayo after it emerged a drone attack may have unintentionally killed 22 Somali soldiers and civilians. The strike occurred on the same day as the one in Nangarhar. \\nIn both the Somali and Afghan incidents, the US at first denied that any non-combatants had been killed. It is now investigating both the strikes in Nangarhar and Galcayo. \\nThe rate at which civilians are being killed by American airstrikes in Afghanistan is now higher than it was in 2014 when the US was engaged in active combat operations. Name']\n",
        "trial_df = pd.DataFrame({'id':1,'comment_text': trial_tok})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfoizp6cpd_T",
        "colab_type": "code",
        "outputId": "57658685-6f87-4f85-9277-c6756fd4d14f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "trial_ds = reader.read(trial_df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "1it [00:00, 80.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4--shOdotX0",
        "colab_type": "code",
        "outputId": "fe1f6a38-f743-4b7b-8146-b2ea13757289",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "trial = predictor.predict(trial_ds) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 73.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocZkt4rXpsEW",
        "colab_type": "code",
        "outputId": "09ef9ecc-3ea5-470e-a22e-1eb01b8e5a1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "trial"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9981964]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 288
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgliwBIZpxxf",
        "colab_type": "code",
        "outputId": "a89e4afd-26ae-4303-b636-900cbf1f86f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Ever get the feeling your life circles the rou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                       comment_text  label\n",
              "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...      1\n",
              "1   1  Ever get the feeling your life circles the rou...      0\n",
              "2   2  Why the Truth Might Get You Fired October 29, ...      1\n",
              "3   3  Videos 15 Civilians Killed In Single US Airstr...      1\n",
              "4   4  Print \\nAn Iranian woman has been sentenced to...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 268
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgo9WmjXqCEP",
        "colab_type": "text"
      },
      "source": [
        "Seems to be working great!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhJ9lzlnpyec",
        "colab_type": "code",
        "outputId": "7bf6d8fa-e0c6-4d96-efb6-004fcd7cd38f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "torch.save(model, 'fnd_model.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type BaselineModel. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARE_alyUt44X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mod = torch.load('fnd_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fwlh_iH-t-Y9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictor = Predictor(mod, seq_iterator, cuda_device=0 if USE_GPU else -1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAy2nIzXuDYs",
        "colab_type": "code",
        "outputId": "b8b2572b-852b-44d8-a757-5c68a9b88b30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "trial = predictor.predict(trial_ds) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 72.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t59RjmsVuXVW",
        "colab_type": "code",
        "outputId": "a85bcc3d-b578-4c04-dc51-feb14407c54d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "trial"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9981964]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 290
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0Oj5qkkuwkU",
        "colab_type": "text"
      },
      "source": [
        "###How to Switch to ELMo\n",
        "\n",
        "Simply building a single NLP pipeline to train one model is easy. Writing the pipeline so that we can iterate over multiple configurations, swap components in and out, and implement crazy architectures without making our codebase explode is much harder.\n",
        "\n",
        "Here, I’ll demonstrate how you can use ELMo to train your model with minimal changes to your code. ELMo is a recently developed method for text embedding in NLP that takes contextual information into account and achieved state-of-the-art results in many NLP tasks (If you want to learn more about ELMo, please refer to this blog post I wrote in the past explaining the method – sorry for the shameless plug).\n",
        "\n",
        "To incorporate ELMo, we’ll need to change two things:\n",
        "\n",
        "The token indexer\n",
        "The embedder\n",
        "\n",
        "ELMo uses character-level features so we’ll need to change the token indexer from a word-level indexer to a character-level indexer. In addition to converting characters to integers, we’re using a pre-trained model so we need to ensure that the mapping we use is the same as the mapping that was used to train ELMo. This seems like a lot of work, but in AllenNLP, all you need to is to use the ELMoTokenCharactersIndexer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_x_BlUquXo0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
        "from allennlp.data.token_indexers.elmo_indexer import ELMoCharacterMapper, ELMoTokenCharactersIndexer\n",
        "\n",
        "# the token indexer is responsible for mapping tokens to integers\n",
        "token_indexer = ELMoTokenCharactersIndexer()\n",
        "\n",
        "def tokenizer(x: str):\n",
        "    return [w.text for w in\n",
        "            SpacyWordSplitter(language='en_core_web_sm', \n",
        "                              pos_tags=False).split_words(x)[:config.max_seq_len]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01yoaTFDwKBd",
        "colab_type": "text"
      },
      "source": [
        "Wait, is that it? you may ask. What about the DatasetReader? Surely if we use a different indexer, we’ll need to change the way we read the dataset? Well, not in AllenNLP. This is where composition shines; since we delegate all the decisions regarding how to convert raw text into integers to the token indexer, we get to reuse all the remaining code simply by swapping in a new token indexer.\n",
        "\n",
        "One thing to note is that the ELMoTokenCharactersIndexer handles the mapping from characters to indices for you (you need to use the same mappings as the pretrained model for ELMo to have any benefit). Therefore, the code for initializing the Vocabulary is as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKgj1BPIv_9w",
        "colab_type": "code",
        "outputId": "461d5442-b919-4ab0-95aa-9e353dc8a25e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "reader = FNDDatasetReader(\n",
        "    tokenizer=tokenizer,\n",
        "    token_indexers={\"tokens\": token_indexer}\n",
        ")\n",
        "\n",
        "train_ds = reader.read(train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000it [00:11, 89.34it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtQPWvS5xzAz",
        "colab_type": "code",
        "outputId": "edabdbe4-906a-4b3b-a940-90a714d6db73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "test_ds = reader.read(test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000it [00:09, 107.47it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31jAZI0fyiFS",
        "colab_type": "code",
        "outputId": "dfa136a1-b796-48e5-b979-03ecc0f0cc34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "source": [
        "train_ds[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<allennlp.data.instance.Instance at 0x7f8b0b01a978>,\n",
              " <allennlp.data.instance.Instance at 0x7f8b0b0d0320>,\n",
              " <allennlp.data.instance.Instance at 0x7f8b0afb8668>,\n",
              " <allennlp.data.instance.Instance at 0x7f8b0af78e80>,\n",
              " <allennlp.data.instance.Instance at 0x7f8b0b09e080>,\n",
              " <allennlp.data.instance.Instance at 0x7f8b0b0fdba8>,\n",
              " <allennlp.data.instance.Instance at 0x7f8b0af84cc0>,\n",
              " <allennlp.data.instance.Instance at 0x7f8b0af41710>,\n",
              " <allennlp.data.instance.Instance at 0x7f8b0c4f6128>,\n",
              " <allennlp.data.instance.Instance at 0x7f8b0c0c3048>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 316
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb2OXHzrwduw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vars(train_ds[0].fields[\"tokens\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhxRI2kkwjkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Vocabulary() #We don't need to build the vocab: all that is handled by the token indexer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGppKWqPyAZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iterator = BucketIterator(batch_size=config.batch_size, \n",
        "                          sorting_keys=[(\"tokens\", \"num_tokens\")],\n",
        "                         )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiYJo7_9yDU0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iterator.index_with(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1fpUcyYyIco",
        "colab_type": "code",
        "outputId": "704ba414-1fd4-41df-df7f-55e14407bc70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "batch = next(iter(iterator(train_ds)))\n",
        "batch[\"tokens\"][\"tokens\"].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 100, 50])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 322
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUKeJcmKw0Ei",
        "colab_type": "text"
      },
      "source": [
        "Now, to change the embeddings to ELMo, you can simply follow a similar process:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZiIp1_xwpQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
        "from allennlp.modules.token_embedders import ElmoTokenEmbedder\n",
        "\n",
        "options_file = 'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_options.json'\n",
        "weight_file = 'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_weights.hdf5'\n",
        "\n",
        "elmo_embedder = ElmoTokenEmbedder(options_file, weight_file)\n",
        "word_embeddings = BasicTextFieldEmbedder({\"tokens\": elmo_embedder})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpER0px2zYl0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.modules.seq2vec_encoders import PytorchSeq2VecWrapper\n",
        "encoder: Seq2VecEncoder = PytorchSeq2VecWrapper(nn.LSTM(word_embeddings.get_output_dim(), config.hidden_sz, bidirectional=True, batch_first=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mi2CD3FD0bv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
        "from allennlp.nn.util import get_text_field_mask\n",
        "from allennlp.models import Model\n",
        "from allennlp.modules.text_field_embedders import TextFieldEmbedder\n",
        "\n",
        "class BaselineModel2(Model):\n",
        "    def __init__(self, word_embeddings: TextFieldEmbedder,\n",
        "                 encoder: Seq2VecEncoder,\n",
        "                 out_sz: int=len(label_cols)):\n",
        "        super().__init__(vocab)\n",
        "        self.word_embeddings = word_embeddings\n",
        "        self.encoder = encoder\n",
        "        self.projection = nn.Linear(self.encoder.get_output_dim(), out_sz)\n",
        "        self.loss = nn.BCEWithLogitsLoss()\n",
        "        \n",
        "    def forward(self, tokens: Dict[str, torch.Tensor],\n",
        "                id: Any, label: torch.Tensor) -> torch.Tensor:\n",
        "        mask = get_text_field_mask(tokens)\n",
        "        embeddings = self.word_embeddings(tokens)\n",
        "        state = self.encoder(embeddings, mask)\n",
        "        class_logits = self.projection(state)\n",
        "        \n",
        "        output = {\"class_logits\": class_logits}\n",
        "        output[\"loss\"] = self.loss(class_logits, label)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5-F8aSnxaB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BaselineModel2(\n",
        "    word_embeddings, \n",
        "    encoder, \n",
        ")\n",
        "if USE_GPU: model.cuda()\n",
        "else: model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XbfI4PZxfGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch = nn_util.move_to_device(batch, 0 if USE_GPU else -1)\n",
        "tokens = batch[\"tokens\"]\n",
        "labels = batch\n",
        "tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRqjADrLxn9k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mask = get_text_field_mask(tokens)\n",
        "mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUixkIJwxp0n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings = model.word_embeddings(tokens)\n",
        "state = model.encoder(embeddings, mask)\n",
        "class_logits = model.projection(state)\n",
        "class_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RjhuRY7zey1",
        "colab_type": "code",
        "outputId": "6bf5ba7f-6e2f-4ce6-e3f6-2cf1bffe82f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "loss = model(**batch)[\"loss\"]\n",
        "loss"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6850, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 333
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeVyrD69zi5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    iterator=iterator,\n",
        "    train_dataset=train_ds,\n",
        "    cuda_device=0 if USE_GPU else -1,\n",
        "    num_epochs=config.epochs,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w7Cr6sQw0oq",
        "colab_type": "code",
        "outputId": "bc0da124-90ab-4e12-dd12-9c3ac4d7fd11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "metrics = trainer.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 0.6862 ||: 100%|██████████| 16/16 [00:09<00:00,  1.68it/s]\n",
            "loss: 0.6618 ||: 100%|██████████| 16/16 [00:09<00:00,  1.68it/s]\n",
            "loss: 0.6309 ||: 100%|██████████| 16/16 [00:09<00:00,  1.66it/s]\n",
            "loss: 0.5769 ||: 100%|██████████| 16/16 [00:09<00:00,  1.73it/s]\n",
            "loss: 0.4997 ||: 100%|██████████| 16/16 [00:09<00:00,  1.66it/s]\n",
            "loss: 0.4226 ||: 100%|██████████| 16/16 [00:09<00:00,  1.71it/s]\n",
            "loss: 0.3593 ||: 100%|██████████| 16/16 [00:09<00:00,  1.77it/s]\n",
            "loss: 0.3272 ||: 100%|██████████| 16/16 [00:09<00:00,  1.70it/s]\n",
            "loss: 0.2837 ||: 100%|██████████| 16/16 [00:09<00:00,  1.66it/s]\n",
            "loss: 0.2532 ||: 100%|██████████| 16/16 [00:09<00:00,  1.68it/s]\n",
            "loss: 0.2225 ||: 100%|██████████| 16/16 [00:09<00:00,  1.81it/s]\n",
            "loss: 0.1910 ||: 100%|██████████| 16/16 [00:09<00:00,  1.70it/s]\n",
            "loss: 0.1770 ||: 100%|██████████| 16/16 [00:09<00:00,  1.72it/s]\n",
            "loss: 0.1575 ||: 100%|██████████| 16/16 [00:09<00:00,  1.66it/s]\n",
            "loss: 0.1401 ||: 100%|██████████| 16/16 [00:09<00:00,  1.68it/s]\n",
            "loss: 0.1329 ||: 100%|██████████| 16/16 [00:09<00:00,  1.70it/s]\n",
            "loss: 0.1363 ||: 100%|██████████| 16/16 [00:09<00:00,  1.74it/s]\n",
            "loss: 0.1068 ||: 100%|██████████| 16/16 [00:09<00:00,  1.78it/s]\n",
            "loss: 0.0990 ||: 100%|██████████| 16/16 [00:09<00:00,  1.67it/s]\n",
            "loss: 0.0996 ||: 100%|██████████| 16/16 [00:09<00:00,  1.66it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcJzfDme120s",
        "colab_type": "text"
      },
      "source": [
        "###BERT\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W62WPqnxEiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = Config(\n",
        "    testing=True,\n",
        "    seed=1,\n",
        "    batch_size=64,\n",
        "    lr=3e-4,\n",
        "    epochs=2,\n",
        "    hidden_sz=64,\n",
        "    max_seq_len=100, # necessary to limit memory usage\n",
        "    max_vocab_size=100000,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YT--8SRJ2ESk",
        "colab_type": "text"
      },
      "source": [
        "You’re probably thinking that switching to BERT is mostly the same as above. Well, you’re right – mostly. BERT has a few quirks that make it slightly different from your traditional model. One quirk is that BERT uses wordpiece embeddings so we need to use a special tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5Ebo73s15M3",
        "colab_type": "code",
        "outputId": "5af76425-dc4f-46d5-e768-1fc4c776cea2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from allennlp.data.token_indexers import PretrainedBertIndexer\n",
        "\n",
        "token_indexer = PretrainedBertIndexer(\n",
        "    pretrained_model=\"bert-base-uncased\",\n",
        "    max_pieces=config.max_seq_len,\n",
        "    do_lowercase=True,\n",
        " )\n",
        "# apparently we need to truncate the sequence here, which is a stupid design decision\n",
        "def tokenizer(s: str):\n",
        "    return token_indexer.wordpiece_tokenizer(s)[:config.max_seq_len - 2]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 929600.31B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJJIqbS019eS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reader = FNDDatasetReader(\n",
        "    tokenizer=tokenizer,\n",
        "    token_indexers={\"tokens\": token_indexer}\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-Nm99Du2MIp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Vocabulary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSHsMHcN3PJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iterator = BucketIterator(batch_size=config.batch_size, \n",
        "                          sorting_keys=[(\"tokens\", \"num_tokens\")],\n",
        "                         )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7WUJujj3m3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iterator.index_with(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wu3COn13oFt",
        "colab_type": "code",
        "outputId": "cb59e0e1-ced8-4013-b5fa-5a7593a46880",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "batch = next(iter(iterator(train_ds)))\n",
        "batch[\"tokens\"][\"tokens\"].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 100, 50])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 353
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgOCS7k13tAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um45I_Wz3uvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaselineModel3(Model):\n",
        "    def __init__(self, word_embeddings: TextFieldEmbedder,\n",
        "                 encoder: Seq2VecEncoder,\n",
        "                 out_sz: int=len(label_cols)):\n",
        "        super().__init__(vocab)\n",
        "        self.word_embeddings = word_embeddings\n",
        "        self.encoder = encoder\n",
        "        self.projection = nn.Linear(self.encoder.get_output_dim(), out_sz)\n",
        "        self.loss = nn.BCEWithLogitsLoss()\n",
        "        \n",
        "    def forward(self, tokens: Dict[str, torch.Tensor],\n",
        "                id: Any, label: torch.Tensor) -> torch.Tensor:\n",
        "        mask = get_text_field_mask(tokens)\n",
        "        embeddings = self.word_embeddings(tokens)\n",
        "        state = self.encoder(embeddings, mask)\n",
        "        class_logits = self.projection(state)\n",
        "        \n",
        "        output = {\"class_logits\": class_logits}\n",
        "        output[\"loss\"] = self.loss(class_logits, label)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jl5lAeEI3yDr",
        "colab_type": "code",
        "outputId": "6a1a5b76-1996-426f-addc-f41dd2ca029c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
        "from allennlp.modules.token_embedders.bert_token_embedder import PretrainedBertEmbedder\n",
        "\n",
        "bert_embedder = PretrainedBertEmbedder(\n",
        "        pretrained_model=\"bert-base-uncased\",\n",
        "        top_layer_only=True, # conserve memory\n",
        ")\n",
        "word_embeddings: TextFieldEmbedder = BasicTextFieldEmbedder({\"tokens\": bert_embedder},\n",
        "                                                            # we'll be ignoring masks so we'll need to set this to True\n",
        "                                                           allow_unmatched_keys = True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:13<00:00, 29521335.45B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEOYDOeL30i1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BERT_DIM = word_embeddings.get_output_dim()\n",
        "\n",
        "class BertSentencePooler(Seq2VecEncoder):\n",
        "    def forward(self, embs: torch.tensor, \n",
        "                mask: torch.tensor=None) -> torch.tensor:\n",
        "        # extract first token tensor\n",
        "        return embs[:, 0]\n",
        "    \n",
        "    @overrides\n",
        "    def get_output_dim(self) -> int:\n",
        "        return BERT_DIM\n",
        "    \n",
        "encoder = BertSentencePooler(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AW6e-jaz38w3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BaselineModel(\n",
        "    word_embeddings, \n",
        "    encoder, \n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5yLKVDT3-Vd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if USE_GPU: model.cuda()\n",
        "else: model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnCZMXN23_ix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch = nn_util.move_to_device(batch, 0 if USE_GPU else -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EGI-T6C4Dy4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens = batch[\"tokens\"]\n",
        "labels = batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C813N61s4FRB",
        "colab_type": "code",
        "outputId": "247e53c1-83ad-42ec-a913-ff7c0fb5cab4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        }
      },
      "source": [
        "mask = get_text_field_mask(tokens)\n",
        "mask"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
              "        [1, 1, 1,  ..., 1, 1, 1],\n",
              "        [1, 1, 1,  ..., 1, 1, 1],\n",
              "        ...,\n",
              "        [1, 1, 1,  ..., 1, 1, 1],\n",
              "        [1, 1, 1,  ..., 1, 1, 1],\n",
              "        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 364
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_bpRqze4F8G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings = model.word_embeddings(tokens)\n",
        "state = model.encoder(embeddings, mask)\n",
        "class_logits = model.projection(state)\n",
        "class_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFxGgra84MA1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model(**batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMHM9XGl4Pii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=config.lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kx1Ck3pi4UeP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    iterator=iterator,\n",
        "    train_dataset=train_ds,\n",
        "    cuda_device=0 if USE_GPU else -1,\n",
        "    num_epochs=config.epochs,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AR5_Au6R4WLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metrics = trainer.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqX_tiDp4Xpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}