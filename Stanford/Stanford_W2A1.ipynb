{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stanford W2A1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paruliansaragi/DL-Notebooks/blob/master/Stanford_W2A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpIJPjEhA795",
        "colab_type": "code",
        "outputId": "2585e23d-7cac-4cff-cfab-e0f0a1429205",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "import scipy\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.10.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzKBtIYoBbGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCRNEWHrBdmL",
        "colab_type": "code",
        "outputId": "ba1da7cb-7043-4446-f705-e209dc204cbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "\n",
        "# Example of a picture# Examp \n",
        "index = 25\n",
        "plt.imshow(train_images[index])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f1eda801610>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f1eda94cc50>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slUIMoPRCDBT",
        "colab_type": "code",
        "outputId": "30d2bc01-f701-4e26-c405-fbc8b25b7e4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x_train = train_images#storing it for later use\n",
        "y_test = test_images\n",
        "train_images.shape\n",
        "test_images.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTOFmSBsCF41",
        "colab_type": "code",
        "outputId": "a82968a8-e687-45cb-8ef8-01573df35679",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_labels\n",
        "test_labels"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9, 2, 1, ..., 8, 1, 5], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyK29PEXCMY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_flatten = train_images.reshape(train_images.shape[0], -1).T\n",
        "y_flatten = test_images.reshape(test_images.shape[0], -1).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8ohG_HrCl65",
        "colab_type": "code",
        "outputId": "bb94345b-4111-414f-9400-31ebb65d3afa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_flatten.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784, 60000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhqfHTyJCoEk",
        "colab_type": "code",
        "outputId": "13398793-06e9-4743-9a7c-657fa8146596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "print (\"sanity check after reshaping: \" + str(X_flatten[0:5,0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sanity check after reshaping: [0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TX-6qTqEDGLm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "norm_x_train = x_train/255.\n",
        "norm_y_train = test_images/255."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTVGynQtDY6L",
        "colab_type": "text"
      },
      "source": [
        "So the equation is the vector of pixels of an image dotted with the weights (transposed) + the bias then passed through an activation function which outputs a number between 0-1. \n",
        "\n",
        "![alt text](https://github.com/Kulbear/deep-learning-coursera/raw/997fdb2e2db67acd45d29ae418212463a54be06d/Neural%20Networks%20and%20Deep%20Learning/images/LogReg_kiank.png)\n",
        "\n",
        "\n",
        "For one example $x^{(i)}$:\n",
        "![alt text](https://render.githubusercontent.com/render/math?math=%5Cmathcal%7BL%7D%28a%5E%7B%28i%29%7D%2C%20y%5E%7B%28i%29%7D%29%20%3D%20%20-%20y%5E%7B%28i%29%7D%20%20%5Clog%28a%5E%7B%28i%29%7D%29%20-%20%281-y%5E%7B%28i%29%7D%20%29%20%20%5Clog%281-a%5E%7B%28i%29%7D%29%5Ctag%7B3%7D&mode=display)\n",
        "\n",
        "The cost is then computed by summing over all training examples:\n",
        "![alt text](https://render.githubusercontent.com/render/math?math=J%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%20%5Csum_%7Bi%3D1%7D%5Em%20%5Cmathcal%7BL%7D%28a%5E%7B%28i%29%7D%2C%20y%5E%7B%28i%29%7D%29%5Ctag%7B6%7D&mode=display)\n",
        "\n",
        "Key steps: In this exercise, you will carry out the following steps:\n",
        "\n",
        "- Initialize the parameters of the model\n",
        "- Learn the parameters for the model by minimizing the cost  \n",
        "- Use the learned parameters to make predictions (on the test set)\n",
        "- Analyse the results and conclude"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7JqtG96D8G6",
        "colab_type": "text"
      },
      "source": [
        "# 4 - Building the parts of our algorithm\n",
        "# The main steps for building a Neural Network are:\n",
        "\n",
        "Define the model structure (such as number of input features)\n",
        "Initialize the model's parameters\n",
        "Loop:\n",
        "Calculate current loss (forward propagation)\n",
        "Calculate current gradient (backward propagation)\n",
        "Update parameters (gradient descent)\n",
        "You often build 1-3 separately and integrate them into one function we call model().\n",
        "\n",
        "**4.1 - Helper functions**\n",
        "Exercise: Using your code from \"Python Basics\", implement sigmoid(). As you've seen in the figure above, you need to compute $sigmoid( w^T x + b)$ to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GlsVJqUECLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# GRADED FUNCTION: sigmoid# GRADE \n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of z\n",
        "\n",
        "    Arguments:\n",
        "    x -- A scalar or numpy array of any size.\n",
        "\n",
        "    Return:\n",
        "    s -- sigmoid(z)\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    s = 1 / (1 + np.exp(-z))\n",
        "    s = 1 /(1+ (np.exp(-z)))\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTaImc46Eapv",
        "colab_type": "code",
        "outputId": "d5c97a36-c03c-42ed-9f28-ea904d3d103d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print (\"sigmoid(0) = \" + str(sigmoid(0)))\n",
        "print (\"sigmoid(9.2) = \" + str(sigmoid(9.2)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sigmoid(0) = 0.5\n",
            "sigmoid(9.2) = 0.9998989708060922\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3_iV8BaEo6E",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**4.2 - Initializing parameters**\n",
        "Exercise: Implement parameter initialization in the cell below. You have to initialize w as a vector of zeros. If you don't know what numpy function to use, look up np.zeros() in the Numpy library's documentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUstOx_PEqCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: initialize_with_zeros\n",
        "\n",
        "def initialize_with_zeros(dim):\n",
        "    \"\"\"\n",
        "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
        "    \n",
        "    Argument:\n",
        "    dim -- size of the w vector we want (or number of parameters in this case)\n",
        "    \n",
        "    Returns:\n",
        "    w -- initialized vector of shape (dim, 1)\n",
        "    b -- initialized scalar (corresponds to the bias)\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    w = np.zeros(shape=(dim, 1))\n",
        "    b = 0\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    assert(w.shape == (dim, 1))\n",
        "    assert(isinstance(b, float) or isinstance(b, int))\n",
        "    \n",
        "    return w, b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9iwlfq9FetP",
        "colab_type": "code",
        "outputId": "9cf41da0-4030-4895-9a32-79492a360fa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "dim = 4\n",
        "w, b = initialize_with_zeros(dim)\n",
        "print (\"w = \" + str(w))\n",
        "print (\"b = \" + str(b))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w = [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "b = 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VshARJhKGLYr",
        "colab_type": "text"
      },
      "source": [
        "4.3 - Forward and Backward propagation\n",
        "Now that your parameters are initialized, you can do the \"forward\" and \"backward\" propagation steps for learning the parameters.\n",
        "\n",
        "Exercise: Implement a function propagate() that computes the cost function and its gradient.\n",
        "\n",
        "Hints:\n",
        "\n",
        "Forward Propagation:\n",
        "\n",
        "You get X\n",
        "You compute ![alt text](https://render.githubusercontent.com/render/math?math=A%20%3D%20%5Csigma%28w%5ET%20X%20%2B%20b%29%20%3D%20%28a%5E%7B%280%29%7D%2C%20a%5E%7B%281%29%7D%2C%20...%2C%20a%5E%7B%28m-1%29%7D%2C%20a%5E%7B%28m%29%7D%29&mode=inline)\n",
        "You calculate the cost function: ![alt text](https://render.githubusercontent.com/render/math?math=J%20%3D%20-%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5E%7Bm%7Dy%5E%7B%28i%29%7D%5Clog%28a%5E%7B%28i%29%7D%29%2B%281-y%5E%7B%28i%29%7D%29%5Clog%281-a%5E%7B%28i%29%7D%29&mode=inline)\n",
        "\n",
        "Here are the two formulas you will be using:\n",
        "\n",
        "![alt text](https://render.githubusercontent.com/render/math?math=%5Cfrac%7B%5Cpartial%20J%7D%7B%5Cpartial%20w%7D%20%3D%20%5Cfrac%7B1%7D%7Bm%7DX%28A-Y%29%5ET%5Ctag%7B7%7D%24%24%24%24%20%5Cfrac%7B%5Cpartial%20J%7D%7B%5Cpartial%20b%7D%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%20%5Csum_%7Bi%3D1%7D%5Em%20%28a%5E%7B%28i%29%7D-y%5E%7B%28i%29%7D%29%5Ctag%7B8%7D&mode=display)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuI3FJ_7GqfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# GRADED FUNCTION: propagate# GRADED \n",
        "\n",
        "def propagate(w, b, X, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function and its gradient for the propagation explained above\n",
        "\n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of size (num_px * num_px * 3, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
        "\n",
        "    Return:\n",
        "    cost -- negative log-likelihood cost for logistic regression\n",
        "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
        "    db -- gradient of the loss with respect to b, thus same shape as b\n",
        "    \n",
        "    Tips:\n",
        "    - Write your code step by step for the propagation\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    \n",
        "    # FORWARD PROPAGATION (FROM X TO COST)\n",
        "    ### START CODE HERE ### (≈ 2 lines of code)\n",
        "    A = sigmoid(np.dot(w.T, X) + b)  # compute activation\n",
        "    cost = (- 1 / m) * np.sum(Y * np.log(A) + (1 - Y) * (np.log(1 - A)))  # compute cost\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
        "    ### START CODE HERE ### (≈ 2 lines of code)\n",
        "    dw = (1 / m) * np.dot(X, (A - Y).T)\n",
        "    db = (1 / m) * np.sum(A - Y)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    assert(dw.shape == w.shape)\n",
        "    assert(db.dtype == float)\n",
        "    cost = np.squeeze(cost)\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return grads, cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNrZxE8NIKj5",
        "colab_type": "code",
        "outputId": "6fd14cbc-ad8f-4aff-e716-5fe174cc531b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "w, b, X, Y = np.array([[1], [2]]), 2, np.array([[1,2], [3,4]]), np.array([[1, 0]])\n",
        "grads, cost = propagate(w, b, X, Y)\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))\n",
        "print (\"cost = \" + str(cost))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dw = [[0.]\n",
            " [0.]]\n",
            "db = 0.0\n",
            "cost = 12.00012954638441\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYbcbo_dIyCD",
        "colab_type": "text"
      },
      "source": [
        "**d) Optimization¶**\n",
        "You have initialized your parameters.\n",
        "You are also able to compute a cost function and its gradient.\n",
        "Now, you want to update the parameters using gradient descent.\n",
        "Exercise: Write down the optimization function. The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrGC0GUFIzF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: optimize\n",
        "\n",
        "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
        "    \"\"\"\n",
        "    This function optimizes w and b by running a gradient descent algorithm\n",
        "    \n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    print_cost -- True to print the loss every 100 steps\n",
        "    \n",
        "    Returns:\n",
        "    params -- dictionary containing the weights w and bias b\n",
        "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
        "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
        "    \n",
        "    Tips:\n",
        "    You basically need to write down two steps and iterate through them:\n",
        "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
        "        2) Update the parameters using gradient descent rule for w and b.\n",
        "    \"\"\"\n",
        "    \n",
        "    costs = []\n",
        "    \n",
        "    for i in range(num_iterations):\n",
        "        \n",
        "        \n",
        "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
        "        ### START CODE HERE ### \n",
        "        grads, cost = propagate(w, b, X, Y)\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Retrieve derivatives from grads\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "        \n",
        "        # update rule (≈ 2 lines of code)\n",
        "        ### START CODE HERE ###\n",
        "        w = w - learning_rate * dw  # need to broadcast\n",
        "        b = b - learning_rate * db\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Record the costs\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "        \n",
        "        # Print the cost every 100 training examples\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" % (i, cost))\n",
        "    \n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return params, grads, costs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vT0pBkfYJiMV",
        "colab_type": "code",
        "outputId": "9d05b6fc-5588-43bc-a39c-beb8f3349979",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n",
        "\n",
        "print (\"w = \" + str(params[\"w\"]))\n",
        "print (\"b = \" + str(params[\"b\"]))\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w = [[1.]\n",
            " [2.]]\n",
            "b = 2.0\n",
            "dw = [[0.]\n",
            " [0.]]\n",
            "db = 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oshgceCTJkuB",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Exercise: The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the predict() function. There is two steps to computing predictions:\n",
        "\n",
        "Calculate $\\hat{Y} = A = \\sigma(w^T X + b)$\n",
        "\n",
        "Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector Y_prediction. If you wish, you can use an if/else statement in a for loop (though there is also a way to vectorize this)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMzlc7T5Jn4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: predict\n",
        "\n",
        "def predict(w, b, X):\n",
        "    '''\n",
        "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
        "    \n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of size (num_px * num_px * 3, number of examples)\n",
        "    \n",
        "    Returns:\n",
        "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
        "    '''\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    Y_prediction = np.zeros((1, m))\n",
        "    w = w.reshape(X.shape[0], 1)\n",
        "    \n",
        "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    A = sigmoid(np.dot(w.T, X) + b)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    for i in range(A.shape[1]):\n",
        "        # Convert probabilities a[0,i] to actual predictions p[0,i]\n",
        "        ### START CODE HERE ### (≈ 4 lines of code)\n",
        "        Y_prediction[0, i] = 1 if A[0, i] > 0.5 else 0\n",
        "        ### END CODE HERE ###\n",
        "    \n",
        "    assert(Y_prediction.shape == (1, m))\n",
        "    \n",
        "    return Y_prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuPztEQ8KQQJ",
        "colab_type": "code",
        "outputId": "ae833c6f-947e-457e-afc3-d225efa8b3c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(\"predictions = \" + str(predict(w, b, X)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predictions = [[1. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW9GhhrJKS2x",
        "colab_type": "text"
      },
      "source": [
        "# 5 - Merge all functions into a model\n",
        "You will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.\n",
        "\n",
        "Exercise: Implement the model function. Use the following notation:\n",
        "\n",
        "- Y_prediction for your predictions on the test set\n",
        "- Y_prediction_train for your predictions on the train set\n",
        "- w, costs, grads for the outputs of optimize()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fISH75MUKWFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: model\n",
        "\n",
        "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
        "    \"\"\"\n",
        "    Builds the logistic regression model by calling the function you've implemented previously\n",
        "    \n",
        "    Arguments:\n",
        "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
        "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
        "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
        "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
        "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
        "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
        "    print_cost -- Set to true to print the cost every 100 iterations\n",
        "    \n",
        "    Returns:\n",
        "    d -- dictionary containing information about the model.\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # initialize parameters with zeros (≈ 1 line of code)\n",
        "    w, b = initialize_with_zeros(X_train.shape[0])\n",
        "\n",
        "    # Gradient descent (≈ 1 line of code)\n",
        "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
        "    \n",
        "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
        "    w = parameters[\"w\"]\n",
        "    b = parameters[\"b\"]\n",
        "    \n",
        "    # Predict test/train set examples (≈ 2 lines of code)\n",
        "    Y_prediction_test = predict(w, b, X_test)\n",
        "    Y_prediction_train = predict(w, b, X_train)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Print train/test Errors\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
        "\n",
        "    \n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test, \n",
        "         \"Y_prediction_train\" : Y_prediction_train, \n",
        "         \"w\" : w, \n",
        "         \"b\" : b,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "    \n",
        "    return d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLJh--eMKpJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "d = model(X_flatten, X_flatten, y_flatten, y_flatten, num_iterations = 2000, learning_rate = 0.005, print_cost = True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
