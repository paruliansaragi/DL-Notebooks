{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFL4EagerExec.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paruliansaragi/DL-Notebooks/blob/master/TFL4EagerExec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIkgafw0hrc4",
        "colab_type": "text"
      },
      "source": [
        "Eager execution is (1) a NumPy-like library for numerical computation with support for GPU acceleration and automatic differentiation, and (2) a flexible platform for machine learning research and experimentation. It's available as tf.contrib.eager, starting with version 1.50 of TensorFlow. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USYdiVILhx2E",
        "colab_type": "text"
      },
      "source": [
        "- Motivation:\n",
        "    - TensorFlow today: Construct a graph and execute it.\n",
        "              -This is declarative programming. Its benefits include performance and easy translation to other platforms; drawbacks include that declarative programming is non-Pythonic and difficult to debug.\n",
        "      -What if you could execute operations directly? \n",
        "                -Eager execution offers just that: it is an imperative front-end to TensorFlow.\n",
        "\n",
        "- Key advantages: Eager execution …\n",
        "      -is compatible with Python debugging tools\n",
        "          \n",
        "          -pdb.set_trace() to your heart's content!\n",
        "\n",
        "      -provides immediate error reporting\n",
        "      -permits use of Python data structures\n",
        "              -e.g., for structured input\n",
        "\n",
        "- enables you to use and differentiate through Python control flow \n",
        "Enabling eager execution requires two lines of code\n",
        "\n",
        "`import tensorflow as tf\n",
        "import tensorflow.contrib.eager as tfe\n",
        "tfe.enable_eager_execution() # Call this at program start-up`\n",
        "\n",
        "\tand lets you write code that you can easily execute in a REPL, like this\n",
        "\n",
        "  x = [[2.]]  # No need for placeholders!\n",
        "  m = tf.matmul(x, x)\n",
        "\n",
        "  print(m)  # No sessions!\n",
        "  #tf.Tensor([[4.]], shape=(1, 1), dtype=float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNw8n9y2lNdD",
        "colab_type": "text"
      },
      "source": [
        "Automatic differentiation is built into eager execution\n",
        "\n",
        "Under the hood ...\n",
        "Operations are recorded on a tape\n",
        "The tape is played back to compute gradients\n",
        "This is reverse-mode differentiation (backpropagation\n",
        "\n",
        "When eager execution is enabled, the operations executed are traced in a “tape” that is played back to compute gradients. If you’re familiar with the autograd package, the API is very similar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3jHHPBpmEz8",
        "colab_type": "code",
        "outputId": "4c9275fc-4263-49f1-d655-fd17ce1c526b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/chiphuyen/stanford-tensorflow-tutorials/master/examples/utils.py\n",
        "!wget https://raw.githubusercontent.com/chiphuyen/stanford-tensorflow-tutorials/master/examples/data/birth_life_2010.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-25 10:40:29--  https://raw.githubusercontent.com/chiphuyen/stanford-tensorflow-tutorials/master/examples/utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5167 (5.0K) [text/plain]\n",
            "Saving to: ‘utils.py’\n",
            "\n",
            "\rutils.py              0%[                    ]       0  --.-KB/s               \rutils.py            100%[===================>]   5.05K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-01-25 10:40:29 (74.0 MB/s) - ‘utils.py’ saved [5167/5167]\n",
            "\n",
            "--2019-01-25 10:40:30--  https://raw.githubusercontent.com/chiphuyen/stanford-tensorflow-tutorials/master/examples/data/birth_life_2010.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5324 (5.2K) [text/plain]\n",
            "Saving to: ‘birth_life_2010.txt’\n",
            "\n",
            "birth_life_2010.txt 100%[===================>]   5.20K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-01-25 10:40:30 (63.5 MB/s) - ‘birth_life_2010.txt’ saved [5324/5324]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPCl19g3l_bb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Starter code for simple linear regression example using placeholders\n",
        "Created by Chip Huyen (huyenn@cs.stanford.edu)\n",
        "CS20: \"TensorFlow for Deep Learning Research\"\n",
        "cs20.stanford.edu\n",
        "Lecture 03\n",
        "\"\"\"\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "import utils\n",
        "\n",
        "DATA_FILE = 'birth_life_2010.txt'\n",
        "\n",
        "# Step 1: read in data from the .txt file\n",
        "data, n_samples = utils.read_birth_life_data(DATA_FILE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76u4QeJdmYf-",
        "colab_type": "text"
      },
      "source": [
        "One way to view TensorFlow is as a collection of operations - math, linear algebra, image processing, summary generation for TensorBoard visualization etc. - and a means to execute computations that compose them. Sessions provide one way to execute these compositions. With eager execution, Python is the way to execute compositions.\n",
        "\n",
        "But the underlying operations remain the same. And as a result, a bulk of the API surface remains the same too.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_Z6R2zOmccO",
        "colab_type": "text"
      },
      "source": [
        "Majority of TF API works regardless of whether eager execution is enabled.\n",
        "But, when eager execution is enabled  …\n",
        "prefer tfe.Variable under eager execution (compatible with graph construction)\n",
        "manage your own variable storage — variable collections are not supported!\n",
        "use tf.contrib.summary\n",
        "use tfe.Iterator to iterate over datasets under eager execution\n",
        "prefer object-oriented layers (e.g., tf.layers.Dense) \n",
        "functional layers (e.g., tf.layers.dense) only work if wrapped in tfe.make_template\n",
        "prefer tfe.py_func over tf.py_func\n",
        "\n",
        "See the user guide for details and updates\n",
        "\n",
        "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/g3doc/guide.md\n",
        "\n",
        "Imperative over declarative"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igB4_SEEaAWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In order to use eager execution, `tfe.enable_eager_execution()` must be\n",
        "# called at the very beginning of a TensorFlow program.\n",
        "tfe.enable_eager_execution()\n",
        "\n",
        "# Step 2: create placeholders for X (birth rate) and Y (life expectancy)\n",
        "# Remember both X and Y are scalars with type float\n",
        "#X, Y = None, None\n",
        "\n",
        "#############################\n",
        "########## TO DO ############\n",
        "#############################\n",
        "\n",
        "# Step 3: create weight and bias, initialized to 0.0\n",
        "# Make sure to use tf.get_variable\n",
        "#w, b = None, None\n",
        "# Create variables.\n",
        "w = tfe.Variable(0.0)\n",
        "b = tfe.Variable(0.0)\n",
        "#############################\n",
        "########## TO DO ############\n",
        "#############################\n",
        "\n",
        "# Step 4: build model to predict Y\n",
        "# e.g. how would you derive at Y_predicted given X, w, and b\n",
        "Y_predicted = x * w + b\n",
        "#############################\n",
        "########## TO DO ############\n",
        "#############################\n",
        "\n",
        "# Step 5: use the square error as the loss function\n",
        "def squared_loss(y, y_predicted):\n",
        "  return (y - y_predicted) ** 2\n",
        "#############################\n",
        "########## TO DO ############\n",
        "#############################\n",
        "\n",
        "# Step 6: using gradient descent with learning rate of 0.001 to minimize loss\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Create a filewriter to write the model's graph to TensorBoard\n",
        "#############################\n",
        "########## TO DO ############\n",
        "#############################\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    # Step 7: initialize the necessary variables, in this case, w and b\n",
        "    #############################\n",
        "    ########## TO DO ############\n",
        "    #############################\n",
        "\n",
        "    # Step 8: train the model for 100 epochs\n",
        "    for i in range(100):\n",
        "        total_loss = 0\n",
        "        for x, y in data:\n",
        "            # Execute train_op and get the value of loss.\n",
        "            # Don't forget to feed in data for placeholders\n",
        "            _, loss = ########## TO DO ############\n",
        "            total_loss += loss\n",
        "\n",
        "        print('Epoch {0}: {1}'.format(i, total_loss/n_samples))\n",
        "\n",
        "    # close the writer when you're done using it\n",
        "    #############################\n",
        "    ########## TO DO ############\n",
        "    #############################\n",
        "    writer.close()\n",
        "    \n",
        "    # Step 9: output the values of w and b\n",
        "    w_out, b_out = None, None\n",
        "    #############################\n",
        "    ########## TO DO ############\n",
        "    #############################\n",
        "\n",
        "print('Took: %f seconds' %(time.time() - start))\n",
        "\n",
        "# uncomment the following lines to see the plot \n",
        "# plt.plot(data[:,0], data[:,1], 'bo', label='Real data')\n",
        "# plt.plot(data[:,0], data[:,0] * w_out + b_out, 'r', label='Predicted data')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
