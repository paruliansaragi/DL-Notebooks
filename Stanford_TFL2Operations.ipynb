{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stanford TFL2Operations.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paruliansaragi/DL-Notebooks/blob/master/Stanford_TFL2Operations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9jqNaYsNnUj",
        "colab_type": "text"
      },
      "source": [
        "# Constant op"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMFah1BNNph_",
        "colab_type": "text"
      },
      "source": [
        "It’s straightforward to create a constant in TensorFlow. We’ve already done it several times. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU9TS9m4QI0O",
        "colab_type": "text"
      },
      "source": [
        "https://docs.google.com/document/d/1FSPNZFQsnaUVeTo0OQ2RrEZ0f4el9bIGI5sQALbG_F0/edit#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkuRc8nANxbr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca2lr25INf1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tf.constant(value, dtype=None, shape=None, name='Const', verify_shape=False)\n",
        "# constant of 1d tensor (vector)\n",
        "a = tf.constant([2, 2], name=\"vector\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NC0NvZ89OK6S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "h = tf.constant(2, shape=[2,2], verify_shape=False)\n",
        "# verify shape : provide a value and a shape, it throws an error if the val doesn't match the shape. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q04AiNi3QRp2",
        "colab_type": "code",
        "outputId": "c59fa696-fc6c-4df8-9a24-9ea41270166a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.InteractiveSession()\n",
        "a.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 2], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuTXV4zmNyqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# constant of 2x2 tensor (matrix)\n",
        "b = tf.constant([[0, 1], [2, 3]], name=\"matrix\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6feE6LKHQbdW",
        "colab_type": "code",
        "outputId": "1cf2c480-2df8-44d5-ebdb-b6c8abe2f32e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "b.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1],\n",
              "       [2, 3]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_zLS_IZP-KQ",
        "colab_type": "text"
      },
      "source": [
        "You can create a tensor of a specific dimension and fill it with a specific value, similar to Numpy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilJKKxRqP-jd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tf.zeros(shape, dtype=tf.float32, name=None)\n",
        "# create a tensor of shape and all elements are zeros\n",
        "tf.zeros([2, 3], tf.int32) ==> [[0, 0, 0], \n",
        "                                [0, 0, 0]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qCpVzFmQCzz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.zeros_like(input_tensor, dtype=None, name=None, optimize=True)\n",
        "# create a tensor of shape and type (unless type is specified) as the input_tensor but all elements are zeros.\n",
        "# input_tensor [[0, 1], [2, 3], [4, 5]]\n",
        "tf.zeros_like(input_tensor) ==> [[0, 0], \n",
        "                                 [0, 0], \n",
        "                                 [0, 0]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6g04KeVQqEM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tf.ones(shape, dtype=tf.float32, name=None)\n",
        "# create a tensor of shape and all elements are ones\n",
        "tf.ones([2, 3], tf.int32) ==> [[1, 1, 1], \n",
        "                               [1, 1, 1]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPc6EtTgQt_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.ones_like(input_tensor, dtype=None, name=None, optimize=True)\n",
        "# create a tensor of shape and type (unless type is specified) as the input_tensor but all elements are ones.\n",
        "# input_tensor is [[0, 1], [2, 3], [4, 5]]\n",
        "tf.ones_like(input_tensor) ==> [[1, 1], \n",
        "                                [1, 1], \n",
        "                                [1, 1]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PPeEtMOQwmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.fill(dims, value, name=None) \n",
        "# create a tensor filled with a scalar value.\n",
        "tf.fill([2, 3], 8) ==> [[8, 8, 8], \n",
        "                        [8, 8, 8]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEcfZ13XQzKT",
        "colab_type": "text"
      },
      "source": [
        "You can create constants that are sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXjtTouPQz0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tf.lin_space(start, stop, num, name=None)\n",
        "# create a sequence of num evenly-spaced values are generated beginning at start. If num > 1, the values in the sequence increase by (stop - start) / (num - 1), so that the last one is exactly stop.\n",
        "# comparable to but slightly different from numpy.linspace\n",
        "\n",
        "tf.lin_space(10.0, 13.0, 4, name=\"linspace\") ==> [10.0 11.0 12.0 13.0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1LLOcEWRDBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.range([start], limit=None, delta=1, dtype=None, name='range')\n",
        "# create a sequence of numbers that begins at start and extends by increments of delta up to but not including limit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Pi6gJzORECq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# slight different from range in Python\n",
        "\n",
        "# 'start' is 3, 'limit' is 18, 'delta' is 3\n",
        "tf.range(start, limit, delta) ==> [3, 6, 9, 12, 15]\n",
        "# 'start' is 3, 'limit' is 1,  'delta' is -0.5\n",
        "tf.range(start, limit, delta) ==> [3, 2.5, 2, 1.5]\n",
        "# 'limit' is 5\n",
        "tf.range(limit) ==> [0, 1, 2, 3, 4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G09ifOF7RHE6",
        "colab_type": "text"
      },
      "source": [
        "Note that unlike NumPy or Python sequences, TensorFlow sequences are not iterable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNKRcx5QRHV2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for _ in np.linspace(0, 10, 4): # OK\n",
        "for _ in tf.linspace(0.0, 10.0, 4): # TypeError: 'Tensor' object is not iterable.\n",
        "\n",
        "for _ in range(4): # OK\n",
        "for _ in tf.range(4): # TypeError: 'Tensor' object is not iterable."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5wrmmCgRKpV",
        "colab_type": "text"
      },
      "source": [
        "You can also generate random constants from certain distributions. See details.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T5uVJNJRLkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.random_normal\n",
        "tf.truncated_normal\n",
        "tf.random_uniform\n",
        "tf.random_shuffle\n",
        "tf.random_crop\n",
        "tf.multinomial\n",
        "tf.random_gamma\n",
        "tf.set_random_seed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXf1tRw4SxZI",
        "colab_type": "text"
      },
      "source": [
        "# Math Operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEqWCgzaS_bT",
        "colab_type": "text"
      },
      "source": [
        "TensorFlow math ops are pretty standard. You can visit the full list here. There are a few things that seem a bit tricky.\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1_Dha3mTeci",
        "colab_type": "text"
      },
      "source": [
        "TensorFlow’s zillion operations for division\n",
        "\n",
        "The last time I checked, TensorFlow has 7 different div operations, all doing more or less the same thing: tf.div, tf.divide, tf.truediv, tf.floordiv, tf.realdiv, tf.truncateddiv, tf.floor_div. The person who created those ops must really like dividing numbers. Make sure you read the documentation to understand which one to use. At a high level, tf.div does TensorFlow’s style division, while tf.divide does exactly Python’s style division.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeoNEeEHRNJc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = tf.constant([2, 2], name='a')\n",
        "b = tf.constant([[0, 1], [2, 3]], name='b')\n",
        "with tf.Session() as sess:\n",
        "\tprint(sess.run(tf.div(b, a)))             ⇒ [[0 0] \n",
        "                                               [1 1]]\n",
        "\tprint(sess.run(tf.divide(b, a)))          ⇒ [[0. 0.5] \n",
        "                                               [1. 1.5]]\n",
        "\tprint(sess.run(tf.truediv(b, a)))         ⇒ [[0. 0.5] \n",
        "                                               [1. 1.5]]\n",
        "\tprint(sess.run(tf.floordiv(b, a)))        ⇒ [[0 0] \n",
        "                                               [1 1]]\n",
        "\tprint(sess.run(tf.realdiv(b, a)))         ⇒ # Error: only works for real values\n",
        "  print(sess.run(tf.truncatediv(b, a)))     ⇒ [[0 0] \n",
        "                                               [1 1]]\n",
        "\tprint(sess.run(tf.floor_div(b, a)))       ⇒ [[0 0] \n",
        "                                               [1 1]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gPtrh0VTybd",
        "colab_type": "text"
      },
      "source": [
        "tf.add_n\n",
        "\n",
        "Allows you to add multiple tensors.\n",
        "tf.add_n([a, b, b])  => equivalent to a + b + b\n",
        "\n",
        "Dot product in TensorFlow\n",
        "\n",
        "Note that tf.matmul no longer does dot product. It multiplies matrices of ranks greater or equal to 2. To do dot product in TensorFlow, we use tf.tensordot. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWdzaWxgTkXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = tf.constant([10, 20], name='a')\n",
        "b = tf.constant([2, 3], name='b')\n",
        "with tf.Session() as sess:\n",
        "\tprint(sess.run(tf.multiply(a, b)))           ⇒ [20 60] # element-wise multiplication\n",
        "\tprint(sess.run(tf.tensordot(a, b, 1)))       ⇒ 80"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0uJyWzJT_uK",
        "colab_type": "text"
      },
      "source": [
        "Below is the table of ops in Python, taken from the book Fundamentals of Deep Learning.\n",
        "![alt text](https://lh4.googleusercontent.com/BvRZPPzVWtjlEqIajquibb9mqMuPSoZF2-2w2skJo5zXliInBufpKsDev3iMj7qfPRSCPm-ZHvI0-PAsOU_vxG1j3taUQNOF77_EObeb1ttw4_lV2YgtlZEsaof1tcgx3CWN2blj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLR1bteJUJGN",
        "colab_type": "text"
      },
      "source": [
        "# Data Types\n",
        "Python Native Types\n",
        "\n",
        "TensorFlow takes in Python native types such as Python boolean values, numeric values (integers, floats), and strings. Single values will be converted to 0-d tensors (or scalars), lists of values will be converted to 1-d tensors (vectors), lists of lists of values will be converted to 2-d\n",
        "tensors (matrices), and so on. Example below is adapted from TensorFlow for Machine Intelligence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ep4W6P-YUHev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_0 = 19 # Treated as a 0-d tensor, or \"scalar\" \n",
        "tf.zeros_like(t_0)                   # ==> 0\n",
        "tf.ones_like(t_0)                    # ==> 1\n",
        "\n",
        "t_1 = [b\"apple\", b\"peach\", b\"grape\"] # treated as a 1-d tensor, or \"vector\"\n",
        "tf.zeros_like(t_1)                   # ==> [b'' b'' b'']\n",
        "tf.ones_like(t_1)                    # ==> TypeError\n",
        "\n",
        "t_2 = [[True, False, False],\n",
        "       [False, False, True],\n",
        "       [False, True, False]]         # treated as a 2-d tensor, or \"matrix\"\n",
        "\n",
        "tf.zeros_like(t_2)                   # ==> 3x3 tensor, all elements are False\n",
        "tf.ones_like(t_2)                    # ==> 3x3 tensor, all elements are True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCHMB6LKUqEq",
        "colab_type": "text"
      },
      "source": [
        "TensorFlow Native Types\n",
        "\n",
        "Like NumPy, TensorFlow also has its own data types as you've seen: tf.int32, tf.float32, together with more exciting types such as tf.bfloat, tf.complex, tf.quint. Below is the full list of TensorFlow data types, as literally screenshot from tf.DType class. \n",
        "https://www.tensorflow.org/api_docs/python/tf/DType"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VR3pyb2Uwnr",
        "colab_type": "text"
      },
      "source": [
        "NumPy Data Types\n",
        "\n",
        "By now, you've probably noticed the similarity between NumPy and TensorFlow. TensorFlow was designed to integrate seamlessly with Numpy, the package that has become the lingua franca of data science.\n",
        "\n",
        "TensorFlow's data types are based on those of NumPy; in fact, np.int32 == tf.int32 returns True. You can pass NumPy types to TensorFlow ops. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92hrc66BUswq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.ones([2, 2], np.float32) ==> [[1.0 1.0], [1.0 1.0]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GYhUxmFVA-w",
        "colab_type": "text"
      },
      "source": [
        "Remember our best friend tf.Session.run()? If the requested object is a Tensor, the output of will be a NumPy array. \n",
        "\n",
        "**TL;DR:** Most of the times, you can use TensorFlow types and NumPy types interchangeably.\n",
        "\n",
        "**Note 1:** There is a catch here for string data types. For numeric and boolean types, TensorFlow and NumPy dtypes match down the line. However, tf.string does not have an exact match in NumPy due to the way NumPy handles strings. TensorFlow can still import string arrays from NumPy perfectly fine -- just don't specify a dtype in NumPy!\n",
        "\n",
        "**Note 2:** Both TensorFlow and NumPy are n-d array libraries. NumPy supports ndarray, but doesn't offer methods to create tensor functions and automatically compute derivatives, nor GPU support. There have been numerous efforts to create “NumPy for GPU”, such as Numba, PyCUDA,  gnumpy, but none has really taken off, so I guess TensorFlow is “NumPy for GPU”. Please correct me if I’m wrong here.\n",
        "\n",
        "**Note 3:** Using Python types to specify TensorFlow objects is quick and easy, and it is useful for prototyping ideas. However, there is an important pitfall in doing it this way. Python types lack the ability to explicitly state the data type, while TensorFlow's data types are more explicit. For example, all integers are the same type, but TensorFlow has 8-bit, 16-bit, 32-bit, and 64-bit integers available. Therefore, if you use a Python type, TensorFlow has to infer which data type you mean. \n",
        "\n",
        "It's possible to convert the data into the appropriate type when you pass it into TensorFlow, but certain data types still may be difficult to declare correctly, such as complex numbers. Because of this, it is common to create hand-defined Tensor objects as NumPy arrays. However, always use TensorFlow types when possible.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWRcSINhVSlD",
        "colab_type": "text"
      },
      "source": [
        "## Variables\n",
        "\n",
        "Constants have been fun and now is the time to learn about what really matters: **variables**. The differences between a constant and a variable:\n",
        "1. A constant is, well, constant. Often, you’d want your weights and biases to be updated during training.\n",
        "2. A constant's value is stored in the graph and replicated wherever the graph is loaded. A variable is stored separately, and may live on a parameter server.\n",
        "\n",
        "Point 2 means that constants are stored in the graph definition. When constants are memory expensive, such as a weight matrix with millions of entries, it will be slow each time you have to load the graph. To see what’s stored in the graph's definition, simply print out the graph's protobuf. Protobuf stands for protocol buffer, “Google's language-neutral, platform-neutral, extensible mechanism for serializing structured data – think XML, but smaller, faster, and simpler.” https://developers.google.com/protocol-buffers/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOHyaThyVN7s",
        "colab_type": "code",
        "outputId": "7f8f9a94-64f2-44c9-9ebd-fb5b82ab4da3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1880
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "my_const = tf.constant([1.0, 2.0], name=\"my_const\")\n",
        "print(tf.get_default_graph().as_graph_def())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "node {\n",
            "  name: \"vector\"\n",
            "  op: \"Const\"\n",
            "  attr {\n",
            "    key: \"dtype\"\n",
            "    value {\n",
            "      type: DT_INT32\n",
            "    }\n",
            "  }\n",
            "  attr {\n",
            "    key: \"value\"\n",
            "    value {\n",
            "      tensor {\n",
            "        dtype: DT_INT32\n",
            "        tensor_shape {\n",
            "          dim {\n",
            "            size: 2\n",
            "          }\n",
            "        }\n",
            "        tensor_content: \"\\002\\000\\000\\000\\002\\000\\000\\000\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "node {\n",
            "  name: \"Const\"\n",
            "  op: \"Const\"\n",
            "  attr {\n",
            "    key: \"dtype\"\n",
            "    value {\n",
            "      type: DT_INT32\n",
            "    }\n",
            "  }\n",
            "  attr {\n",
            "    key: \"value\"\n",
            "    value {\n",
            "      tensor {\n",
            "        dtype: DT_INT32\n",
            "        tensor_shape {\n",
            "          dim {\n",
            "            size: 2\n",
            "          }\n",
            "          dim {\n",
            "            size: 2\n",
            "          }\n",
            "        }\n",
            "        int_val: 2\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "node {\n",
            "  name: \"matrix\"\n",
            "  op: \"Const\"\n",
            "  attr {\n",
            "    key: \"dtype\"\n",
            "    value {\n",
            "      type: DT_INT32\n",
            "    }\n",
            "  }\n",
            "  attr {\n",
            "    key: \"value\"\n",
            "    value {\n",
            "      tensor {\n",
            "        dtype: DT_INT32\n",
            "        tensor_shape {\n",
            "          dim {\n",
            "            size: 2\n",
            "          }\n",
            "          dim {\n",
            "            size: 2\n",
            "          }\n",
            "        }\n",
            "        tensor_content: \"\\000\\000\\000\\000\\001\\000\\000\\000\\002\\000\\000\\000\\003\\000\\000\\000\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "node {\n",
            "  name: \"my_const\"\n",
            "  op: \"Const\"\n",
            "  attr {\n",
            "    key: \"dtype\"\n",
            "    value {\n",
            "      type: DT_FLOAT\n",
            "    }\n",
            "  }\n",
            "  attr {\n",
            "    key: \"value\"\n",
            "    value {\n",
            "      tensor {\n",
            "        dtype: DT_FLOAT\n",
            "        tensor_shape {\n",
            "          dim {\n",
            "            size: 2\n",
            "          }\n",
            "        }\n",
            "        tensor_content: \"\\000\\000\\200?\\000\\000\\000@\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "versions {\n",
            "  producer: 27\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI6K6XR2V4ZZ",
        "colab_type": "text"
      },
      "source": [
        "Creating variables\n",
        "\n",
        "To declare a variable, you create an **instance of the class tf.Variable**. Note that it's written tf.constant with lowercase ‘c’ but tf.Variable with uppercase ‘V’. It’s because tf.constant is an op, while tf.Variable is a class with multiple ops.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vbpSufPVwLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = tf.Variable(...) \n",
        "x.initializer # init \n",
        "x.value() # read op \n",
        "x.assign(...) # write op \n",
        "x.assign_add(...) \n",
        "# and more"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSCq6SENYHPn",
        "colab_type": "text"
      },
      "source": [
        "The old way to create a variable is simply call tf.Variable(< initial-value>, name=< optional- name>)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ol7yTDqAYHix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s = tf.Variable(2, name=\"scalar\") \n",
        "m = tf.Variable([[0, 1], [2, 3]], name=\"matrix\") \n",
        "W = tf.Variable(tf.zeros([784,10]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drsKU0CNYUy3",
        "colab_type": "text"
      },
      "source": [
        "However, this old way is discouraged and TensorFlow recommends that we use the wrapper tf.get_variable, which allows for easy variable sharing. With tf.get_variable, we can provide variable’s internal name, shape, type, and initializer to give the variable its initial value. Note that when we use tf.constant as an initializer, we don’t need to provide shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcBS7-JgYVGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.get_variable(\n",
        "    name,\n",
        "    shape=None,\n",
        "    dtype=None,\n",
        "    initializer=None,\n",
        "    regularizer=None,\n",
        "    trainable=True,\n",
        "    collections=None,\n",
        "    caching_device=None,\n",
        "    partitioner=None,\n",
        "    validate_shape=True,\n",
        "    use_resource=None,\n",
        "    custom_getter=None,\n",
        "    constraint=None\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_VLPBYLYcmC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s = tf.get_variable(\"scalar\", initializer=tf.constant(2)) \n",
        "m = tf.get_variable(\"matrix\", initializer=tf.constant([[0, 1], [2, 3]]))\n",
        "W = tf.get_variable(\"big_matrix\", shape=(784, 10), initializer=tf.zeros_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPiKauGJiW_E",
        "colab_type": "text"
      },
      "source": [
        "Initialize variables\n",
        "\n",
        "You have to initialize a variable before using it. If you try to evaluate the variables before initializing them you'll run into FailedPreconditionError: Attempting to use uninitialized value. To get a list of uninitialized variables, you can just print them out:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AIcOh3oiWqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(session.run(tf.report_uninitialized_variables()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naH7T3mBiZqb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The easiest way is initialize all variables at once\n",
        "with tf.Session() as sess:\n",
        "\tsess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNQ4F1H2igbn",
        "colab_type": "text"
      },
      "source": [
        "In this case, you use tf.Session.run() to fetch an initializer op, not a tensor op like we have used it previously.\n",
        "\n",
        "To initialize only a subset of variables, you use tf.variables_initializer() with a list of variables you want to initialize:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-bwmfhbidaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as sess:\n",
        "\tsess.run(tf.variables_initializer([a, b]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxpObYA_kNNw",
        "colab_type": "text"
      },
      "source": [
        "You can also initialize each variable separately using tf.Variable.initializer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIrRpywwkO_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as sess:\n",
        "\tsess.run(W.initializer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ye5z-x2IkVgu",
        "colab_type": "text"
      },
      "source": [
        "Another way to initialize a variable is to load its value from a file. We will talk about it in a few weeks.\n",
        "\n",
        "Evaluate values of variables\n",
        "\n",
        "Similar to TensorFlow tensors, to get the value of a variable, we need to fetch it within a session.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD0gkJSDkWoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# V is a 784 x 10 variable of random values\n",
        "V = tf.get_variable(\"normal_matrix\", shape=(784, 10), \n",
        "                     initializer=tf.truncated_normal_initializer())\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\tsess.run(tf.global_variables_initializer())\n",
        "\tprint(sess.run(V))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q82U3Grakg_I",
        "colab_type": "text"
      },
      "source": [
        "You can also get a variable’s value from tf.Variable.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7FVBRRikiog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  print(V.eval())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUyo8NOrkmYu",
        "colab_type": "text"
      },
      "source": [
        "Assign values to variables\n",
        "\n",
        "We can assign a value to a variable using tf.Variable.assign()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Unl3IWP6kn23",
        "colab_type": "code",
        "outputId": "91dbea9c-26bf-4ec4-a933-8d04ad774308",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "W = tf.Variable(10)\n",
        "W.assign(100)\n",
        "with tf.Session() as sess:\n",
        "\tsess.run(W.initializer)\n",
        "\tprint(W.eval()) # >> 10"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70xMVVS_kqVB",
        "colab_type": "text"
      },
      "source": [
        "Why 10 and not 100? W.assign(100) doesn't assign the value 100 to W, but instead create an assign op to do that. For this op to take effect, we have to run this op in session. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Pl-ougQkrk8",
        "colab_type": "code",
        "outputId": "913c654f-addf-464c-a5e3-d7cfb17b332a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "W = tf.Variable(10)\n",
        "\n",
        "assign_op = W.assign(100)\n",
        "with tf.Session() as sess:\n",
        "\tsess.run(assign_op)\n",
        "\tprint(W.eval()) # >> 100"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNonm_wEks17",
        "colab_type": "text"
      },
      "source": [
        "Note that we don't have to initialize W in this case, because assign() does it for us. In fact, the initializer op is an assign op that assigns the variable's initial value to the variable itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_Z1n_wEktS8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# in the source code\n",
        "self._initializer_op = state_ops.assign(self._variable, self._initial_value, validate_shape=validate_shape).op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n615gp8dlzpw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a variable whose original value is 2\n",
        "a = tf.get_variable('scalar', initializer=tf.constant(2)) \n",
        "a_times_two = a.assign(a * 2)\n",
        "with tf.Session() as sess:\n",
        "\tsess.run(tf.global_variables_initializer()) \n",
        "\tsess.run(a_times_two) # >> 4\n",
        "\tsess.run(a_times_two) # >> 8\n",
        "\tsess.run(a_times_two) # >> 16\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmcc9rqzl2Bm",
        "colab_type": "text"
      },
      "source": [
        "For simple incrementing and decrementing of variables, TensorFlow includes the tf.Variable.assign_add() and tf.Variable.assign_sub() methods. Unlike tf.Variable.assign(), tf.Variable.assign_add() and tf.Variable.assign_sub() don't initialize your variables for you because these ops depend on the initial values of the variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6WkYjkpl2bE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W = tf.Variable(10)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\tsess.run(W.initializer)\n",
        "\tprint(sess.run(W.assign_add(10))) # >> 20\n",
        "\tprint(sess.run(W.assign_sub(2)))  # >> 18"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWuVCQ0nmBuF",
        "colab_type": "text"
      },
      "source": [
        "Because TensorFlow sessions maintain values separately, each Session can have its own current value for a variable defined in a graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgYseZhpmEaY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W = tf.Variable(10)\n",
        "sess1 = tf.Session()\n",
        "sess2 = tf.Session()\n",
        "sess1.run(W.initializer)\n",
        "sess2.run(W.initializer)\n",
        "print(sess1.run(W.assign_add(10)))\t\t# >> 20\n",
        "print(sess2.run(W.assign_sub(2)))\t\t# >> 8\n",
        "print(sess1.run(W.assign_add(100)))\t\t# >> 120\n",
        "print(sess2.run(W.assign_sub(50)))\t\t# >> -42\n",
        "sess1.close()\n",
        "sess2.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeWP8kMdmKbV",
        "colab_type": "text"
      },
      "source": [
        "When you have a variable that depends on another variable, suppose you want to declare U = W * 2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyy57ZCXmLgn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# W is a random 700 x 10 tensor\n",
        "W = tf.Variable(tf.truncated_normal([700, 10]))\n",
        "U = tf.Variable(W * 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJvCyLk7mMpD",
        "colab_type": "text"
      },
      "source": [
        "In this case, you should use initialized_value() to make sure that W is initialized before its value is used to initialize U."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyIFfFGGmM6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "U = tf.Variable(W.initialized_value() * 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgsm_YULmO7S",
        "colab_type": "text"
      },
      "source": [
        "## Interactive Session\n",
        "You sometimes see InteractiveSession instead of Session. **The only difference is an InteractiveSession makes itself the default session so you can call run() or eval() without explicitly call the session**. This is convenient in interactive shells and IPython notebooks, as it avoids having to pass an explicit session object to run ops. However, it is complicated when you have multiple sessions to run.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBxRLbr6mRSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = tf.InteractiveSession()\n",
        "a = tf.constant(5.0)\n",
        "b = tf.constant(6.0)\n",
        "c = a * b\n",
        "print(c.eval()) # we can use 'c.eval()' without explicitly stating a session\n",
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yT9jC-qmUIe",
        "colab_type": "text"
      },
      "source": [
        "tf.get_default_session() returns the default session for the current thread. The returned Session will be the innermost session on which a Session or Session.as_default() context has been entered.\n",
        "## Control Dependencies\n",
        "Sometimes, we have two or more independent ops and **we'd like to specify which ops should be run first.** In this case, we use tf.Graph.control_dependencies([control_inputs]).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkRSd70amXcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# your graph g have 5 ops: a, b, c, d, e\n",
        "with g.control_dependencies([a, b, c]):\n",
        "  # `d` and `e` will only run after `a`, `b`, and `c` have executed.\n",
        "  d = ...\n",
        "  e = …"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SamhU4AomZM-",
        "colab_type": "text"
      },
      "source": [
        "## 8. Importing Data\n",
        "\n",
        "8.1 The old way: placeholders and feed_dict\n",
        "Remember from the lecture 1 that a TensorFlow program often has 2 phases:\n",
        "\n",
        "Phase 1: assemble a graph \n",
        "\n",
        "Phase 2: use a session to execute operations and evaluate variables in the graph\n",
        "\n",
        "We can assemble the graphs first without knowing the values needed for computation. This is equivalent to defining the function of x, y without knowing the values of x, y. For example:\n",
        "\n",
        "f(x, y) = 2x + y\n",
        "\n",
        "x, y are placeholders for the actual values.\n",
        "\n",
        "With the graph assembled, we, or our clients, can later supply their own data when they need to execute the computation. To define a placeholder, we use:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhGHtD_OmbDE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.placeholder(dtype, shape=None, name=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSdBJ04LmmGl",
        "colab_type": "text"
      },
      "source": [
        "Dtype, shape, and name are self-explanatory. The only thing to note here is when you set the shape of the placeholder to None. **shape=None means that tensors of any shape will be accepted.** Using shape=None is easy to construct graphs, but nightmarish for debugging. **You should always define the shape of your placeholders as detailed as possible.** shape=None also breaks all following shape inference, which makes many ops not work because they expect certain rank. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sij3hwabmnX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = tf.placeholder(tf.float32, shape=[3]) # a is placeholder for a vector of 3 elements\n",
        "b = tf.constant([5, 5, 5], tf.float32)\n",
        "c = a + b # use the placeholder as you would any tensor\n",
        "with tf.Session() as sess:\n",
        "\tprint(sess.run(c)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "909wRbTrmszu",
        "colab_type": "text"
      },
      "source": [
        "**When we try to get the value of c through a session, we will run into an error because to compute the value of c, we need to know the value of a**. **However, a is just a placeholder with no value**. To supplement the value of placeholders, **we use a feed_dict, which is basically a dictionary with keys being the placeholders, value being the values of those placeholders.**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2Fva_1UmuKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as sess:\n",
        "\t# compute the value of c given the value of a is [1, 2, 3]\n",
        "\tprint(sess.run(c, {a: [1, 2, 3]})) \t\t# [6. 7. 8.]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0aKelOAmveh",
        "colab_type": "text"
      },
      "source": [
        "Let's see how it looks in TensorBoard. Remember, first write the graph to the log file.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DlR1AYPmwr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "writer = tf.summary.FileWriter('./Graph/placeholders', tf.get_default_graph())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3L1Xh7GmzeW",
        "colab_type": "text"
      },
      "source": [
        "and type the following in the terminal:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlpbAs6umyvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "$ tensorboard --logdir='./Graph/placeholders'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1lS7GU3m3no",
        "colab_type": "text"
      },
      "source": [
        "As you can see, **placeholder are treated like any other op**. 3 is the shape of placeholder.\n",
        "\n",
        "\n",
        "In the previous example, we feed one single value to the placeholder. **What if we want to feed multiple data points to placeholder, for example, when we run computation through multiple data points in our training or testing set?**\n",
        "\n",
        "**We can feed as many data points to the placeholder as we want by iterating through the data set and feed in the value one at a time.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ecmTCJWm4C6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as sess:\n",
        "\tfor a_value in list_of_a_values:\n",
        "\t\tprint(sess.run(c, {a: a_value}))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1OHhBZ3m8dP",
        "colab_type": "text"
      },
      "source": [
        "You can feed values to tensors that aren't placeholders. Any tensors that are feedable can be fed. To check if a tensor is feedable or not, use:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKm32KzEm-VT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "tf.Graph.is_feedable(tensor)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OYi0s0om_sO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = tf.add(2, 5)\n",
        "b = tf.multiply(a, 3)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\tprint(sess.run(b)) \t\t\t\t\t\t# >> 21\n",
        "\t# compute the value of b given the value of a is 15\n",
        "\tprint(sess.run(b, feed_dict={a: 15})) \t\t\t# >> 45"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL1W26_snCzc",
        "colab_type": "text"
      },
      "source": [
        "feed_dict can be extremely useful to test your model. When you have a large graph and just want to test out certain parts, you can provide dummy values so TensorFlow won't waste time doing unnecessary computations.\n",
        "\n",
        "**8.2 The new way: tf.data**\n",
        "\n",
        "This is best to learn with examples, so we will cover it in the next lecture with linear and logistic regression.\n",
        "\n",
        "## 9. The trap of lazy loading\n",
        "One of the most common TensorFlow non-bug bugs I see is what my friend Danijar and I call “lazy loading”. **Lazy loading is a term that refers to a programming pattern when you defer declaring/initializing an object until it is loaded.** In the context of TensorFlow, it means you defer creating an op until you need to compute it. For example, this is normal loading: **you create the op z when you assemble the graph.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bSfmr8EnFBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = tf.Variable(10, name='x')\n",
        "y = tf.Variable(20, name='y')\n",
        "z = tf.add(x, y)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\tsess.run(tf.global_variables_initializer())\n",
        "\twriter = tf.summary.FileWriter('graphs/normal_loading', sess.graph)\n",
        "\tfor _ in range(10):\n",
        "    sess.run(z)\n",
        "\twriter.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdJ_dLvxnKA1",
        "colab_type": "text"
      },
      "source": [
        "This is what happens when someone decides to be clever and use lazy loading to save one line of code:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u3I3QXYnLKL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = tf.Variable(10, name='x')\n",
        "y = tf.Variable(20, name='y')\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\tsess.run(tf.global_variables_initializer())\n",
        "\twriter = tf.summary.FileWriter('graphs/lazy_loading', sess.graph)\n",
        "\tfor _ in range(10):\n",
        "\t\tsess.run(tf.add(x, y))\n",
        "\tprint(tf.get_default_graph().as_graph_def()) \n",
        "\twriter.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-sn2l-gnNzi",
        "colab_type": "text"
      },
      "source": [
        "Let's see the graphs for them on TensorBoard. Note that you can open Tensorboard with logdir=’graphs’ and you can easily switch between normal_loading graph and lazy_loading graph.\n",
        "\n",
        "They seem ostensibly similar. The first graph is normal loading, and the second is lazy loading.\n",
        "![alt text](https://lh4.googleusercontent.com/tjtLcMfZStJVu7QJs7kwpdUcFk2EgVHRi3r2739hyIGTdNQoFvWhNa80PVlEZps8sOsdMdngu5TEWdaSzIJ-VSG_qnLA4jQk_Hr7cDbf-ILCFDeVgTw8P7JTANiFESaUs0NlJUaB)\n",
        "![alt text](https://lh4.googleusercontent.com/gdEpS69ygHcdbDYeBaY-g4ZjCUMrrXEf0bsrwX4OjbMsg1eoZmjXq-5bwwVXNnVtU7Fy4Y2WMXoMIds5-5U7dA6j5cPEe0qzYaJPuCQxtXGSw_U6PBX0RvSlL3xTVXYARb0lAecG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf10M3Rpnbp8",
        "colab_type": "text"
      },
      "source": [
        "Let's look at the graph definition. Remember that to print out the graph definition, we use:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPjf3fTynZuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print(tf.get_default_graph().as_graph_def())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kic0rWNIneFJ",
        "colab_type": "text"
      },
      "source": [
        "The protobuf for the graph in normal loading has only 1 node “Add”:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXesxrrand87",
        "colab_type": "text"
      },
      "source": [
        "On the other hand, the protobuf for the graph in lazy loading has 10 copies of the node “Add”. It adds a new node “Add” every time you want to compute z!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTV6TpFyniS_",
        "colab_type": "text"
      },
      "source": [
        "You probably think: “This is stupid. Why would I want to compute the same value more than once?” and think that it's a bug that nobody will ever commit. It happens more often than you think. For example, you might want to compute the same loss function or make the same prediction every batch of training samples. If you aren’t careful, you can add thousands of unnecessary nodes to your graph. Your graph definition becomes bloated, slow to load and expensive to pass around.\n",
        "\n",
        "There are two ways to avoid this bug. First, always separate the definition of ops and their execution when you can. But when it is not possible because you want to group related ops into classes, you can use Python @property to ensure that your function is only loaded once when it's first called. This is not a Python course so I won't dig into how to do it. But if you want to know, check out this wonderful blog post by Danijar Hafner.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE_ENyuOninB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}