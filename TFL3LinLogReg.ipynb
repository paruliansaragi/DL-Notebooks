{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFL3LinLogReg.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paruliansaragi/DL-Notebooks/blob/master/TFL3LinLogReg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tnn9UPRd39KX",
        "colab_type": "text"
      },
      "source": [
        "Find a linear relationship between X and Y\n",
        "to predict Y from X\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2k0CKxN4M_s",
        "colab_type": "code",
        "outputId": "e2d4efae-a315-4c63-b4b5-18f1cbe50469",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/chiphuyen/stanford-tensorflow-tutorials/master/examples/data/birth_life_2010.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-24 22:04:58--  https://raw.githubusercontent.com/chiphuyen/stanford-tensorflow-tutorials/master/examples/data/birth_life_2010.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5324 (5.2K) [text/plain]\n",
            "Saving to: ‘birth_life_2010.txt’\n",
            "\n",
            "\rbirth_life_2010.txt   0%[                    ]       0  --.-KB/s               \rbirth_life_2010.txt 100%[===================>]   5.20K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-01-24 22:04:58 (93.9 MB/s) - ‘birth_life_2010.txt’ saved [5324/5324]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3k8_g2-4-ZG",
        "colab_type": "text"
      },
      "source": [
        "First, assume that the relationship between the birth rate and the life expectancy is linear, which means that we can find w and b such that Y = wX + b. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOriVU1T5HHI",
        "colab_type": "text"
      },
      "source": [
        "To find w and b (in this case, they are both scalars), we will use backpropagation through a one layer neural network. For the loss function, we will be using mean squared error. After each epoch, we measure the mean squared difference between the actual value Ys and the predicted values of Ys.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYV-TVem5chu",
        "colab_type": "code",
        "outputId": "cfa9375a-1f94-4f38-8ab4-3dbcd8268d6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/chiphuyen/stanford-tensorflow-tutorials/master/examples/utils.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-24 22:05:01--  https://raw.githubusercontent.com/chiphuyen/stanford-tensorflow-tutorials/master/examples/utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5167 (5.0K) [text/plain]\n",
            "Saving to: ‘utils.py’\n",
            "\n",
            "\rutils.py              0%[                    ]       0  --.-KB/s               \rutils.py            100%[===================>]   5.05K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-01-24 22:05:01 (78.9 MB/s) - ‘utils.py’ saved [5167/5167]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olumP98B5VLG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Starter code for simple linear regression example using placeholders\n",
        "Created by Chip Huyen (huyenn@cs.stanford.edu)\n",
        "CS20: \"TensorFlow for Deep Learning Research\"\n",
        "cs20.stanford.edu\n",
        "Lecture 03\n",
        "\"\"\"\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "import utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1UYSYCI5iRY",
        "colab_type": "code",
        "outputId": "b5f7d1c9-be7c-413e-98a9-5b4b7f902821",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "DATA_FILE = 'birth_life_2010.txt'\n",
        "\n",
        "# Step 1: read in data from the .txt file\n",
        "data, n_samples = utils.read_birth_life_data(DATA_FILE)\n",
        "\n",
        "#***********************\n",
        "# Step 2: create Dataset and iterator\n",
        "#https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices\n",
        "dataset = tf.data.Dataset.from_tensor_slices((data[:,0], data[:,1]))\n",
        "#inits tf.data.Dataset which does nothing but pass\n",
        "#@staticmethod\n",
        "#from_tensor_slices(tensors) ~ Creates a Dataset whose elements are slices of the given tensors.\n",
        "#tensors: A nested structure of tensors, each having the same size in the 0th dimension.\n",
        "#returns a dataset\n",
        "#with dataset we can now call make initializable iterator()\n",
        "#Creates an Iterator for enumerating the elements of this dataset.\n",
        "#Note: The returned iterator will be in an uninitialized state, \n",
        "#and you must run the iterator.initializer operation before using it:\n",
        "iterator = dataset.make_initializable_iterator()\n",
        "'''\n",
        "dataset = ...\n",
        "iterator = dataset.make_initializable_iterator()\n",
        "# ...\n",
        "sess.run(iterator.initializer)\n",
        "'''\n",
        "#returns An Iterator over the elements of this dataset.\n",
        "#https://www.tensorflow.org/api_docs/python/tf/data/Iterator#get_next\n",
        "#Class iterator\n",
        "#Represents the state of iterating through a Dataset.\n",
        "'''\n",
        "__init__(\n",
        "iterator_resource: A tf.resource scalar tf.Tensor representing the iterator.\n",
        "initializer: A tf.Operation that should be run to initialize this iterator.\n",
        "output_types: A nested structure of tf.DType objects corresponding to each component of an element of this iterator.\n",
        "output_shapes: A nested structure of tf.TensorShape objects corresponding to each component of an element of this iterator.\n",
        "output_classes: A nested structure of Python type objects corresponding to each component of an element of this iterator.\n",
        ")\n",
        "'''\n",
        "#Creates a new iterator from the given iterator resource.\n",
        "#get_next()\n",
        "#Returns a nested structure of tf.Tensors representing the next element.\n",
        "'''\n",
        "In graph mode, you should typically call this method once and use its result as the input \n",
        "to another computation. A typical loop will then call tf.Session.run on the result of that computation. \n",
        "The loop will terminate when the Iterator.get_next() operation raises tf.errors.OutOfRangeError. \n",
        "The following skeleton shows how to use this method when building a training loop:\n",
        "'''\n",
        "X, Y = iterator.get_next()\n",
        "'''\n",
        "dataset = ...  # A <a href=\"../../tf/data/Dataset\"><code>tf.data.Dataset</code></a> object.\n",
        "iterator = dataset.make_initializable_iterator()\n",
        "next_element = iterator.get_next()\n",
        "\n",
        "# Build a TensorFlow graph that does something with each element.\n",
        "loss = model_function(next_element)\n",
        "optimizer = ...  # A <a href=\"../../tf/train/Optimizer\"><code>tf.train.Optimizer</code></a> object.\n",
        "train_op = optimizer.minimize(loss)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  try:\n",
        "    while True:\n",
        "      sess.run(train_op)\n",
        "  except tf.errors.OutOfRangeError:\n",
        "    pass\n",
        "'''\n",
        "#**************************"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndataset = ...  # A <a href=\"../../tf/data/Dataset\"><code>tf.data.Dataset</code></a> object.\\niterator = dataset.make_initializable_iterator()\\nnext_element = iterator.get_next()\\n\\n# Build a TensorFlow graph that does something with each element.\\nloss = model_function(next_element)\\noptimizer = ...  # A <a href=\"../../tf/train/Optimizer\"><code>tf.train.Optimizer</code></a> object.\\ntrain_op = optimizer.minimize(loss)\\n\\nwith tf.Session() as sess:\\n  try:\\n    while True:\\n      sess.run(train_op)\\n  except tf.errors.OutOfRangeError:\\n    pass\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x67PjXWG55sT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#feed dict approach\n",
        "# Step 2: create placeholders for X (birth rate) and Y (life expectancy)\n",
        "# Remember both X and Y are scalars with type float\n",
        "#X = tf.placeholder(tf.float32, name='X')\n",
        "#Y = tf.placeholder(tf.float32, name='Y')\n",
        "#############################\n",
        "########## TO DO ############\n",
        "#############################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2AlPevG6ZBr",
        "colab_type": "text"
      },
      "source": [
        "The best way to create a variable is to call the tf.get_variable function. This function requires you to specify the Variable's name. This name will be used by other replicas to access the same variable, as well as to name this variable's value when checkpointing and exporting models. tf.get_variable also allows you to reuse a previously created variable of the same name, making it easy to define models which reuse layers.\n",
        "https://www.tensorflow.org/guide/variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSmgF0P1A-D1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3dG8rJI598b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 3: create weight and bias, initialized to 0.0\n",
        "# Make sure to use tf.get_variable\n",
        "#To create a variable with tf.get_variable, simply provide the name and shape\n",
        "#w, b = None, None\n",
        "w = tf.get_variable('weights', initializer=tf.constant(0.0))\n",
        "b = tf.get_variable('bias', initializer=tf.constant(0.0))\n",
        "#############################\n",
        "########## TO DO ############\n",
        "#############################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsUtwwx-68kr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 4: build model to predict Y\n",
        "# e.g. how would you derive at Y_predicted given X, w, and b\n",
        "Y_predicted = X * w + b \n",
        "#############################\n",
        "########## TO DO ############\n",
        "#############################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zIwpZki7Jaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 5: use the square error as the loss function\n",
        "loss = tf.square(Y - Y_predicted, name='loss')\n",
        "#############################\n",
        "########## TO DO ############\n",
        "#############################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECDygTUe7b-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 6: using gradient descent with learning rate of 0.001 to minimize loss\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)\n",
        "\n",
        "start = time.time()\n",
        "# Create a filewriter to write the model's graph to TensorBoard\n",
        "#############################\n",
        "########## TO DO ############\n",
        "#############################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjGx1X5S4rRE",
        "colab_type": "code",
        "outputId": "5960cacb-860f-4649-bc41-fd131b9787e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1768
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  \n",
        "\t# Step 7: initialize the necessary variables, in this case, w and b\n",
        "  sess.run(tf.global_variables_initializer()) \n",
        "  writer = tf.summary.FileWriter('./graphs/linear_reg', tf.get_default_graph())\n",
        "\t# Step 8: train the model for 100 epochs\n",
        "  for i in range(100):\n",
        "    sess.run(iterator.initializer) # initialize the iterator\n",
        "    total_loss = 0\n",
        "    for x, y in data:\n",
        "\t\t\t# Session execute optimizer and fetch values of loss\n",
        "      _, l = sess.run([optimizer, loss]) \n",
        "      total_loss += l\n",
        "    print('Epoch {0}: {1}'.format(i, total_loss/n_samples))\n",
        "\n",
        "\t# close the writer when you're done using it\n",
        "  writer.close() \n",
        "\t\n",
        "\t# Step 9: output the values of w and b\n",
        "  w_out, b_out = sess.run([w, b]) \n",
        "\n",
        "print('Took: %f seconds' %(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: 1661.8637834631543\n",
            "Epoch 1: 956.3224148609137\n",
            "Epoch 2: 844.6737023980994\n",
            "Epoch 3: 750.7312486011339\n",
            "Epoch 4: 667.6598341012079\n",
            "Epoch 5: 594.1417715627896\n",
            "Epoch 6: 529.07878103068\n",
            "Epoch 7: 471.5004191489204\n",
            "Epoch 8: 420.5458626462441\n",
            "Epoch 9: 375.45530721966765\n",
            "Epoch 10: 335.5543025185697\n",
            "Epoch 11: 300.24629857978107\n",
            "Epoch 12: 269.00376475843336\n",
            "Epoch 13: 241.35957466852116\n",
            "Epoch 14: 216.90039135300015\n",
            "Epoch 15: 195.25972298129324\n",
            "Epoch 16: 176.1137693605349\n",
            "Epoch 17: 159.17551693441837\n",
            "Epoch 18: 144.1907111125557\n",
            "Epoch 19: 130.93503488078713\n",
            "Epoch 20: 119.20935661137888\n",
            "Epoch 21: 108.8379309807855\n",
            "Epoch 22: 99.66466760624593\n",
            "Epoch 23: 91.55177013029001\n",
            "Epoch 24: 84.37664046781751\n",
            "Epoch 25: 78.03217824997724\n",
            "Epoch 26: 72.42182927812989\n",
            "Epoch 27: 67.46136239485718\n",
            "Epoch 28: 63.07566952367442\n",
            "Epoch 29: 59.19874146522856\n",
            "Epoch 30: 55.77168446383194\n",
            "Epoch 31: 52.74269822355127\n",
            "Epoch 32: 50.065632780875376\n",
            "Epoch 33: 47.70006421631674\n",
            "Epoch 34: 45.61017902122909\n",
            "Epoch 35: 43.76379750625255\n",
            "Epoch 36: 42.13259221098116\n",
            "Epoch 37: 40.69221939330516\n",
            "Epoch 38: 39.420219863367905\n",
            "Epoch 39: 38.297008645340895\n",
            "Epoch 40: 37.305591759538146\n",
            "Epoch 41: 36.43066341609841\n",
            "Epoch 42: 35.658453942681234\n",
            "Epoch 43: 34.97724816803575\n",
            "Epoch 44: 34.37655378567349\n",
            "Epoch 45: 33.84671358035044\n",
            "Epoch 46: 33.379665882282545\n",
            "Epoch 47: 32.96800991297258\n",
            "Epoch 48: 32.60548541990942\n",
            "Epoch 49: 32.28618434173986\n",
            "Epoch 50: 32.004961317298495\n",
            "Epoch 51: 31.757531331044525\n",
            "Epoch 52: 31.53978877073019\n",
            "Epoch 53: 31.348356819100445\n",
            "Epoch 54: 31.180119247269193\n",
            "Epoch 55: 31.03225782010038\n",
            "Epoch 56: 30.902462910201574\n",
            "Epoch 57: 30.78859985760776\n",
            "Epoch 58: 30.688725355066556\n",
            "Epoch 59: 30.60122861903357\n",
            "Epoch 60: 30.524590178362192\n",
            "Epoch 61: 30.457532704476954\n",
            "Epoch 62: 30.398967422668726\n",
            "Epoch 63: 30.34777825418737\n",
            "Epoch 64: 30.303121465726413\n",
            "Epoch 65: 30.26424930739051\n",
            "Epoch 66: 30.230392129550456\n",
            "Epoch 67: 30.200964921590334\n",
            "Epoch 68: 30.175501555469697\n",
            "Epoch 69: 30.153343991707324\n",
            "Epoch 70: 30.134226098457216\n",
            "Epoch 71: 30.117758308603477\n",
            "Epoch 72: 30.103543774372174\n",
            "Epoch 73: 30.09139442229674\n",
            "Epoch 74: 30.0809388476427\n",
            "Epoch 75: 30.07208499982095\n",
            "Epoch 76: 30.06452690966084\n",
            "Epoch 77: 30.058150938555205\n",
            "Epoch 78: 30.05278219980139\n",
            "Epoch 79: 30.04828310612785\n",
            "Epoch 80: 30.04458791257593\n",
            "Epoch 81: 30.041550708114855\n",
            "Epoch 82: 30.039046437352113\n",
            "Epoch 83: 30.03704103724602\n",
            "Epoch 84: 30.03545715799831\n",
            "Epoch 85: 30.034288759106282\n",
            "Epoch 86: 30.03338805212261\n",
            "Epoch 87: 30.032769865304076\n",
            "Epoch 88: 30.032386838833535\n",
            "Epoch 89: 30.032150670733166\n",
            "Epoch 90: 30.032092865493677\n",
            "Epoch 91: 30.032186730024037\n",
            "Epoch 92: 30.03240725137661\n",
            "Epoch 93: 30.032643962397827\n",
            "Epoch 94: 30.033039376884087\n",
            "Epoch 95: 30.033435566514413\n",
            "Epoch 96: 30.033922631802085\n",
            "Epoch 97: 30.03442924663878\n",
            "Epoch 98: 30.0349335548615\n",
            "Epoch 99: 30.03552558278714\n",
            "Took: 23.077729 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVrkzoGS9r4y",
        "colab_type": "code",
        "outputId": "ab277abc-caf3-4e47-bd4a-919ad30a5297",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "source": [
        "# uncomment the following lines to see the plot \n",
        "plt.plot(data[:,0], data[:,1], 'bo', label='Real data')\n",
        "plt.plot(data[:,0], data[:,0] * w_out + b_out, 'r', label='Predicted data')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd8AAAFOCAYAAADHOhe+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2HgG8A4Kjjf",
        "colab_type": "text"
      },
      "source": [
        "## Control flow: Huber loss \n",
        "\n",
        "Looking at the graph, we see that **several outliers on the central bottom are outliers**: they have low birth rate but also low life expectancy. Those **outliers pull the fitted line towards them, making the model perform worse**. One way to **deal with outliers is to use Huber loss.** Intuitively, **squared loss has the disadvantage of giving too much weights to outliers (you square the difference - the larger the difference, the larger its square).** **Huber loss was designed to give less weight to outliers**. Wikipedia has a pretty good article on it. Below is the Huber loss function:\n",
        "![alt text](https://lh5.googleusercontent.com/neMN1SXQIDTHCesfeYi9UZ_u5yDePwJKbulZ_fdGsU-TYVSgBopd1JftiTheGEtsvj7RoEGbmUgjfRHMSt2o0z3CLtd7F9hRG4JWgljJcE51qw9L55mkRfz9L8OnTT63873I0thpFRI)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mZJO5-RLAJO",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Control Flow Ops\n",
        "tf.count_up_to, tf.cond, tf.case, tf.while_loop, tf.group ...\n",
        "\n",
        "Comparison Ops\n",
        "tf.equal, tf.not_equal, tf.less, tf.greater, tf.where, ...\n",
        "\n",
        "Logical Ops\n",
        "tf.logical_and, tf.logical_not, tf.logical_or, tf.logical_xor\n",
        "\n",
        "Debugging Ops\n",
        "tf.is_finite, tf.is_inf, tf.is_nan, tf.Assert, tf.Print, ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0slgZEGoLGRc",
        "colab_type": "text"
      },
      "source": [
        "To implement Huber loss, we can use either tf.greater, tf.less, or tf.cond. We will be using tf.cond since it’s the most general. Other ops’ usage is pretty similar.\n",
        "\n",
        "tf.cond(\n",
        "    pred,\n",
        "    true_fn=None,\n",
        "    false_fn=None,\n",
        "    ...)\n",
        "\n",
        "This basically means that if the condition is true, use the true function. Else, use the false function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv2O3mSBDgnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def huber_loss(labels, predictions, delta=14.0):\n",
        "    residual = tf.abs(labels - predictions)\n",
        "    def f1(): return 0.5 * tf.square(residual)\n",
        "    def f2(): return delta * residual - 0.5 * tf.square(delta)\n",
        "    return tf.cond(residual < delta, f1, f2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lm3aH81fLoYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = huber_loss(Y, Y_predicted)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4FJkSU1LtNa",
        "colab_type": "code",
        "outputId": "f7bdc686-acef-41b2-ac7c-b8c85c9356f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1768
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  \n",
        "\t# Step 7: initialize the necessary variables, in this case, w and b\n",
        "  sess.run(tf.global_variables_initializer()) \n",
        "  writer = tf.summary.FileWriter('./graphs/linear_reg', tf.get_default_graph())\n",
        "\t# Step 8: train the model for 100 epochs\n",
        "  for i in range(100):\n",
        "    sess.run(iterator.initializer) # initialize the iterator\n",
        "    total_loss = 0\n",
        "    for x, y in data:\n",
        "\t\t\t# Session execute optimizer and fetch values of loss\n",
        "      _, l = sess.run([optimizer, loss]) \n",
        "      total_loss += l\n",
        "    print('Epoch {0}: {1}'.format(i, total_loss/n_samples))\n",
        "\n",
        "\t# close the writer when you're done using it\n",
        "  writer.close() \n",
        "\t\n",
        "\t# Step 9: output the values of w and b\n",
        "  w_out, b_out = sess.run([w, b]) \n",
        "\n",
        "print('Took: %f seconds' %(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: 405.33390211719086\n",
            "Epoch 1: 276.92073493803804\n",
            "Epoch 2: 255.1913424629914\n",
            "Epoch 3: 236.14337305095242\n",
            "Epoch 4: 218.3417923638107\n",
            "Epoch 5: 201.69615517933119\n",
            "Epoch 6: 186.1379784750311\n",
            "Epoch 7: 171.61408097820456\n",
            "Epoch 8: 158.0709525794183\n",
            "Epoch 9: 145.4507711169653\n",
            "Epoch 10: 133.70481315503682\n",
            "Epoch 11: 122.79710759645405\n",
            "Epoch 12: 112.70195787006784\n",
            "Epoch 13: 103.37940461973807\n",
            "Epoch 14: 94.7801957005929\n",
            "Epoch 15: 86.86284926594729\n",
            "Epoch 16: 79.58955065573774\n",
            "Epoch 17: 72.94197483201953\n",
            "Epoch 18: 66.88776330485156\n",
            "Epoch 19: 61.392573215283065\n",
            "Epoch 20: 56.41485941145373\n",
            "Epoch 21: 51.923847821036254\n",
            "Epoch 22: 47.88031687229186\n",
            "Epoch 23: 44.248855766663816\n",
            "Epoch 24: 40.990793884389326\n",
            "Epoch 25: 38.06812910462215\n",
            "Epoch 26: 35.44948532420064\n",
            "Epoch 27: 33.1085885187562\n",
            "Epoch 28: 31.019667888507644\n",
            "Epoch 29: 29.159701655594336\n",
            "Epoch 30: 27.506428265787267\n",
            "Epoch 31: 26.0378558288552\n",
            "Epoch 32: 24.734017096518073\n",
            "Epoch 33: 23.576581879115913\n",
            "Epoch 34: 22.54903337754505\n",
            "Epoch 35: 21.63648791424624\n",
            "Epoch 36: 20.8263478327855\n",
            "Epoch 37: 20.10872991984851\n",
            "Epoch 38: 19.473010796134506\n",
            "Epoch 39: 18.909780728406467\n",
            "Epoch 40: 18.411400310921415\n",
            "Epoch 41: 17.971281752380005\n",
            "Epoch 42: 17.582569208311217\n",
            "Epoch 43: 17.239419227546865\n",
            "Epoch 44: 16.936586540246285\n",
            "Epoch 45: 16.66924735735244\n",
            "Epoch 46: 16.433386825547114\n",
            "Epoch 47: 16.22529684587774\n",
            "Epoch 48: 16.04185219096314\n",
            "Epoch 49: 15.880102042503362\n",
            "Epoch 50: 15.73747709291117\n",
            "Epoch 51: 15.611812187160831\n",
            "Epoch 52: 15.50104022405239\n",
            "Epoch 53: 15.403479099247487\n",
            "Epoch 54: 15.317567124534062\n",
            "Epoch 55: 15.241904297462813\n",
            "Epoch 56: 15.175339418261252\n",
            "Epoch 57: 15.116800298227195\n",
            "Epoch 58: 15.065320343227665\n",
            "Epoch 59: 15.020090105864954\n",
            "Epoch 60: 14.98035169768233\n",
            "Epoch 61: 14.945467418105048\n",
            "Epoch 62: 14.914888141522992\n",
            "Epoch 63: 14.888056993047634\n",
            "Epoch 64: 14.864550017980907\n",
            "Epoch 65: 14.843990490390876\n",
            "Epoch 66: 14.825992139893444\n",
            "Epoch 67: 14.810261577777075\n",
            "Epoch 68: 14.796562326871362\n",
            "Epoch 69: 14.784567937337009\n",
            "Epoch 70: 14.774138516950648\n",
            "Epoch 71: 14.76507739488305\n",
            "Epoch 72: 14.757188596684443\n",
            "Epoch 73: 14.75037001452522\n",
            "Epoch 74: 14.744441888934938\n",
            "Epoch 75: 14.739349641666234\n",
            "Epoch 76: 14.734939284266071\n",
            "Epoch 77: 14.731158213675219\n",
            "Epoch 78: 14.727910313125406\n",
            "Epoch 79: 14.72512839516919\n",
            "Epoch 80: 14.722779266089569\n",
            "Epoch 81: 14.720785713972107\n",
            "Epoch 82: 14.719084528490509\n",
            "Epoch 83: 14.717657714410942\n",
            "Epoch 84: 14.71646744008314\n",
            "Epoch 85: 14.715504902977791\n",
            "Epoch 86: 14.714700746353245\n",
            "Epoch 87: 14.714054514015114\n",
            "Epoch 88: 14.713544533790225\n",
            "Epoch 89: 14.713127698712082\n",
            "Epoch 90: 14.712817432052965\n",
            "Epoch 91: 14.712598700769625\n",
            "Epoch 92: 14.712457392199921\n",
            "Epoch 93: 14.712343171641452\n",
            "Epoch 94: 14.712315410198828\n",
            "Epoch 95: 14.7123047808415\n",
            "Epoch 96: 14.71234918628807\n",
            "Epoch 97: 14.712415332612153\n",
            "Epoch 98: 14.712492050804002\n",
            "Epoch 99: 14.71261949530239\n",
            "Took: 2147.981470 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clJ39LYNL0CI",
        "colab_type": "code",
        "outputId": "761c2d9c-f830-4bc7-c160-4c7d231a92a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "source": [
        "plt.plot(data[:,0], data[:,1], 'bo', label='Real data')\n",
        "plt.plot(data[:,0], data[:,0] * w_out + b_out, 'r', label='Predicted data')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd8AAAFOCAYAAADHOhe+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8nFC1bzLkDn",
        "colab_type": "text"
      },
      "source": [
        "## tf.data\n",
        "\n",
        "According to Derek Murray in his introduction to tf.data, a nice thing about placeholder and feed_dicts is that they put the data processing outside TensorFlow, making it easy to shuffle, batch, and generate arbitrary data in Python. The drawback is that this mechanism can potentially slow down your program. Users often end up **processing their data in a single thread and creating data bottleneck** that slows execution down.\n",
        "\n",
        "TensorFlow also offers queues as another option to handle your data. This provides performance as it lets you do pipelining, threading and reduces the time loading data into placeholders. However, queues are notorious for being difficult to use and prone to crashing.\n",
        "\n",
        "Recently, demand for a better way to handle your data has been all the rage, and TensorFlow answers with tf.data module. It promises to be faster than placeholders and easier to use than queues, and doesn’t crash. So how does this magical thing work?\n",
        "\n",
        "Notice that in our linear regression, we stored the input data in a numpy array called data, each row of this numpy array is a pair value for (x, y), corresponding to a data point. To import this data into our TensorFlow model, we created placeholders for x (feature) and y (label). We then iterate through each data point with a for loop in step 8 and feed it into the placeholders with a feed_dict. We can, of course, use batches of data points instead of individual data points, but the key here is that the process of feeding the data from this numpy array to the TensorFlow model is slow and can get in the way of other execution of other ops.\n",
        "\n",
        "Step 1: read in data from the .txt file\n",
        "data is a numpy array of shape (190, 2), each row is a datapoint\n",
        "data, n_samples = utils.read_birth_life_data(DATA_FILE)\n",
        "\n",
        "Step 2: create placeholders for X (birth rate) and Y (life expectancy)\n",
        "X = tf.placeholder(tf.float32, name='X')\n",
        "Y = tf.placeholder(tf.float32, name='Y')\n",
        "\n",
        "...\n",
        "`with tf.Session() as sess:`\n",
        "       ...\n",
        "\t\n",
        "\t# Step 8: train the model\n",
        "\tfor i in range(100): # run 100 epochs\n",
        "\t\tfor x, y in data:\n",
        "\t\t\t# Session runs train_op to minimize loss\n",
        "\t\t\tsess.run(optimizer, feed_dict={X: x, Y:y}) \n",
        "      \n",
        "With tf.data, instead of storing our input data in a non-TensorFlow object, we store it in a tf.data.Dataset object. We can create a Dataset from tensors with:\n",
        "\n",
        "`tf.data.Dataset.from_tensor_slices((features, labels))`\n",
        "\n",
        "features and labels are supposed to be tensors, but remember that since TensorFlow and Numpy are seamlessly integrated, they can be NumPy arrays. We can initialize our dataset as followed:\n",
        "\n",
        "`dataset = tf.data.Dataset.from_tensor_slices((data[:,0], data[:,1]))`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZJik3NPN9x4",
        "colab_type": "text"
      },
      "source": [
        "You can also create a tf.data.Dataset from files using one of TensorFlow’s file format parsers, all of them have striking similarity to the old DataReader.\n",
        "tf.data.**TextLineDataset(filenames)**: each of the line in those files will become one entry. **It’s good for datasets whose entries are delimited by newlines such as data used for machine translation or data in csv files.**\n",
        "\n",
        "tf.data.**FixedLengthRecordDataset(filenames)**: each of the data point in this dataset is of the same length. It’s good for datasets whose entries are of a fixed length, such as CIFAR or ImageNet.\n",
        "\n",
        "tf.data.TFRecordDataset(filenames): it’s good to use if your data is stored in tfrecord format.\n",
        "\n",
        "Example:\n",
        "dataset = tf.data.FixedLengthRecordDataset([file1, file2, file3, ...])\n",
        "\n",
        "After we have **turned our data into a magical Dataset object**, we can **iterate through samples in this Dataset using an iterator**. An iterator **iterates through the Dataset and returns a new sample or batch each time we call get_next().** Let’s start with make_one_shot_iterator(), we’ll find out what it is in a bit. The iterator is of the class tf.data.Iterator.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqL91bEtLLta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iterator = dataset.make_one_shot_iterator()\n",
        "X, Y = iterator.get_next()         # X is the birth rate, Y is the life expectancy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0TWib4JOGKM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Each time we execute ops X, Y, we get a new data point.\n",
        "with tf.Session() as sess:\n",
        "\tprint(sess.run([X, Y]))\t\t# >> [1.822, 74.82825]\n",
        "\tprint(sess.run([X, Y]))\t\t# >> [3.869, 70.81949]\n",
        "\tprint(sess.run([X, Y]))\t\t# >> [3.911, 72.15066]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0M5H1S2OM9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now we can just compute Y_predicted and losses from X and Y just like you did with placeholders. The difference is that when you execute your graph, you no longer need to supplement data through feed_dict.\n",
        "for i in range(100): # train the model 100 epochs\n",
        "        total_loss = 0\n",
        "        try:\n",
        "            while True:\n",
        "                sess.run([optimizer]) \n",
        "        except tf.errors.OutOfRangeError:\n",
        "            pass\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7aPtErEOYm3",
        "colab_type": "text"
      },
      "source": [
        "We have to catch the OutOfRangeError because miraculously, TensorFlow doesn’t automatically catch it for us. If we run this code, we will see that we only get non zero loss in the first epoch. After that, the loss is always 0. It’s because dataset.make_one_shot_iterator() literally gives you only one shot. It’s fast to use -- you don’t have to initialize it -- but it can be used only once. After one epoch, you reach the end of your data and you can’t re-initialize it for the next epoch.\n",
        "\n",
        "To use for multiple epochs, we use `dataset.make_initializable_iterator()`. At the beginning of each epoch, you have to re-initialize your iterator.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIXV8mZFOZLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iterator = dataset.make_initializable_iterator()\n",
        "...\n",
        "for i in range(100): \n",
        "        sess.run(iterator.initializer) \n",
        "        total_loss = 0\n",
        "        try:\n",
        "            while True:\n",
        "                sess.run([optimizer]) \n",
        "        except tf.errors.OutOfRangeError:\n",
        "            pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eITvIixOeE7",
        "colab_type": "text"
      },
      "source": [
        "With tf.data.Dataset, you can batch, shuffle, repeat your data with just one command. You can also map each element of your dataset to transform it in a specific way to create a new dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlF3vOQPOeZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = dataset.shuffle(1000)\n",
        "dataset = dataset.repeat(100)\n",
        "dataset = dataset.batch(128)\n",
        "dataset = dataset.map(lambda x: tf.one_hot(x, 10)) \n",
        "# convert each element of dataset to one_hot vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcdDu_nqOlCu",
        "colab_type": "text"
      },
      "source": [
        "## Does tf.data really perform better?\n",
        "To compare the performance of tf.data with that of placeholders, I ran each model 100 times and calculated the average time each model took. On my Macbook Pro with 2.7 GHz Intel Core i5, the model with placeholder took on average 9.05271519 seconds, while the model with tf.data took on average 6.12285947 seconds. tf.data **improves the performance by 32.4%** compared to placeholders!\n",
        "\n",
        "So yes, tf.data does deliver. It makes importing and processing data easier while making our program run faster. \n",
        "## Optimizers\n",
        "In the code above, there are two lines that haven’t been explained.\n",
        "\n",
        "`optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)\n",
        "sess.run([optimizer]) `\n",
        "\n",
        "I remember the first time I ran into code similar to these, I was very confused.\n",
        "\n",
        "Why is optimizer in the fetches list of tf.Session.run()?\n",
        "\n",
        "How does TensorFlow know what variables to update?\n",
        "\n",
        "optimizer is an op whose job is to minimize loss. To execute this op, we need to pass it into the list of fetches of tf.Session.run(). When TensorFlow executes optimizer, it will execute the part of the graph that this op depends on. In this case, we see that optimizer depends on loss, and loss depends on inputs X,  Y, as well as two variables weights and bias. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0Py62VZ-c1O",
        "colab_type": "text"
      },
      "source": [
        "From the graph, you can see that the giant node GradientDescentOptimizer depends on 3 nodes: weights, bias, and gradients (which are automatically taken care of for us).\n",
        "\n",
        "**GradientDescentOptimizer means that our update rule is gradient descent. TensorFlow does auto differentiation for us, then update the values of w and b to minimize the loss. Autodiff is amazing!**\n",
        "\n",
        "By default, the optimizer **trains all the trainable variables its objective function depends on**. If there are variables that you do not want to train, you can set the keyword trainable=False when you declare a variable. One example of a variable you don’t want to train is the variable **global_step, a common variable you will see in many TensorFlow model to keep track of how many times you’ve run your model.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwwoM65--cWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global_step = tf.Variable(0, trainable=False, dtype=tf.int32)\n",
        "learning_rate = 0.01 * 0.99 ** tf.cast(global_step, tf.float32)\n",
        "increment_step = global_step.assign_add(1)#??assign_add()\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate) # learning rate can be a tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dQywkXh-wHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.Variable(\n",
        "    initial_value=None,\n",
        "    trainable=True,\n",
        "    collections=None,\n",
        "    validate_shape=True,\n",
        "    caching_device=None,\n",
        "    name=None,\n",
        "    variable_def=None,\n",
        "    dtype=None,\n",
        "    expected_shape=None,\n",
        "    import_scope=None,\n",
        "    constraint=None\n",
        ")\n",
        "\n",
        "tf.get_variable(\n",
        "    name,\n",
        "    shape=None,\n",
        "    dtype=None,\n",
        "    initializer=None,\n",
        "    regularizer=None,\n",
        "    trainable=True,\n",
        "    collections=None,\n",
        "    caching_device=None,\n",
        "    partitioner=None,\n",
        "    validate_shape=True,\n",
        "    use_resource=None,\n",
        "    custom_getter=None,\n",
        "    constraint=None\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe-eTiM9-x_m",
        "colab_type": "text"
      },
      "source": [
        "You can also ask your optimizer to take gradients of specific variables. You can also modify the gradients calculated by your optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiCLPePt-zgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create an optimizer.\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
        "\n",
        "# compute the gradients for a list of variables.\n",
        "grads_and_vars = optimizer.compute_gradients(loss, <list of variables>)\n",
        "\n",
        "# grads_and_vars is a list of tuples (gradient, variable).  Do whatever you\n",
        "# need to the 'gradient' part, for example, subtract each of them by 1.\n",
        "subtracted_grads_and_vars = [(gv[0] - 1.0, gv[1]) for gv in grads_and_vars]\n",
        "\n",
        "# ask the optimizer to apply the subtracted gradients.\n",
        "optimizer.apply_gradients(subtracted_grads_and_vars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXCHNM3C-4VC",
        "colab_type": "text"
      },
      "source": [
        "You can also **prevent certain tensors from contributing to the calculation of  the derivatives with respect to a specific loss with tf.stop_gradient.** \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzhvWokK-6Jp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_gradient( input, name=None )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uiIJt1J-4J6",
        "colab_type": "text"
      },
      "source": [
        "This is very useful in situations when you want to freeze certain variables during training. Here are some examples given by TensorFlow’s official documentation.\n",
        "**When you train a GAN (Generative Adversarial Network) where no backprop should happen through the adversarial example generation process.**\n",
        "The EM algorithm where the M-step should not involve backpropagation through the output of the E-step.\n",
        "\n",
        "The optimizer classes automatically compute derivatives on your graph, but you can explicitly ask TensorFlow to calculate certain gradients with tf.gradients.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvIbX8Rg--yI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.gradients(\n",
        "    ys,\n",
        "    xs,\n",
        "    grad_ys=None,\n",
        "    name='gradients',\n",
        "    colocate_gradients_with_ops=False,\n",
        "    gate_gradients=False,\n",
        "    aggregation_method=None,\n",
        "    stop_gradients=None\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZYcLCr2_BmW",
        "colab_type": "text"
      },
      "source": [
        "*This method constructs symbolic partial derivatives of sum of ys w.r.t. x in xs. ys and xs are each a Tensor or a list of tensors. grad_ys is a list of Tensor, holding the gradients received by the ys. The list must be the same length as ys.*\n",
        "\n",
        "Technical detail: This is especially useful when training only parts of a model. For example, we can use tf.gradients()  to take the derivative G of the loss w.r.t. to the middle layer. Then we use an optimizer to minimize the difference between the middle layer output M and M + G. This only updates the lower half of the network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoOKmhzv_HJt",
        "colab_type": "text"
      },
      "source": [
        "List of optimizers\n",
        "\n",
        "GradientDescentOptimizer is not the only update rule that TensorFlow supports. Here is the list of optimizers that TensorFlow supports, as of 1/17/2017. The names are self-explanatory. You can visit the official documentation for more details:\n",
        "\n",
        "tf.train.Optimizer\n",
        "\n",
        "tf.train.GradientDescentOptimizer\n",
        "\n",
        "tf.train.AdadeltaOptimizer\n",
        "\n",
        "tf.train.AdagradOptimizer\n",
        "\n",
        "tf.train.AdagradDAOptimizer\n",
        "\n",
        "tf.train.MomentumOptimizer\n",
        "\n",
        "tf.train.AdamOptimizer\n",
        "\n",
        "tf.train.FtrlOptimizer\n",
        "\n",
        "tf.train.ProximalGradientDescentOptimizer\n",
        "\n",
        "tf.train.ProximalAdagradOptimizer\n",
        "\n",
        "tf.train.RMSPropOptimizer\n",
        "\n",
        "**Sebastian Ruder, a PhD candidate at the Insight Research Centre for Data Analytics did a pretty great comparison of these optimizers in his blog post**. If you’re too lazy to read, here is the conclusion:\n",
        "\n",
        "“RMSprop is an extension of Adagrad that deals with its radically diminishing learning rates. It is identical to Adadelta, except that Adadelta uses the RMS of parameter updates in the numerator update rule. Adam, finally, adds bias-correction and momentum to RMSprop. Insofar, RMSprop, Adadelta, and Adam are very similar algorithms that do well in similar circumstances. Kingma et al. [15] show that its bias-correction helps Adam slightly outperform RMSprop towards the end of optimization as gradients become sparser. Insofar, Adam might be the best overall choice.”\n",
        "\n",
        "**TL;DR:** Use AdamOptimizer.\n",
        "Discussion questions\n",
        "What are some of the real world problems that we can solve using linear regression? Can you write a quick program to do so?\n",
        "\n",
        "# Logistic Regression with MNIST\n",
        "Let’s build a  logistic regression model in TensorFlow solving the good old classifier on the MNIST database.\n",
        "\n",
        "The MNIST (Mixed National Institute of Standards and Technology database) is one of the most popular databases used for training various image processing systems. It is a database of handwritten digits. The images look like this:\n",
        "\n",
        "\n",
        "Each image is 28 x 28 pixels. You can flatten each image to be a 1-d tensor of size 784. Each comes with a label from 0 to 9. For example, images on the first row is labelled as 0, the second as 1, and so on. The dataset is hosted on Yann Lecun’s website. \n",
        "\n",
        "TF Learn (the simplified interface of TensorFlow) has a script that lets you load the MNIST dataset from Yann Lecun’s website and divide it into train set, validation set, and test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db98xvo4_DQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets('data/mnist', one_hot=True) \n",
        "\n",
        "One-hot encoding\n",
        "In digital circuits, one-hot refers to a group of bits among which the legal combinations of values are only those with a single high (1) bit and all the others low (0).\n",
        "\n",
        "In this case, one-hot encoding means that if the output of the image is the digit 7, then the output will be encoded as a vector of 10 elements with all elements being 0, except for the element at index 7 which is 1.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGw6_3-O_Y03",
        "colab_type": "text"
      },
      "source": [
        "input_data.read_data_sets('data/mnist', one_hot=True) returns an instance of learn.datasets.base.Datasets, which contains three generators to 55,000 data points of training data (mnist.train), 10,000 points of test data (mnist.test), and 5,000 points of validation data (mnist.validation). You get the samples of these datasets by calling next_batch(batch_size), for example, mnist.train.next_batch(batch_size) with a batch_size of your choice. However, in real life, we often don’t have access to an off the shelf data parser I thought it’d be nice for us to just read in the MNIST data ourselves. It’s also a good practice because in your real life work, you’re likely to have to write your own data parser.\n",
        "\n",
        "I’ve already written the code for downloading and parsing MNIST data into numpy arrays in the file utils.py. All you need to do in your program is: \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-bgVO3r_bBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "mnist_folder = 'data/mnist'\n",
        "utils.download_mnist(mnist_folder)\n",
        "train, val, test = utils.read_mnist(mnist_folder, flatten=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UZqpU5h_ctG",
        "colab_type": "text"
      },
      "source": [
        "We choose flatten=True because we want each image to be flattened into a 1-d tensor. Each of train, val, and test in this case is a tuple of NumPy arrays, the first is a NumPy array of images, the second of labels. We need to create two Dataset objects, one for train set and one for test set (in this example, we won’t be using val set). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV9HmWk1_eY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "train_data = tf.data.Dataset.from_tensor_slices(train)\n",
        "# train_data = train_data.shuffle(10000) # if you want to shuffle your data\n",
        "test_data = tf.data.Dataset.from_tensor_slices(test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_WbnHRy_gEY",
        "colab_type": "text"
      },
      "source": [
        "The construction of the logistic regression model is pretty similar to the linear regression model. However, now we have A LOT more data. If we calculate gradient after every single data point it’d be painfully slow. Fortunately, we can process the data in batches. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N5JEjPj_pQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "train_data = train_data.batch(batch_size)\n",
        "test_data = test_data.batch(batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0S0REXC_qjP",
        "colab_type": "text"
      },
      "source": [
        "The next step is to create an iterator to get samples from the two datasets. In the linear regression example, we used only the train set, so it was okay to create an iterator for that dataset and just draw samples from that dataset. When we have more than one dataset, if we have one iterator for each dataset, we would need to build one graph for each iterator! A better way to do it is to create one single iterator and initialize it with a dataset when we need to draw data from that dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw9t_HA6_gZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "iterator = tf.data.Iterator.from_structure(train_data.output_types, \n",
        "                                           train_data.output_shapes)\n",
        "img, label = iterator.get_next()\n",
        "\n",
        "train_init = iterator.make_initializer(train_data)\t# initializer for train_data\n",
        "test_init  = iterator.make_initializer(test_data)\t# initializer for test_data\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    ...\n",
        "    for i in range(n_epochs):       # train the model n_epochs times\n",
        "        sess.run(train_init)\t       # drawing samples from train_data\n",
        "        try:\n",
        "            while True:\n",
        "                _, l = sess.run([optimizer, loss])\n",
        "        except tf.errors.OutOfRangeError:\n",
        "            pass\n",
        "\n",
        "    # test the model\n",
        "    sess.run(test_init)\t\t# drawing samples from test_data\n",
        "    try:\n",
        "        while True:\n",
        "            sess.run(accuracy)\n",
        "    except tf.errors.OutOfRangeError:\n",
        "        pass\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdSXHu3P_lTv",
        "colab_type": "text"
      },
      "source": [
        "Similar to linear regression, you can download the starter file examples/03_logreg_starter.py from the class’s GitHub repo and give it a shot. You can see the solution at examples/03_logreg.py.\n",
        "\n",
        "Running on my Mac, the batch version of the model with batch size 128 runs in 1 second, while the non-batch model runs in 30 seconds! Note that larger batch size typically requires more epochs since it does fewer update steps. See “mini-batch size” in Bengio's practical tips. Larger batch size also requires more memory.\n",
        "\n",
        "We achieved the accuracy of 91.34% after 30 epochs. This is about as good as we can get from a linear classifier.\n",
        "\n",
        "Shuffling can affect performance: without shuffling, the accuracy is consistently at 91.34%. With shuffle, the accuracy fluctuates between 88% to 93%.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zslco-M__mo_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}