{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFL5w2v.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paruliansaragi/DL-Notebooks/blob/master/TFL5w2v.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_gcIzHPWJ-7",
        "colab_type": "text"
      },
      "source": [
        "There are two main models used in word2vec: skip-gram and CBOW. \n",
        "\n",
        "\n",
        "Skip-gram vs CBOW (Continuous Bag-of-Words)\n",
        "\n",
        "\n",
        "Algorithmically, these models are similar, except that CBOW predicts center words from context words, while the skip-gram does the inverse and predicts source context-words from the center words. For example, if we have the sentence: \"\"The quick brown fox jumps\"\", then CBOW tries to predict \"\"brown\"\" from \"\"the\"\", \"\"quick\"\", \"\"fox\"\", and \"\"jumps\"\", while skip-gram tries to predict \"\"the\"\", \"\"quick\"\", \"\"fox\"\", and \"\"jumps\"\" from \"\"brown\"\".\n",
        "\n",
        "\n",
        "Statistically it has the effect that CBOW smoothes over a lot of the distributional information (by treating an entire context as one observation). For the most part, this turns out to be a useful thing for smaller datasets. However, skip-gram treats each context-target pair as a new observation, and this tends to do better when we have larger datasets.```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQMAB0qLXAli",
        "colab_type": "text"
      },
      "source": [
        "Instead, we care about the weights of the hidden layer. These weights are actually the “word vectors”, or “embedding matrix” that we’re trying to learn.\n",
        "\n",
        "The certain, fake task we’re going to train our model on is predicting the neighboring (context) words given the center word. Given a specific word in a sentence (the center word), look at the words nearby and pick one at random. The network is going to tell us the probability for every word in our vocabulary of being a neighbor to a specific word.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOJOOo1SXiRq",
        "colab_type": "text"
      },
      "source": [
        "**To get the distribution of the possible neighboring words, in theory, we often use softmax. Softmax maps arbitrary values xi to a probability distribution pi. In this case, softmax(xi) is the probability that xi is a neighboring word of a specific word we are considering.**\n",
        "\n",
        "`softmax(xi) = exp(xi) / SUMi exp(xi)`\n",
        "\n",
        " However, **the normalization term in the denominator requires us to perform exp on all words in the dictionary and sum the results up**, which could be millions of words. Even if you disregard uncommon words, a natural language model doesn’t perform well unless you consider at least tens of thousands of the most common words. **The normalization term causes softmax to be computationally prohibitive.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37Drzm5qX4wL",
        "colab_type": "text"
      },
      "source": [
        "There are two main approaches to circumvent this bottleneck: **hierarchical softmax and sample-based softmax**. Mikolov et al. have shown in their paper Distributed Representations of Words and Phrases and their Compositionality that for training the skip-gram model, **negative sample results in faster training and better vector representations for frequent words**, compared to more complex hierarchical softmax.\n",
        "\n",
        "Negative sampling, as the name suggests, belongs to the family of sample-based approaches. This family also includes importance sampling and target sampling. **Negative sampling is actually a simplified model of an approach called Noise Contrastive Estimation (NCE)**, e.g. **negative sampling makes certain assumption about the number of noise samples to generate -- let’s call it k -- and the distribution of noise samples -- let’s call it Q -- such that kQ(w) = 1 to simplify computation.** For more details, please see Sebastian Rudder’s On word embeddings - Part 2: Approximating the Softmax and Chris Dyer’s Notes on Noise Contrastive Estimation and Negative Sampling. \n",
        "\n",
        "While negative sampling is useful for the learning word embeddings, it doesn’t have the theoretical guarantee that its derivative tends towards the gradient of the softmax function. NCE, on the contrary, offers this guarantee as the number of noise samples increases. Mnih and Teh (2012) reported that 25 noise samples are sufficient to match the performance of the regular softmax, with an expected speed-up factor of about 45. For this reason, in this example, we will be using NCE.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1a28M2uWBxl",
        "colab_type": "code",
        "outputId": "8cc31601-eb1a-4d30-e48b-32fe65bf936f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "!wget http://mattmahoney.net/dc/text8.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-30 20:43:54--  http://mattmahoney.net/dc/text8.zip\n",
            "Resolving mattmahoney.net (mattmahoney.net)... 67.195.197.75\n",
            "Connecting to mattmahoney.net (mattmahoney.net)|67.195.197.75|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31344016 (30M) [application/zip]\n",
            "Saving to: ‘text8.zip’\n",
            "\n",
            "text8.zip           100%[===================>]  29.89M   862KB/s    in 36s     \n",
            "\n",
            "2019-01-30 20:44:30 (850 KB/s) - ‘text8.zip’ saved [31344016/31344016]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNaKMj5OZPw7",
        "colab_type": "code",
        "outputId": "96624861-c2f5-46ff-84a3-a36b1d31dad2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!unzip text8.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  text8.zip\n",
            "  inflating: text8                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NtN976FZrXh",
        "colab_type": "code",
        "outputId": "d461e1d8-4935-44ed-9f43-61fcc5a0c363",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/chiphuyen/stanford-tensorflow-tutorials/master/examples/word2vec_utils.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-30 20:44:34--  https://raw.githubusercontent.com/chiphuyen/stanford-tensorflow-tutorials/master/examples/word2vec_utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3032 (3.0K) [text/plain]\n",
            "Saving to: ‘word2vec_utils.py’\n",
            "\n",
            "\rword2vec_utils.py     0%[                    ]       0  --.-KB/s               \rword2vec_utils.py   100%[===================>]   2.96K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-01-30 20:44:34 (58.2 MB/s) - ‘word2vec_utils.py’ saved [3032/3032]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0gQK8hJZvic",
        "colab_type": "code",
        "outputId": "06c2e429-2aa2-417b-cdc5-8d174433ac0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/chiphuyen/stanford-tensorflow-tutorials/master/examples/utils.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-30 20:44:36--  https://raw.githubusercontent.com/chiphuyen/stanford-tensorflow-tutorials/master/examples/utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5167 (5.0K) [text/plain]\n",
            "Saving to: ‘utils.py’\n",
            "\n",
            "\rutils.py              0%[                    ]       0  --.-KB/s               \rutils.py            100%[===================>]   5.05K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-01-30 20:44:36 (48.5 MB/s) - ‘utils.py’ saved [5167/5167]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R64irkGjZYvz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" starter code for word2vec skip-gram model with NCE loss\n",
        "Eager execution\n",
        "CS 20: \"TensorFlow for Deep Learning Research\"\n",
        "cs20.stanford.edu\n",
        "Chip Huyen (chiphuyen@cs.stanford.edu) & Akshay Agrawal (akshayka@cs.stanford.edu)\n",
        "Lecture 04\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.eager as tfe\n",
        "\n",
        "import utils\n",
        "import word2vec_utils\n",
        "\n",
        "# Enable eager execution!\n",
        "tfe.enable_eager_execution()\n",
        "#############################\n",
        "########## TO DO ############\n",
        "#############################\n",
        "\n",
        "# Model hyperparameters\n",
        "VOCAB_SIZE = 50000\n",
        "BATCH_SIZE = 128\n",
        "EMBED_SIZE = 128            # dimension of the word embedding vectors\n",
        "SKIP_WINDOW = 1             # the context window\n",
        "NUM_SAMPLED = 64            # number of negative examples to sample\n",
        "LEARNING_RATE = 1.0\n",
        "NUM_TRAIN_STEPS = 100000\n",
        "VISUAL_FLD = 'visualization'\n",
        "SKIP_STEP = 5000\n",
        "\n",
        "# Parameters for downloading data\n",
        "DOWNLOAD_URL = 'http://mattmahoney.net/dc/text8.zip'\n",
        "EXPECTED_BYTES = 31344016"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrwlD_G7cqBt",
        "colab_type": "text"
      },
      "source": [
        "2. Define the weight (in this case, embedding matrix)\n",
        "Each row corresponds to the representation vector of one word. If one word is represented with a vector of size EMBED_SIZE, then the embedding matrix will have shape [VOCAB_SIZE, EMBED_SIZE]. We initialize the embedding matrix to value from a random distribution. In this case, let’s choose uniform distribution.\n",
        "\n",
        "\n",
        "```\n",
        "embed_matrix = tf.get_variable('embed_matrix', \n",
        "                                shape=[VOCAB_SIZE, EMBED_SIZE],\n",
        "                                initializer=tf.random_uniform_initializer())\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBi15fMScxR6",
        "colab_type": "text"
      },
      "source": [
        "3. Inference (compute the forward path of the graph)\n",
        "Our goal is to get the vector representations of words in our dictionary. Remember that the embed_matrix has dimension VOCAB_SIZE x EMBED_SIZE, with each row of the embedding matrix corresponds to the vector representation of the word at that index. So to get the representation of all the center words in the batch, we get the slice of all corresponding rows in the embedding matrix. TensorFlow provides a convenient method to do so.\n",
        "\n",
        "\n",
        "```\n",
        "tf.nn.embedding_lookup(\n",
        "    params,\n",
        "    ids,\n",
        "    partition_strategy='mod',\n",
        "    name=None,\n",
        "    validate_indices=True,\n",
        "    max_norm=None\n",
        ")\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LRjoLF6dpxe",
        "colab_type": "text"
      },
      "source": [
        "This method is really useful when it comes to matrix multiplication with one-hot vectors because it saves us from doing a bunch of unnecessary computation that will return 0 anyway. An illustration from Chris McCormick for multiplication of a one-hot vector with a matrix.\n",
        "\n",
        "![alt text](https://lh6.googleusercontent.com/BbA_0M6nE13xXBX-h2P8e2kwB7vtrZjWUOINPW2gt4J0iYMvU7cts3sN15AKxIRbq4Oa1nnrUVLST2fxizAgOi6f02JSHAqMin8dSPMO5E34s16NsNfezd60obfy9yOGsf3kPRg4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MUxD3ePdoxS",
        "colab_type": "text"
      },
      "source": [
        "So, to get the embedding (or vector representation) of the input center words, we use this:\n",
        "\n",
        "\n",
        "```\n",
        "embed = tf.nn.embedding_lookup(embed_matrix, center_words, name='embed')\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOf5pcb4d3H8",
        "colab_type": "text"
      },
      "source": [
        "4. Define the loss function\n",
        "While NCE is cumbersome to implement in pure Python, TensorFlow already implemented it for us.\n",
        "\n",
        "\n",
        "```\n",
        "tf.nn.nce_loss(\n",
        "    weights,\n",
        "    biases,\n",
        "    labels,\n",
        "    inputs,\n",
        "    num_sampled,\n",
        "    num_classes,\n",
        "    num_true=1,\n",
        "    sampled_values=None,\n",
        "    remove_accidental_hits=False,\n",
        "    partition_strategy='mod',\n",
        "    name='nce_loss'\n",
        ")\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f7fne13eoPi",
        "colab_type": "text"
      },
      "source": [
        "For nce_loss, we need weights and biases for the hidden layer to calculate NCE loss. They will be updated by optimizer during training. After sampling, the final output score will be computed as followed. This computation is done internally in tf.nn.nce_loss operation. \n",
        "\n",
        "```\n",
        "tf.matmul(embed, tf.transpose(nce_weight)) + nce_bias\n",
        "\n",
        "nce_weight = tf.get_variable('nce_weight', \n",
        "       shape=[VOCAB_SIZE, EMBED_SIZE],\n",
        "       initializer=tf.truncated_normal_initializer(stddev=1.0 / (EMBED_SIZE ** 0.5)))\n",
        "nce_bias = tf.get_variable('nce_bias', initializer=tf.zeros([VOCAB_SIZE]))\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "Then we define loss: \n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight, \n",
        "\t\t\t\t\tbiases=nce_bias, \n",
        "\t\t\t\t\tlabels=target_words, \n",
        "\t\t\t\t\tinputs=embed, \n",
        "\t\t\t\t\tnum_sampled=NUM_SAMPLED, \n",
        "\t\t\t\t\tnum_classes=VOCAB_SIZE))\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OpTijzaaC24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Word2Vec(object):\n",
        "  def __init__(self, vocab_size, embed_size, num_sampled=NUM_SAMPLED):\n",
        "    self.vocab_size = vocab_size\n",
        "    self.num_sampled = num_sampled\n",
        "    # Create the variables: an embedding matrix, nce_weight, and nce_bias\n",
        "    self.embed_matrix = tfe.Variable(tf.random_uniform(\n",
        "                                      [vocab_size, embed_size]))\n",
        "    self.nce_weight = tfe.Variable(tf.truncated_normal(\n",
        "                                    [vocab_size, embed_size],\n",
        "                                    stddev=1.0 / (embed_size ** 0.5)))\n",
        "    self.nce_bias = tfe.Variable(tf.zeros([vocab_size]))\n",
        "    #############################\n",
        "    ########## TO DO ############\n",
        "    #############################\n",
        "\n",
        "  def compute_loss(self, center_words, target_words):\n",
        "    \"\"\"Computes the forward pass of word2vec with the NCE loss.\"\"\" \n",
        "    # Look up the embeddings for the center words\n",
        "    \n",
        "    #############################\n",
        "    ########## TO DO ############\n",
        "    #############################\n",
        "    embed = tf.nn.embedding_lookup(self.embed_matrix, center_words)\n",
        "    \n",
        "\n",
        "    # Compute the loss, using tf.reduce_mean and tf.nn.nce_loss\n",
        "    #############################\n",
        "    ########## TO DO ############\n",
        "    #############################\n",
        "    loss = tf.reduce_mean(tf.nn.nce_loss(weights=self.nce_weight, \n",
        "                                        biases=self.nce_bias, \n",
        "                                        labels=target_words, \n",
        "                                        inputs=embed, \n",
        "                                        num_sampled=self.num_sampled, \n",
        "                                        num_classes=self.vocab_size))\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4koGsODccNaD",
        "colab_type": "text"
      },
      "source": [
        "1. Create dataset and generate samples from them\n",
        "Input is the center word and output is the neighboring (context) word. Instead of feeding words into our model, we create a dictionary of the most common words, and feed the indices of those words. For example, if the center word is the 1000thword in the vocabulary, we input the number 999. \n",
        "\n",
        "Each sample input is a scalar, so BATCH_SIZE of sample inputs have shape [BATCH_SIZE] Similarly, BATCH_SIZE of sample outputs have shape [BATCH_SIZE, 1].\n",
        "\n",
        "\n",
        "```\n",
        "dataset = tf.data.Dataset.from_generator(gen, \n",
        "                            (tf.int32, tf.int32), \n",
        "                            (tf.TensorShape([BATCH_SIZE]), tf.TensorShape([BATCH_SIZE, 1])))\n",
        "iterator = dataset.make_initializable_iterator()\n",
        "center_words, target_words = iterator.get_next()\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVebLPIfe-C6",
        "colab_type": "text"
      },
      "source": [
        "5. Define optimizer\n",
        "We will use the good old gradient descent.\n",
        "\n",
        "\n",
        "```\n",
        "optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)\n",
        "```\n",
        "\n",
        "\n",
        "Phase 2: Execute the computation\n",
        "We will create a good old session to run the optimizer to minimize the loss, and report the loss value back to us. Don’t forget to reinitialize your iterator!\n",
        "\n",
        "with tf.Session() as sess:\n",
        "        sess.run(iterator.initializer)\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        writer = tf.summary.FileWriter('graphs/word2vec_simple', sess.graph)\n",
        "\n",
        "        for index in range(NUM_TRAIN_STEPS):\n",
        "            try:\n",
        "                loss_batch, _ = sess.run([loss, optimizer])\n",
        "            except tf.errors.OutOfRangeError:\n",
        "                sess.run(iterator.initializer)\n",
        "        writer.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bObAtL5wf02V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir data\n",
        "!mv text8.zip data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xj2_eOqY0nj",
        "colab_type": "code",
        "outputId": "eefdc8f1-d9d6-43c8-f9db-40a4ed9e4a2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "\n",
        "def gen():\n",
        "  yield from word2vec_utils.batch_gen(DOWNLOAD_URL, EXPECTED_BYTES,\n",
        "                                      VOCAB_SIZE, BATCH_SIZE, SKIP_WINDOW,\n",
        "                                      VISUAL_FLD)\n",
        "\n",
        "def main():\n",
        "  dataset = tf.data.Dataset.from_generator(gen, (tf.int32, tf.int32),\n",
        "                              (tf.TensorShape([BATCH_SIZE]),\n",
        "                              tf.TensorShape([BATCH_SIZE, 1])))\n",
        "  optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE)\n",
        "  # Create the model\n",
        "  #############################\n",
        "  ########## TO DO ############\n",
        "  #############################\n",
        "  model = Word2Vec(vocab_size=VOCAB_SIZE, embed_size=EMBED_SIZE)\n",
        "\n",
        "  # Create the gradients function, using `tfe.implicit_value_and_gradients`\n",
        "  #############################\n",
        "  ########## TO DO ############\n",
        "  #############################\n",
        "  grad_fn = tfe.implicit_value_and_gradients(model.compute_loss)\n",
        "\n",
        "  total_loss = 0.0  # for average loss in the last SKIP_STEP steps\n",
        "  num_train_steps = 0\n",
        "  while num_train_steps < NUM_TRAIN_STEPS:\n",
        "    for center_words, target_words in tfe.Iterator(dataset):\n",
        "      if num_train_steps >= NUM_TRAIN_STEPS:\n",
        "        break\n",
        "\n",
        "      # Compute the loss and gradients, and take an optimization step.\n",
        "      #############################\n",
        "      ########## TO DO ############\n",
        "      #############################\n",
        "      loss_batch, grads = grad_fn(center_words, target_words)\n",
        "      total_loss += loss_batch\n",
        "      optimizer.apply_gradients(grads)\n",
        "      if (num_train_steps + 1) % SKIP_STEP == 0:\n",
        "        print('Average loss at step {}: {:5.1f}'.format(\n",
        "                num_train_steps, total_loss / SKIP_STEP))\n",
        "        total_loss = 0.0\n",
        "      num_train_steps += 1\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/text8.zip already exists\n",
            "Average loss at step 4999:  65.1\n",
            "Average loss at step 9999:  18.5\n",
            "Average loss at step 14999:   9.7\n",
            "Average loss at step 19999:   6.7\n",
            "Average loss at step 24999:   5.7\n",
            "Average loss at step 29999:   5.2\n",
            "Average loss at step 34999:   5.0\n",
            "Average loss at step 39999:   4.9\n",
            "Average loss at step 44999:   4.8\n",
            "Average loss at step 49999:   4.8\n",
            "Average loss at step 54999:   4.8\n",
            "Average loss at step 59999:   4.7\n",
            "Average loss at step 64999:   4.6\n",
            "Average loss at step 69999:   4.6\n",
            "Average loss at step 74999:   4.6\n",
            "Average loss at step 79999:   4.7\n",
            "Average loss at step 84999:   4.7\n",
            "Average loss at step 89999:   4.7\n",
            "Average loss at step 94999:   4.6\n",
            "Average loss at step 99999:   4.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxfgrRMpgIMz",
        "colab_type": "text"
      },
      "source": [
        "Question: how do we make our model most easy to reuse? \n",
        "Hint: take advantage of Python’s object-oriented-ness.\n",
        "Answer: build our model as a class!\n",
        "\n",
        "Our model base class should follow the interface. We combined step 3 and 4 because we want to put embed under the name scope of “NCE loss”. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AL6uPKjfaex",
        "colab_type": "text"
      },
      "source": [
        "Phase 1: assemble your graph\n",
        "1. Import data (either with tf.data or with placeholders)\n",
        "2. Define the weights\n",
        "3. Define the inference model\n",
        "4. Define loss function\n",
        "5. Define optimizer\n",
        " \n",
        "Phase 2: execute the computation\n",
        "Which is basically training your model. There are a few steps:\n",
        "1. Initialize all model variables for the first time.\n",
        "2. Initialize iterator / feed in the training data.\n",
        "3. Execute the inference model on the training data, so it calculates for each training input example the output with the current model parameters.\n",
        "4. Compute the cost\n",
        "5. Adjust the model parameters to minimize/maximize the cost depending on the model.\n",
        "\n",
        "Here is a visualization of training loop from the book TensorFlow for Machine Intelligence (Abrahams et al., 2016).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOyWBkyygVCw",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "class SkipGramModel:\n",
        "    \"\"\" Build the graph for word2vec model \"\"\"\n",
        "    def __init__(self, params):\n",
        "        pass\n",
        "\n",
        "    def _import_data(self):\n",
        "        \"\"\" Step 1: import data \"\"\"\n",
        "        pass\n",
        "\n",
        "    def _create_embedding(self):\n",
        "        \"\"\" Step 2: in word2vec, it's actually the weights that we care about \"\"\"\n",
        "        pass\n",
        "\n",
        "    def _create_loss(self):\n",
        "        \"\"\" Step 3 + 4: define the inference + the loss function \"\"\"\n",
        "        pass\n",
        "\n",
        "    def _create_optimizer(self):\n",
        "        \"\"\" Step 5: define optimizer \"\"\"\n",
        "        pass\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWA1m_uDg3mX",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "t-SNE (from Wikipedia)\n",
        "\n",
        "t-distributed stochastic neighbor embedding (t-SNE) is a machine learning algorithm for dimensionality reduction developed by Geoffrey Hinton and Laurens van der Maaten. It is a nonlinear dimensionality reduction technique that is particularly well-suited for embedding high-dimensional data into a space of two or three dimensions, which can then be visualized in a scatter plot. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points. \n",
        "\n",
        "The t-SNE algorithm comprises two main stages. First, t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects have a high probability of being picked, whilst dissimilar points have an extremely small probability of being picked. Second, t-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the Kullback–Leibler divergence between the two distributions with respect to the locations of the points in the map. Note that whilst the original algorithm uses the Euclidean distance between objects as the base of its similarity metric, this should be changed as appropriate. \n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2d0NsMqhb0U",
        "colab_type": "text"
      },
      "source": [
        "And we did all that visualization with less than 10 lines of code using TensorFlow projector with TensorBoard! The tool is super useful, albeit a bit finicky to use. The visualization will be stored in visualization folder. To see it, run  \"'tensorboard --logdir='visualization'\".\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from tensorflow.contrib.tensorboard.plugins import projector\n",
        "\n",
        "def visualize(self, visual_fld, num_visualize):\n",
        "        # create the list of num_variable most common words to visualize\n",
        "        word2vec_utils.most_common_words(visual_fld, num_visualize)\n",
        "\n",
        "        saver = tf.train.Saver()\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
        "\n",
        "            # if that checkpoint exists, restore from checkpoint\n",
        "            if ckpt and ckpt.model_checkpoint_path:\n",
        "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "\n",
        "            final_embed_matrix = sess.run(self.embed_matrix)\n",
        "            \n",
        "            # you have to store embeddings in a new variable\n",
        "            embedding_var = tf.Variable(final_embed_matrix[:num_visualize], name='embeded')\n",
        "            sess.run(embedding_var.initializer)\n",
        "\n",
        "            config = projector.ProjectorConfig()\n",
        "            summary_writer = tf.summary.FileWriter(visual_fld)\n",
        "\n",
        "            # add embedding to the config file\n",
        "            embedding = config.embeddings.add()\n",
        "            embedding.tensor_name = embedding_var.name\n",
        "            \n",
        "            # link this tensor to the file with the first NUM_VISUALIZE words of vocab\n",
        "            embedding.metadata_path = os.path.join(visual_fld,[file_of_most_common_words])\n",
        "\n",
        "            # saves a configuration file that TensorBoard will read during startup.\n",
        "            projector.visualize_embeddings(summary_writer, config)\n",
        "            saver_embed = tf.train.Saver([embedding_var])\n",
        "            saver_embed.save(sess, os.path.join(visual_fld, 'model.ckpt'), 1)\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrLkNlfohjEL",
        "colab_type": "text"
      },
      "source": [
        "# Variable sharing\n",
        "As you can see in the graph, the nodes are scattering all over, rendering the graph difficult to read. TensorBoard doesn’t know which nodes are similar to which nodes and should be grouped together. This can make debugging your graph daunting when you build complex models with hundreds of ops. \n",
        "\n",
        "How can we let TensorBoard know which nodes should be grouped together? For example, we would like to **group all ops related to input/output together**, and group all ops related to NCE loss together. TensorFlow lets us do that with **name_scope**. You can put all the ops that you want to group together under the block: \n",
        "```\n",
        "with tf.name_scope(name_of_that_scope):\n",
        "\t# declare op_1\n",
        "\t# declare op_2\n",
        "\t# ...\n",
        "```\n",
        "For example, our graph can have four name scopes: “data”, “embed”, “loss”, and “optimizer”.\n",
        "\n",
        "```\n",
        "with tf.name_scope('data'):\n",
        "    iterator = dataset.make_initializable_iterator()\n",
        "    center_words, target_words = iterator.get_next()\n",
        "\n",
        "with tf.name_scope('embed'):\n",
        "    embed_matrix = tf.get_variable('embed_matrix', \n",
        "                                    shape=[VOCAB_SIZE, EMBED_SIZE],\n",
        "                                    initializer=tf.random_uniform_initializer())\n",
        "    embed = tf.nn.embedding_lookup(embed_matrix, center_words, name='embedding')\n",
        "\n",
        "with tf.name_scope('loss'):\n",
        "    nce_weight = tf.get_variable('nce_weight', shape=[VOCAB_SIZE, EMBED_SIZE],\n",
        "                                initializer=tf.truncated_normal_initializer())\n",
        "    nce_bias = tf.get_variable('nce_bias', initializer=tf.zeros([VOCAB_SIZE]))\n",
        "\n",
        "    loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight, \n",
        "                                        biases=nce_bias, \n",
        "                                        labels=target_words, \n",
        "                                        inputs=embed, \n",
        "                                        num_sampled=NUM_SAMPLED, \n",
        "                                        num_classes=VOCAB_SIZE), name='loss')\n",
        "\n",
        "with tf.name_scope('optimizer'):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)\n",
        "```\n",
        "\n",
        "When you visualize that on TensorBoard, you will see your nodes are grouped into neat blocks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PdWT-8QiT0l",
        "colab_type": "text"
      },
      "source": [
        "You can click on the plus sign on top of each name scope block to see all the ops inside that block. Take your time to play around with it.\n",
        "\n",
        "\n",
        "TensorBoard has three kinds of edges: the solid grey arrows, the solid orange arrows, and the dotted arrows. The solid grey arrows represent data flow edges. For example, the op tf.add(x + y) get the values from x and y. The solid orange arrows are reference edges which represent which ops can mutate which ops. In this graph, it means that our optimizer can mutate -- in this case, update through backprop -- nce_weights, nce_bias, and embed_matrix.The dotted arrows represent control dependence edges. For example, nce_weight can only be executed after init -- a variable can only be used after being initialized. Control dependencies can be declared using tf.Graph.control_dependencies(control_inputs).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W19_qSpPiZYG",
        "colab_type": "text"
      },
      "source": [
        "#Variable scope\n",
        "One of the questions I’m often asked is: “So what’s the difference between name_scope and variable_scope”. While both create namespaces, **the main thing variable_scope does is to facilitate variable sharing.** Let’s explore why we need variable sharing.\n",
        "\n",
        "Assume that we want to create a neural network with two hidden layers as followed. We then called that two hidden layers network on two different input x1 and x2.\n",
        "```\n",
        "x1 = tf.truncated_normal([200, 100], name='x1')\n",
        "x2 = tf.truncated_normal([200, 100], name='x2')\n",
        "\n",
        "def two_hidden_layers(x):\n",
        "    assert x.shape.as_list() == [200, 100]\n",
        "    w1 = tf.Variable(tf.random_normal([100, 50]), name=\"h1_weights\")\n",
        "    b1 = tf.Variable(tf.zeros([50]), name=\"h1_biases\")\n",
        "    h1 = tf.matmul(x, w1) + b1\n",
        "    assert h1.shape.as_list() == [200, 50]  \n",
        "    w2 = tf.Variable(tf.random_normal([50, 10]), name=\"h2_weights\")\n",
        "    b2 = tf.Variable(tf.zeros([10]), name=\"h2_biases\")\n",
        "    logits = tf.matmul(h1, w2) + b2\n",
        "    return logits\n",
        "\n",
        "logits1 = two_hidden_layers(x1)\n",
        "logits2 = two_hidden_layers(x2)\n",
        "```\n",
        "If we visualize this on TensorBoard, this is what we see.\n",
        "**Each time you call two network, TensorFlow creates a different set of variables, while in fact, you want the network to share the same variables for all inputs, whether it’s x1, x2, or more. To do this, we first need to use tf.get_variable(). When we create a variable with tf.get_variable(), it first checks whether that variable exists. If it does, reuse it. If not, create a new one. However, if we simply replace tf.Variable() with tf.get_variable() such as the following:**\n",
        "```\n",
        "def two_hidden_layers_2(x):\n",
        "    assert x.shape.as_list() == [200, 100]\n",
        "    w1 = tf.get_variable(\"h1_weights\", [100, 50], initializer=tf.random_normal_initializer())\n",
        "    b1 = tf.get_variable(\"h1_biases\", [50], initializer=tf.constant_initializer(0.0))\n",
        "    h1 = tf.matmul(x, w1) + b1\n",
        "    assert h1.shape.as_list() == [200, 50]  \n",
        "    w2 = tf.get_variable(\"h2_weights\", [50, 10], initializer=tf.random_normal_initializer())\n",
        "    b2 = tf.get_variable(\"h2_biases\", [10], initializer=tf.constant_initializer(0.0))\n",
        "    logits = tf.matmul(h1, w2) + b2\n",
        "    return logits\n",
        "```\n",
        "**We will run into this error:\n",
        "ValueError: Variable h1_weights already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope?**\n",
        "\n",
        "To avoid this, we need to put all variables we want to use in a VarScope, and set that VarScope to be reusable.\n",
        "```\n",
        "with tf.variable_scope('two_layers') as scope:\n",
        "    logits1 = two_hidden_layers_2(x1)\n",
        "    scope.reuse_variables()\n",
        "    logits2 = two_hidden_layers_2(x2)\n",
        "def fully_connected(x, output_dim, scope):\n",
        "    with tf.variable_scope(scope) as scope:\n",
        "        w = tf.get_variable(\"weights\", [x.shape[1], output_dim], initializer=tf.random_normal_initializer())\n",
        "        b = tf.get_variable(\"biases\", [output_dim], initializer=tf.constant_initializer(0.0))\n",
        "        return tf.matmul(x, w) + b\n",
        "\n",
        "def two_hidden_layers(x):\n",
        "    h1 = fully_connected(x, 50, 'h1')\n",
        "    h2 = fully_connected(h1, 10, 'h2')\n",
        "\n",
        "with tf.variable_scope('two_layers') as scope:\n",
        "    logits1 = two_hidden_layers(x1)\n",
        "    scope.reuse_variables()\n",
        "    logits2 = two_hidden_layers(x2)\n",
        "```\n",
        "Let’s look at TensorBoard.\n",
        "\n",
        "\n",
        "There’s only one set of variables now, all within the variable_scope two_layers. They take in two different inputs x1 and x2. tf.variable_scope(\"name\") implicitly opens a tf.name_scope(\"name\").\n",
        "\n",
        "In our code, we write code for each layer. When we have more layers that are similar in structure, we probably want to make our code more reusable. \n",
        "```\n",
        "def fully_connected(x, output_dim, scope):\n",
        "    with tf.variable_scope(scope) as scope:\n",
        "        w = tf.get_variable('weights', [x.shape[1], output_dim], initializer=tf.random_normal_initializer())\n",
        "        b = tf.get_variable('biases', [output_dim], initializer=tf.constant_initializer(0.0))\n",
        "        return tf.matmul(x, w) + b\n",
        "\n",
        "def two_hidden_layers(x):\n",
        "    h1 = fully_connected(x, 50, 'h1')\n",
        "    h2 = fully_connected(h1, 10, 'h2')\n",
        "\n",
        "with tf.variable_scope('two_layers') as scope:\n",
        "    logits1 = two_hidden_layers(x1)\n",
        "    scope.reuse_variables()\n",
        "    logits2 = two_hidden_layers(x2)\n",
        "```\n",
        "You can scale it to infinitely many layers with many different kinds of activation functions!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n84KSp-IiuFs",
        "colab_type": "text"
      },
      "source": [
        "#Graph collections\n",
        "As you create a model, you might put your variables to different parts of the graph. Sometimes, you’d want an easy way to access them. tf.get_collection lets you access a certain collection of variables, with key being the name of the collection, scope is the scope of the variables.\n",
        "```\n",
        "tf.get_collection(\n",
        "    key,\n",
        "    scope=None\n",
        ")\n",
        "```\n",
        "By default, all variables are placed in tf.GraphKeys.GLOBAL_VARIABLES. To get all variables in scope “my_scope”, simply call. This turns a list of variables in “my_scope”.\n",
        "```\n",
        "tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='my_scope')\n",
        "```\n",
        "If you set trainable=True (which is always set by default) when you create your variable, that variable will be in the collection tf.GraphKeys.TRAINABLE_VARIABLES. \n",
        "\n",
        "You can have collections of ops that aren’t variables. And yes, you can create your own collections with tf.add_to_collection(name, value). For example, you can create a collection of initializers and  add all init ops to that. \n",
        "\n",
        "The standard library uses various well-known names to collect and retrieve values associated with a graph. For example, the tf.train.Optimizer subclasses default to optimizing the variables collected under tf.GraphKeys.TRAINABLE_VARIABLES if none is specified, but it is also possible to pass an explicit list of variables. For the list of predefined graph keys, please see the official documentation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvsPP0wUi32N",
        "colab_type": "text"
      },
      "source": [
        "Manage experiments\n",
        "We’ve built a word2vec model and it seems to be working pretty well using such a small dataset. We know that it’d take much longer time for a larger dataset, and we also know that training more complicated models can take an ungodly amount of time. For example, a machine translation models can take up to days, if not months on a single GPU. Many computer vision and reinforcement learning tasks require a really long time and a lot of patience. \n",
        "\n",
        "**It’s difficult to let our models run for days, wait and then make adjustment. If the computer or the cluster crashes, the training is interrupted and we’ll have to run our model all over again! It’s crucial to be able to stop training at any point, for any reason, and resume training as if nothing happens.** It will be helpful for analyzing our models, as this allows us closely inspect our models after any number of training steps.\n",
        "\n",
        "**Another problem that researchers often face is how to replicate research results. When building and training neural networks, we often use randomization. For example, we randomize the weights for our models, or we shuffle the order of our training samples. It’s important to learn how to control this random factor in our models.**\n",
        "\n",
        "In this part of the lecture, we will go over the excellent set of tools that TensorFlow provides to help us manage our experiments, including but not limited to **tf.train.Saver() class, TensorFlow’s random state, and visualization our training progress (aka more TensorBoard).**\n",
        "\n",
        "\n",
        "###tf.train.Saver()\n",
        "\n",
        "A good practice is to periodically save the model’s parameters after a certain number of steps or epochs so that we can restore/retrain our model from that step if need be. The tf.train.Saver() class allows us to do so by saving the graph’s variables in binary files. \n",
        "```\n",
        "tf.train.Saver.save(\n",
        "    sess,\n",
        "    save_path,\n",
        "    global_step=None,\n",
        "    latest_filename=None,\n",
        "    meta_graph_suffix='meta',\n",
        "    write_meta_graph=True,\n",
        "    write_state=True\n",
        ")\n",
        "```\n",
        "For example, if we want to save the variables of the graph after every 1000 training steps, we do the following:\n",
        "\n",
        "- define model\n",
        "\n",
        "- create a saver object\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "- launch a session to execute the computation\n",
        "\n",
        "```\n",
        "with tf.Session() as sess:\n",
        "    #actual training loop\n",
        "    for step in range(training_steps): \n",
        "\tsess.run([optimizer])\n",
        "\tif (step + 1) % 1000 == 0:\n",
        "\t   saver.save(sess, 'checkpoint_directory/model_name', global_step=global_step)\n",
        "```\n",
        "\n",
        "In TensorFlow lingo, **the step at which you save your graph’s variables is called a checkpoint. Since we will be creating many checkpoints, it’s helpful to append the number of training steps our model has gone through a variable called global_step.** It’s a variable you’d see in many TensorFlow programs. We first need to create it, initialize it to 0 and set it to be not trainable, since we don’t want TensorFlow to optimize it.\n",
        "```\n",
        "global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
        "```\n",
        "**We need to pass global_step as a parameter to the optimizer so it knows to increment global_step by one with each training step.**\n",
        "```\n",
        "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss,global_step=global_step)\n",
        "```\n",
        "To save the current values of variables in the folder ‘checkpoints’, we use this:\n",
        "```\n",
        "saver.save(sess, 'checkpoints/model-name', global_step=global_step)\n",
        "```\n",
        "To restore the variables, we use tf.train.Saver.restore(sess, save_path). For example, to restore the checkpoint at the 10,000th step.\n",
        "```\n",
        "saver.restore(sess, 'checkpoints/skip-gram-10000')\n",
        "```\n",
        "But of course, we can only load saved variables if there is a valid checkpoint. What you probably want to do is that if there is a checkpoint, restore it. If there isn’t, train from the start. TensorFlow allows you to get checkpoint from a directory with tf.train.get_checkpoint_state(‘directory-name’). The code for checking looks something like this:\n",
        "```\n",
        "ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
        "if ckpt and ckpt.model_checkpoint_path:\n",
        "     saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "```\n",
        "The file checkpoint automatically keeps track of the path to the latest checkpoint, so if we find the latest checkpoint from , 'checkpoints/checkpoint' we can always get the latest checkpoint. This is an example of what the file 'checkpoints/checkpoint' looks like.\n",
        "\n",
        "model_checkpoint_path: \"skip-gram-21999\"\n",
        "\n",
        "all_model_checkpoint_paths: \"skip-gram-13999\"\n",
        "\n",
        "all_model_checkpoint_paths: \"skip-gram-15999\"\n",
        "\n",
        "all_model_checkpoint_paths: \"skip-gram-17999\"\n",
        "\n",
        "all_model_checkpoint_paths: \"skip-gram-19999\"\n",
        "\n",
        "all_model_checkpoint_paths: \"skip-gram-21999\"\n",
        "\n",
        "\n",
        "So our training loop for word2vec now looks like this:\n",
        "```\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "initial_step = 0\n",
        "utils.safe_mkdir('checkpoints')\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(self.iterator.initializer)\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # if a checkpoint exists, restore from the latest checkpoint\n",
        "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
        "    if ckpt and ckpt.model_checkpoint_path:\n",
        "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "\n",
        "    writer = tf.summary.FileWriter('graphs/word2vec' + str(self.lr), sess.graph)\n",
        "\n",
        "    for index in range(num_train_steps):\n",
        "        try:\n",
        "            sess.run(self.optimizer)\n",
        "            # save the model every 1000 steps\n",
        "            if (index + 1) % 1000 == 0: \n",
        "                saver.save(sess, 'checkpoints/skip-gram', index)\n",
        "        except tf.errors.OutOfRangeError:\n",
        "            sess.run(self.iterator.initializer)\n",
        "            \n",
        "    writer.close()\n",
        "```\n",
        "If you go to the folder ‘checkpoints’, you will see files like the below:\n",
        "\n",
        "\n",
        "By default, saver.save() stores all variables of the graph, and this is recommended. However, you can also choose what variables to store by passing them in as a list or a dict when we create the saver object. This is an example from TensorFlow official documentation.\n",
        "```\n",
        "v1 = tf.Variable(..., name='v1') \n",
        "v2 = tf.Variable(..., name='v2') \n",
        "\n",
        "# pass the variables as a dict: \n",
        "saver = tf.train.Saver({'v1': v1, 'v2': v2}) \n",
        "\n",
        "# pass them as a list\n",
        "saver = tf.train.Saver([v1, v2]) \n",
        "\n",
        "# passing a list is equivalent to passing a dict with the variable op names # as keys\n",
        "saver = tf.train.Saver({v.op.name: v for v in [v1, v2]})\n",
        "```\n",
        "Note that savers only save variables, not the entire graph, so we still have to create the graph ourselves, and then load in variables. The checkpoints specify the way to map from variable names to tensors.\n",
        "\n",
        "What people usually do is not just save the parameters from the last iteration, but also save the parameters that give the best result so far so that you can evaluate your model on the best parameters so far.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gbi7ehQYi_1K",
        "colab_type": "text"
      },
      "source": [
        "##tf.summary \n",
        "\n",
        "We’ve been using matplotlib to visualize our losses and accuracy, which is unnecessary because TensorBoard provides us with a great set of tools to visualize our summary statistics during our training. Some popular statistics to visualize is loss, average loss, accuracy. You can visualize them as scalar plots, histograms, or even images. So we have a new namescope in our graph to hold all the summary ops.\n",
        "```\n",
        "def _create_summaries(self):\n",
        "     with tf.name_scope(\"summaries\"):\n",
        "            tf.summary.scalar(\"loss\", self.loss)\n",
        "            tf.summary.scalar(\"accuracy\", self.accuracy)            \n",
        "            tf.summary.histogram(\"histogram loss\", self.loss)\n",
        "            # because you have several summaries, we should merge them all\n",
        "            # into one op to make it easier to manage\n",
        "            self.summary_op = tf.summary.merge_all()\n",
        "```\n",
        "Because it’s an op, you have to execute it with sess.run()\n",
        "```\n",
        "loss_batch, _, summary = sess.run([model.loss, model.optimizer, model.summary_op], \n",
        "                                  feed_dict=feed_dict)\n",
        "```\n",
        "Now you’ve obtained the summary, you need to write the summary to file using the same FileWriter object we created to visualize our graph.\n",
        "```\n",
        "writer.add_summary(summary, global_step=step)\n",
        "```\n",
        "Now, if you go run tensorboard and go to http://localhost:6006/, in the Scalars page, you will see the plot of your scalar summaries. This is the summary of your loss in scalar plot.\n",
        "\n",
        "And the loss in histogram plot.\n",
        "\n",
        "If you save your summaries into different sub-folder in your graph folder, you can compare your progresses. For example, the first time we run our model with learning rate 1.0, we save it in ‘improved_graph/lr1.0’ and the second time we run our model, we save it in ‘improved_graph/lr0.5’, on the left corner of the Scalars page, we can toggle the plots of these two runs to compare them. This can be really helpful when you want to compare the progress made with different optimizers or different parameters.\n",
        "\n",
        "You can write a Python script to automate the naming of folders where you store the graphs/plots of each experiment.\n",
        "\n",
        "You can visualize the statistics as images using tf.summary.image.\n",
        "```\n",
        "tf.summary.image(name, tensor, max_outputs=3, collections=None)\n",
        "```\n",
        "##Control randomization \n",
        "I never realized what an oxymoron “control randomization” sounds like until I’ve written it down, but the truth is that you often have to control the randomization process to get stable results for your experiments. **You’re probably familiar with random seed and random state from NumPy. TensorFlow doesn’t allow you to get random state the way NumPy does (at least not that I know of -- I will double check), but it does allow you to get stable results in randomization through two ways:**\n",
        "\n",
        "1. **Set random seed at operation level. All random tensors allow you to pass in seed value in their initialization.** For example:\n",
        "```my_var = tf.Variable(tf.truncated_normal((-1.0,1.0), stddev=0.1, seed=0))```\n",
        "\n",
        "Note that, session is the thing that keeps track of random state, so each new session will start the random state all over again.\n",
        "```\n",
        "c = tf.random_uniform([], -10, 10, seed=2)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\tprint sess.run(c) # >> 3.57493\n",
        "\tprint sess.run(c) # >> -5.97319\n",
        "\n",
        "c = tf.random_uniform([], -10, 10, seed=2)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\tprint sess.run(c) # >> 3.57493\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\tprint sess.run(c) # >> 3.57493\n",
        "```\n",
        "With operation level random seed, each op keeps its own seed.\n",
        "```\n",
        "c = tf.random_uniform([], -10, 10, seed=2)\n",
        "d = tf.random_uniform([], -10, 10, seed=2)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\tprint sess.run(c) # >> 3.57493\n",
        "\tprint sess.run(d) # >> 3.57493\n",
        "```\n",
        "2. Set random seed at graph level with tf.Graph.seed\n",
        "```\n",
        "tf.set_random_seed(seed)\n",
        "```\n",
        "If you don’t care about the randomization for each op inside the graph, but just want to be able to replicate result on another graph (so that other people can replicate your results on their own graph), you can use tf.set_random_seed instead. Setting the current TensorFlow random seed affects the current default graph only. \n",
        "\n",
        "For example, you have two models a.py and b.py that have identical code:\n",
        "```\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.set_random_seed(2)\n",
        "c = tf.random_uniform([], -10, 10)\n",
        "d = tf.random_uniform([], -10, 10)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\tprint sess.run(c)\n",
        "\tprint sess.run(d)\n",
        "```\n",
        "Without graph level seed, running python a.py and b.py will return 2 completely different results, but with tf.set_random_seed, you will get two identical results:\n",
        "\n",
        "$ python a.py \n",
        ">> -4.00752\n",
        ">> -2.98339\n",
        "\n",
        "$ python b.py \n",
        ">> -4.00752\n",
        ">> -2.98339\n",
        "\n",
        "##Autodiff (how TensorFlow takes gradients)\n",
        "In all the models we’ve built so far, we haven’t taken a single gradient. All we need to do is to build a forward pass and TensorFlow takes care of the backward path for us. For example, if tensor C depends on a set of previous nodes, the gradient of C with respect to those previous nodes can be automatically computed with a built-in function, even if there are many layers in between them.\n",
        "\n",
        "TensorFlow uses what’s known as the reverse mode automatic differentiation. It allows you to take derivative of a function at roughly the same cost as computing the original function. Gradients are computed by creating additional nodes and edges in the graph. For example, you need to compute the gradients of C with respect to I, first TensorFlow looks for the path between these two nodes. Once the path is found, TensorFlow starts at C and moves backward toward I. For every operation on this backward path, a node is added to the graph, composing the partial gradients of each added node via the chain rule. This process is visualized in TensorFlow white paper:\n",
        "\n",
        "To compute partial gradients, we can use tf.gradients()\n",
        "```\n",
        "tf.gradients(ys, xs, grad_ys=None, name='gradients', colocate_gradients_with_ops=False, gate_gradients=False, aggregation_method=None)\n",
        "```\n",
        "tf.gradients(ys, [xs]) with [xs] stands for a list of tensors with respect to those you’re trying to compute the gradient of ys. It will return a list of gradient values.\n",
        "```\n",
        "x = tf.Variable(2.0)\n",
        "y = 2.0 * (x ** 3)\n",
        "\n",
        "grad_y = tf.gradients(y, x)\n",
        "with tf.Session() as sess:\n",
        "\tsess.run(x.initializer)\n",
        "\tprint sess.run(grad_y) # >> 24.0\n",
        "\n",
        "x = tf.Variable(2.0)\n",
        "y = 2.0 * (x ** 3)\n",
        "z = 3.0 + y ** 2\n",
        "\n",
        "grad_z = tf.gradients(z, [x, y])\n",
        "with tf.Session() as sess:\n",
        "\tsess.run(x.initializer)\n",
        "\tprint sess.run(grad_z) # >> [768.0, 32.0]\n",
        "# 768 is the gradient of z with respect to x, 32 with respect to y\n",
        "```\n",
        "You should check by hand to see that this is correct.\n",
        "\n",
        "So, the question is: why should we still learn to take gradient? Why are Chris Manning and Richard Socher making us take gradients of cross entropy and softmax? Shouldn’t taking gradients by hands one day be as obsolete as trying to take square root by hands since the invention of calculator?\n",
        "\n",
        "Well, maybe. But for now, TensorFlow can take gradients for us, but it can’t give us intuition about what functions to use. It doesn’t tell us if a function will suffer from exploding or vanishing gradients. We still need to know about gradients to get an understanding of why a model works while another doesn’t.\n",
        "\n",
        "\n",
        "We plot the error surface of a one hidden unit recurrent network, highlighting the existence of high curvature walls. The solid lines depicts standard trajectories that gradient descent might follow. Using dashed arrow the diagram shows what would happen if the gradients is rescaled to a fixed size when its norm is above a threshold.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3vFMOyHfbN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}